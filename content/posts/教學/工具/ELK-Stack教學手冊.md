+++
date = '2026-01-29T19:09:08+08:00'
draft = false
title = 'ELK Stackæ•™å­¸æ‰‹å†Š'
tags = ['æ•™å­¸', 'å·¥å…·', 'Visualization','ELK stack']
categories = ['æ•™å­¸']
+++


# Logstash / Elasticsearch / Kibanaï¼ˆELK Stackï¼‰æ•™å­¸æ‰‹å†Š

> **ç‰ˆæœ¬**ï¼š1.0  
> **æœ€å¾Œæ›´æ–°**ï¼š2026 å¹´ 1 æœˆ  
> **é©ç”¨å°è±¡**ï¼šè³‡æ·±è»Ÿé«”å·¥ç¨‹å¸«ã€ç³»çµ±æ¶æ§‹å¸«ã€SRE / DevOps å·¥ç¨‹å¸« 
> **å‰ç½®çŸ¥è­˜**ï¼šLinux åŸºç¤ã€Java æ‡‰ç”¨ç¨‹å¼ã€åŸºæœ¬ç¶²è·¯æ¦‚å¿µ
> **æœ€å¾Œæ›´æ–°**: 2026å¹´1æœˆ27æ—¥  
> **é©ç”¨æ–¼**: Logs Visualization 
> **Created by**: Eric Cheng

## ç›®éŒ„

- [ç¬¬ä¸€ç« ï¼šLogs Visualization èˆ‡ ELK Stack æ¦‚è¿°](#ç¬¬ä¸€ç« logs-visualization-èˆ‡-elk-stack-æ¦‚è¿°)
  - [1.1 ç‚ºä»€éº¼éœ€è¦ Logs Visualization](#11-ç‚ºä»€éº¼éœ€è¦-logs-visualization)
  - [1.2 Logs èˆ‡ Metrics çš„å·®ç•°èˆ‡äº’è£œ](#12-logs-èˆ‡-metrics-çš„å·®ç•°èˆ‡äº’è£œ)
  - [1.3 ELK Stack æ¶æ§‹ç¸½è¦½](#13-elk-stack-æ¶æ§‹ç¸½è¦½)
  - [1.4 ELK åœ¨ Observability æ¶æ§‹ä¸­çš„è§’è‰²](#14-elk-åœ¨-observability-æ¶æ§‹ä¸­çš„è§’è‰²)
  - [1.5 èˆ‡ AI è¼”åŠ©é–‹ç™¼çš„é—œä¿‚](#15-èˆ‡-ai-è¼”åŠ©é–‹ç™¼çš„é—œä¿‚)
- [ç¬¬äºŒç« ï¼šç³»çµ±æ•´é«”æ¶æ§‹è¨­è¨ˆ](#ç¬¬äºŒç« ç³»çµ±æ•´é«”æ¶æ§‹è¨­è¨ˆ)
  - [2.1 ELK Stack æ¶æ§‹åœ–](#21-elk-stack-æ¶æ§‹åœ–)
  - [2.2 å„å…ƒä»¶è§’è‰²èªªæ˜](#22-å„å…ƒä»¶è§’è‰²èªªæ˜)
  - [2.3 å–®ç¯€é» vs å¤šç¯€é»æ¶æ§‹](#23-å–®ç¯€é»-vs-å¤šç¯€é»æ¶æ§‹)
  - [2.4 Production å»ºè­°æ¶æ§‹](#24-production-å»ºè­°æ¶æ§‹)
  - [2.5 èˆ‡ Prometheus / Grafana ä¸¦å­˜æ¶æ§‹](#25-èˆ‡-prometheus--grafana-ä¸¦å­˜æ¶æ§‹)
- [ç¬¬ä¸‰ç« ï¼šç³»çµ±å®‰è£](#ç¬¬ä¸‰ç« ç³»çµ±å®‰è£)
  - [3.1 ç’°å¢ƒéœ€æ±‚ç¸½è¦½](#31-ç’°å¢ƒéœ€æ±‚ç¸½è¦½)
  - [3.2 Elasticsearch å®‰è£](#32-elasticsearch-å®‰è£)
  - [3.3 Logstash å®‰è£](#33-logstash-å®‰è£)
  - [3.4 Kibana å®‰è£](#34-kibana-å®‰è£)
  - [3.5 å¸¸è¦‹å®‰è£å•é¡Œæ’é™¤](#35-å¸¸è¦‹å®‰è£å•é¡Œæ’é™¤)
- [ç¬¬å››ç« ï¼šç³»çµ±è¨­å®š](#ç¬¬å››ç« ç³»çµ±è¨­å®š)
  - [4.1 Elasticsearch è¨­å®š](#41-elasticsearch-è¨­å®š)
  - [4.2 Logstash è¨­å®š](#42-logstash-è¨­å®š)
  - [4.3 Kibana è¨­å®š](#43-kibana-è¨­å®š)
- [ç¬¬äº”ç« ï¼šä¸‰è€…å¦‚ä½•ä¸²æ¥](#ç¬¬äº”ç« ä¸‰è€…å¦‚ä½•ä¸²æ¥)
  - [5.1 End-to-End è³‡æ–™æµ](#51-end-to-end-è³‡æ–™æµ)
  - [5.2 å¯¦éš›ä¸²æ¥ç¯„ä¾‹](#52-å¯¦éš›ä¸²æ¥ç¯„ä¾‹)
  - [5.3 Filebeat æ•´åˆ](#53-filebeat-æ•´åˆ)
- [ç¬¬å…­ç« ï¼šç³»çµ±ä½¿ç”¨](#ç¬¬å…­ç« ç³»çµ±ä½¿ç”¨)
  - [6.1 Kibana æ“ä½œæ•™å­¸](#61-kibana-æ“ä½œæ•™å­¸)
  - [6.2 æŸ¥è©¢èªæ³•è©³è§£](#62-æŸ¥è©¢èªæ³•è©³è§£)
  - [6.3 å¯¦å‹™ä½¿ç”¨æƒ…å¢ƒ](#63-å¯¦å‹™ä½¿ç”¨æƒ…å¢ƒ)
- [ç¬¬ä¸ƒç« ï¼šç³»çµ±ç¶­è­·](#ç¬¬ä¸ƒç« ç³»çµ±ç¶­è­·)
  - [7.1 Index ç®¡ç†ç­–ç•¥](#71-index-ç®¡ç†ç­–ç•¥)
  - [7.2 æ•ˆèƒ½èª¿æ ¡](#72-æ•ˆèƒ½èª¿æ ¡)
  - [7.3 å¥åº·æª¢æŸ¥èˆ‡ç›£æ§](#73-å¥åº·æª¢æŸ¥èˆ‡ç›£æ§)
- [ç¬¬å…«ç« ï¼šç³»çµ±å‡ç´š](#ç¬¬å…«ç« ç³»çµ±å‡ç´š)
  - [8.1 å‡ç´šå‰æº–å‚™](#81-å‡ç´šå‰æº–å‚™)
  - [8.2 å„å…ƒä»¶å‡ç´šæµç¨‹](#82-å„å…ƒä»¶å‡ç´šæµç¨‹)
  - [8.3 å›å¾©ç­–ç•¥](#83-å›å¾©ç­–ç•¥)
- [ç¬¬ä¹ç« ï¼šå®‰å…¨æ€§èˆ‡æ¬Šé™ç®¡ç†](#ç¬¬ä¹ç« å®‰å…¨æ€§èˆ‡æ¬Šé™ç®¡ç†)
  - [9.1 Security åŸºæœ¬æ¦‚å¿µ](#91-security-åŸºæœ¬æ¦‚å¿µ)
  - [9.2 ä½¿ç”¨è€…èˆ‡è§’è‰²ç®¡ç†](#92-ä½¿ç”¨è€…èˆ‡è§’è‰²ç®¡ç†)
  - [9.3 ä¼æ¥­è³‡å®‰è€ƒé‡](#93-ä¼æ¥­è³‡å®‰è€ƒé‡)
- [ç¬¬åç« ï¼šæœ€ä½³å¯¦å‹™èˆ‡å°å…¥å»ºè­°](#ç¬¬åç« æœ€ä½³å¯¦å‹™èˆ‡å°å…¥å»ºè­°)
  - [10.1 å°å…¥å¸¸è¦‹è¸©é›·é»](#101-å°å…¥å¸¸è¦‹è¸©é›·é»)
  - [10.2 çµæ§‹åŒ– Log è¨­è¨ˆåŸå‰‡](#102-çµæ§‹åŒ–-log-è¨­è¨ˆåŸå‰‡)
  - [10.3 èˆ‡ AI åˆ†æçµåˆ](#103-èˆ‡-ai-åˆ†æçµåˆ)
  - [10.4 èˆ‡ Prometheus / Grafana åˆ†å·¥](#104-èˆ‡-prometheus--grafana-åˆ†å·¥)
- [é™„éŒ„ï¼šæª¢æŸ¥æ¸…å–®](#é™„éŒ„æª¢æŸ¥æ¸…å–®)
  - [å®‰è£æª¢æŸ¥æ¸…å–®](#å®‰è£æª¢æŸ¥æ¸…å–®)
  - [è¨­å®šæª¢æŸ¥æ¸…å–®](#è¨­å®šæª¢æŸ¥æ¸…å–®)
  - [ä¸Šç·šæª¢æŸ¥æ¸…å–®](#ä¸Šç·šæª¢æŸ¥æ¸…å–®)
  - [ç¶­é‹æª¢æŸ¥æ¸…å–®ï¼ˆæ¯æ—¥ï¼‰](#ç¶­é‹æª¢æŸ¥æ¸…å–®æ¯æ—¥)
  - [å‡ç´šæª¢æŸ¥æ¸…å–®](#å‡ç´šæª¢æŸ¥æ¸…å–®)
- [åƒè€ƒè³‡æº](#åƒè€ƒè³‡æº)

---

## ç¬¬ä¸€ç« ï¼šLogs Visualization èˆ‡ ELK Stack æ¦‚è¿°

### 1.1 ç‚ºä»€éº¼éœ€è¦ Logs Visualization

åœ¨ç¾ä»£ä¼æ¥­ç´šç³»çµ±ä¸­ï¼ŒLog æ˜¯ç³»çµ±é‹è¡Œçš„ã€Œé»‘ç›’å­è¨˜éŒ„å™¨ã€ï¼Œè¨˜éŒ„äº†ç³»çµ±æ¯ä¸€å€‹é—œéµæ™‚åˆ»çš„ç‹€æ…‹èˆ‡è¡Œç‚ºã€‚

#### å‚³çµ± Log ç®¡ç†çš„ç—›é»

| ç—›é» | èªªæ˜ |
|------|------|
| **åˆ†æ•£å„²å­˜** | Log æ•£è½åœ¨å„å°ä¼ºæœå™¨ï¼ŒæŸ¥è©¢å›°é›£ |
| **æ ¼å¼ä¸ä¸€** | å„ç³»çµ± Log æ ¼å¼ä¸çµ±ä¸€ï¼Œé›£ä»¥åˆ†æ |
| **æŸ¥è©¢å›°é›£** | åªèƒ½ç”¨ `grep`ã€`tail` ç­‰æŒ‡ä»¤ï¼Œæ•ˆç‡ä½ä¸‹ |
| **ç„¡æ³•é—œè¯** | è·¨ç³»çµ±å•é¡Œè¿½è¹¤å›°é›£ï¼Œç„¡æ³•å¿«é€Ÿå®šä½æ ¹å›  |
| **ä¿å­˜æœŸé™** | ç£ç¢Ÿç©ºé–“æœ‰é™ï¼Œæ­·å² Log é›£ä»¥ä¿å­˜ |

#### Logs Visualization å¸¶ä¾†çš„åƒ¹å€¼

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Logs Visualization åƒ¹å€¼                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ… é›†ä¸­ç®¡ç†ï¼šæ‰€æœ‰ Log çµ±ä¸€æ”¶é›†ã€å„²å­˜ã€æŸ¥è©¢                    â”‚
â”‚  âœ… å¿«é€Ÿæœå°‹ï¼šç§’ç´šæŸ¥è©¢ TB ç´š Log è³‡æ–™                         â”‚
â”‚  âœ… è¦–è¦ºåŒ–åˆ†æï¼šDashboard å‘ˆç¾è¶¨å‹¢èˆ‡ç•°å¸¸                      â”‚
â”‚  âœ… å³æ™‚å‘Šè­¦ï¼šç•°å¸¸ Log Pattern è‡ªå‹•é€šçŸ¥                       â”‚
â”‚  âœ… æ­·å²å›æº¯ï¼šå®Œæ•´ä¿å­˜ï¼Œæ”¯æ´ç¨½æ ¸èˆ‡äº‹æ•…åˆ†æ                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### å¯¦å‹™æ¡ˆä¾‹

> **æƒ…å¢ƒ**ï¼šæŸéŠ€è¡Œæ ¸å¿ƒç³»çµ±ç™¼ç”Ÿäº¤æ˜“å¤±æ•—ï¼Œéœ€åœ¨ 5 åˆ†é˜å…§å®šä½å•é¡Œã€‚
> 
> - **æ²’æœ‰ ELK**ï¼šéœ€ç™»å…¥ 10+ å°ä¼ºæœå™¨ï¼Œé€ä¸€ grep Logï¼Œè€—æ™‚ 30 åˆ†é˜ä»¥ä¸Š
> - **æœ‰ ELK**ï¼šåœ¨ Kibana è¼¸å…¥ Transaction IDï¼Œ3 ç§’å…§æ‰¾åˆ°å®Œæ•´äº¤æ˜“éˆè·¯

---

### 1.2 Logs èˆ‡ Metrics çš„å·®ç•°èˆ‡äº’è£œ

çµ„ç¹”å·²å°å…¥ Prometheus + Grafana ä½œç‚º Metrics å¹³å°ï¼ŒELK èˆ‡å…¶ç‚ºäº’è£œé—œä¿‚ï¼š

```mermaid
graph TB
    subgraph "Observability ä¸‰å¤§æ”¯æŸ±"
        M[Metrics<br/>Prometheus + Grafana]
        L[Logs<br/>ELK Stack]
        T[Traces<br/>Jaeger / Zipkin]
    end
    
    M -->|"æ•¸å€¼è¶¨å‹¢<br/>å‘Šè­¦è§¸ç™¼"| Alert[ç™¼ç¾å•é¡Œ]
    Alert -->|"æ·±å…¥åˆ†æ"| L
    L -->|"è¿½è¹¤è«‹æ±‚éˆè·¯"| T
    
    style M fill:#e1f5fe
    style L fill:#fff3e0
    style T fill:#f3e5f5
```

#### å°æ¯”è¡¨

| é¢å‘ | Metrics (Prometheus) | Logs (ELK) |
|------|---------------------|------------|
| **è³‡æ–™é¡å‹** | æ•¸å€¼å‹ï¼ˆCounterã€Gaugeã€Histogramï¼‰ | æ–‡å­—å‹ï¼ˆäº‹ä»¶ã€è¨Šæ¯ã€å †ç–Šï¼‰ |
| **ç”¨é€”** | è¶¨å‹¢ç›£æ§ã€å‘Šè­¦ã€å®¹é‡è¦åŠƒ | å•é¡Œæ’æŸ¥ã€ç¨½æ ¸ã€è¡Œç‚ºåˆ†æ |
| **æŸ¥è©¢æ–¹å¼** | PromQLï¼ˆèšåˆæŸ¥è©¢ï¼‰ | KQL / Luceneï¼ˆå…¨æ–‡æª¢ç´¢ï¼‰ |
| **è³‡æ–™é‡** | ç›¸å°å°ï¼ˆèšåˆå¾Œçš„æ•¸å€¼ï¼‰ | ç›¸å°å¤§ï¼ˆå®Œæ•´æ–‡å­—å…§å®¹ï¼‰ |
| **ä¿å­˜é€±æœŸ** | é€šå¸¸ 15-90 å¤© | ä¾æ³•è¦ 30 å¤©è‡³æ•¸å¹´ |
| **å…¸å‹å•é¡Œ** | ã€Œç³»çµ± CPU ä½•æ™‚é£†é«˜ï¼Ÿã€ | ã€ŒCPU é£†é«˜æ™‚ç™¼ç”Ÿä»€éº¼äº‹ï¼Ÿã€ |

#### äº’è£œä½¿ç”¨æµç¨‹

```mermaid
sequenceDiagram
    participant G as Grafana
    participant P as Prometheus
    participant K as Kibana
    participant E as Elasticsearch
    
    Note over G,E: å•é¡Œç™¼ç¾èˆ‡åˆ†ææµç¨‹
    G->>P: 1. Dashboard é¡¯ç¤ºéŒ¯èª¤ç‡ä¸Šå‡
    P->>G: 2. Alert è§¸ç™¼é€šçŸ¥
    G->>K: 3. é»æ“Šé€£çµè·³è½‰ Kibana
    K->>E: 4. æŸ¥è©¢åŒæ™‚æ®µ Error Log
    E->>K: 5. è¿”å›è©³ç´°éŒ¯èª¤å †ç–Š
    K->>K: 6. å®šä½æ ¹å› 
```

---

### 1.3 ELK Stack æ¶æ§‹ç¸½è¦½

ELK Stack ç”±ä¸‰å€‹æ ¸å¿ƒå…ƒä»¶çµ„æˆï¼š

```mermaid
graph LR
    subgraph "è³‡æ–™ä¾†æº"
        A1[Application Log]
        A2[System Log]
        A3[Access Log]
    end
    
    subgraph "ELK Stack"
        L[Logstash<br/>æ”¶é›† & è™•ç†]
        E[Elasticsearch<br/>å„²å­˜ & ç´¢å¼•]
        K[Kibana<br/>è¦–è¦ºåŒ– & æŸ¥è©¢]
    end
    
    A1 --> L
    A2 --> L
    A3 --> L
    L --> E
    E --> K
    
    style L fill:#f9ca24
    style E fill:#6ab04c
    style K fill:#eb4d4b
```

#### å…ƒä»¶ç°¡ä»‹

| å…ƒä»¶ | è§’è‰² | é¡æ¯” |
|------|------|------|
| **Logstash** | è³‡æ–™æ”¶é›†èˆ‡è™•ç†å¼•æ“ | ETL å·¥å…· |
| **Elasticsearch** | åˆ†æ•£å¼æœå°‹èˆ‡åˆ†æå¼•æ“ | è³‡æ–™åº« + æœå°‹å¼•æ“ |
| **Kibana** | è¦–è¦ºåŒ–èˆ‡ç®¡ç†ä»‹é¢ | BI å ±è¡¨å·¥å…· |

---

### 1.4 ELK åœ¨ Observability æ¶æ§‹ä¸­çš„è§’è‰²

```mermaid
graph TB
    subgraph "æ‡‰ç”¨å±¤"
        App[Java / Spring Boot æ‡‰ç”¨]
    end
    
    subgraph "Observability Platform"
        subgraph "Metrics"
            Prom[Prometheus]
            Graf[Grafana]
        end
        
        subgraph "Logs"
            LS[Logstash]
            ES[Elasticsearch]
            Kib[Kibana]
        end
        
        subgraph "Traces"
            Jaeg[Jaeger]
        end
    end
    
    subgraph "å‘Šè­¦èˆ‡é€šçŸ¥"
        AM[Alertmanager]
        Teams[MS Teams]
    end
    
    App -->|"Metrics"| Prom
    App -->|"Logs"| LS
    App -->|"Traces"| Jaeg
    
    Prom --> Graf
    LS --> ES --> Kib
    
    Prom --> AM --> Teams
    ES -->|"Watcher"| Teams
    
    style ES fill:#6ab04c
    style Prom fill:#e17055
```

#### Observability åˆ†å±¤è·è²¬

| å±¤ç´š | å·¥å…· | å›ç­”çš„å•é¡Œ |
|------|------|-----------|
| **What** | Metrics (Prometheus) | ç™¼ç”Ÿäº†ä»€éº¼ï¼Ÿï¼ˆéŒ¯èª¤ç‡ä¸Šå‡ï¼‰ |
| **Why** | Logs (ELK) | ç‚ºä»€éº¼ç™¼ç”Ÿï¼Ÿï¼ˆNullPointerExceptionï¼‰ |
| **Where** | Traces (Jaeger) | åœ¨å“ªè£¡ç™¼ç”Ÿï¼Ÿï¼ˆService A â†’ Service Bï¼‰ |

---

### 1.5 èˆ‡ AI è¼”åŠ©é–‹ç™¼çš„é—œä¿‚

ELK æ”¶é›†çš„çµæ§‹åŒ– Log æ˜¯ AI åˆ†æçš„çµ•ä½³è³‡æ–™ä¾†æºï¼š

#### AI è¼”åŠ©å ´æ™¯

```mermaid
graph LR
    subgraph "Log è³‡æ–™"
        E[Elasticsearch]
    end
    
    subgraph "AI æ‡‰ç”¨"
        A1[ç•°å¸¸åµæ¸¬<br/>Anomaly Detection]
        A2[æ ¹å› åˆ†æ<br/>Root Cause Analysis]
        A3[é æ¸¬æ€§ç¶­è­·<br/>Predictive Maintenance]
        A4[è‡ªç„¶èªè¨€æŸ¥è©¢<br/>NL to Query]
    end
    
    E --> A1
    E --> A2
    E --> A3
    E --> A4
```

#### å¯¦å‹™æ‡‰ç”¨ç¯„ä¾‹

**1. è‡ªç„¶èªè¨€æŸ¥è©¢**
```
ğŸ‘¤ ä½¿ç”¨è€…å•ï¼šã€Œæ˜¨å¤©ä¸‹åˆ 3 é»åˆ° 4 é»ï¼Œè¨‚å–®æœå‹™æœ‰å¤šå°‘ Errorï¼Ÿã€

ğŸ¤– AI è½‰æ›ç‚º KQLï¼š
service.name: "order-service" AND level: "ERROR" 
AND @timestamp >= "2026-01-26T15:00:00" 
AND @timestamp < "2026-01-26T16:00:00"
```

**2. ç•°å¸¸ Pattern è­˜åˆ¥**
```
ğŸ¤– AI åˆ†æçµæœï¼š
ã€Œåµæ¸¬åˆ°ç•°å¸¸ Patternï¼šæ¯é€±ä¸‰ 14:00-14:30 æœŸé–“ï¼Œ
  payment-service çš„ Connection Timeout éŒ¯èª¤ç‡ä¸Šå‡ 300%ã€‚
  å»ºè­°æª¢æŸ¥è©²æ™‚æ®µæ˜¯å¦æœ‰æ’ç¨‹ä»»å‹™é€ æˆè³‡æºç«¶çˆ­ã€‚ã€
```

**3. æ ¹å› åˆ†æè¼”åŠ©**
```
ğŸ‘¤ å•ï¼šã€Œäº¤æ˜“ TXN-20260127-001 ç‚ºä»€éº¼å¤±æ•—ï¼Ÿã€

ğŸ¤– AI åˆ†æ Log éˆè·¯å¾Œå›ç­”ï¼š
ã€Œäº¤æ˜“å¤±æ•—æ ¹å› ï¼š
  1. order-service æ”¶åˆ°è«‹æ±‚ (14:32:01.123)
  2. å‘¼å« inventory-service æª¢æŸ¥åº«å­˜ (14:32:01.456)
  3. inventory-service å›æ‡‰ timeout (14:32:06.789) â† ç“¶é ¸
  4. order-service æ‹‹å‡º ServiceUnavailableException
  
  å»ºè­°ï¼šæª¢æŸ¥ inventory-service è©²æ™‚æ®µçš„è³‡æºä½¿ç”¨ç‹€æ³ã€
```

---

> ğŸ’¡ **æœ¬ç« é‡é»**
> - Logs Visualization è§£æ±ºå‚³çµ± Log ç®¡ç†çš„åˆ†æ•£ã€é›£æŸ¥ã€é›£é—œè¯å•é¡Œ
> - ELK èˆ‡ Prometheus/Grafana æ˜¯äº’è£œé—œä¿‚ï¼Œå…±åŒæ§‹æˆå®Œæ•´ Observability
> - ELK æ”¶é›†çš„çµæ§‹åŒ– Log æ˜¯ AI åˆ†æçš„é‡è¦è³‡æ–™ä¾†æº

---

## ç¬¬äºŒç« ï¼šç³»çµ±æ•´é«”æ¶æ§‹è¨­è¨ˆ

### 2.1 ELK Stack æ¶æ§‹åœ–

#### åŸºç¤æ¶æ§‹ï¼ˆå°å‹ç’°å¢ƒï¼‰

```mermaid
graph TB
    subgraph "Application Servers"
        App1[App Server 1]
        App2[App Server 2]
        App3[App Server 3]
    end
    
    subgraph "ELK Stack - Single Node"
        LS[Logstash]
        ES[Elasticsearch]
        K[Kibana]
    end
    
    App1 -->|"Log File / TCP"| LS
    App2 -->|"Log File / TCP"| LS
    App3 -->|"Log File / TCP"| LS
    
    LS -->|"Index"| ES
    ES -->|"Query"| K
    
    User[ä½¿ç”¨è€…] --> K
```

#### ä¼æ¥­ç´šæ¶æ§‹ï¼ˆå¤§å‹ç’°å¢ƒï¼‰

```mermaid
graph TB
    subgraph "Application Layer"
        App1[App Server 1]
        App2[App Server 2]
        App3[App Server N...]
    end
    
    subgraph "Collection Layer"
        FB1[Filebeat 1]
        FB2[Filebeat 2]
        FB3[Filebeat N]
    end
    
    subgraph "Buffer Layer"
        Kafka[Apache Kafka]
    end
    
    subgraph "Processing Layer"
        LS1[Logstash 1]
        LS2[Logstash 2]
    end
    
    subgraph "Storage Layer - ES Cluster"
        ES1[ES Master 1]
        ES2[ES Master 2]
        ES3[ES Master 3]
        ES4[ES Data 1]
        ES5[ES Data 2]
        ES6[ES Data N...]
    end
    
    subgraph "Presentation Layer"
        K1[Kibana 1]
        K2[Kibana 2]
        LB[Load Balancer]
    end
    
    App1 --> FB1
    App2 --> FB2
    App3 --> FB3
    
    FB1 --> Kafka
    FB2 --> Kafka
    FB3 --> Kafka
    
    Kafka --> LS1
    Kafka --> LS2
    
    LS1 --> ES4
    LS2 --> ES5
    
    ES1 --- ES2 --- ES3
    ES4 --- ES5 --- ES6
    
    ES4 --> K1
    ES5 --> K2
    
    K1 --> LB
    K2 --> LB
    
    User[ä½¿ç”¨è€…] --> LB
```

---

### 2.2 å„å…ƒä»¶è§’è‰²èªªæ˜

#### Logstash - è³‡æ–™è™•ç†å¼•æ“

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Logstash Pipeline                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   INPUT     â”‚        FILTER           â”‚      OUTPUT        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ file      â”‚ â€¢ grok (æ­£è¦è¡¨é”å¼è§£æ)  â”‚ â€¢ elasticsearch   â”‚
â”‚ â€¢ beats     â”‚ â€¢ mutate (æ¬„ä½ä¿®æ”¹)      â”‚ â€¢ file            â”‚
â”‚ â€¢ tcp/udp   â”‚ â€¢ date (æ™‚é–“è§£æ)        â”‚ â€¢ kafka           â”‚
â”‚ â€¢ kafka     â”‚ â€¢ geoip (åœ°ç†ä½ç½®)       â”‚ â€¢ stdout          â”‚
â”‚ â€¢ jdbc      â”‚ â€¢ useragent             â”‚ â€¢ email           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ¸å¿ƒåŠŸèƒ½**ï¼š
- å¾å¤šç¨®ä¾†æºæ”¶é›†è³‡æ–™
- è§£æã€è½‰æ›ã€enrichment
- è¼¸å‡ºåˆ°å¤šç¨®ç›®çš„åœ°

#### Elasticsearch - åˆ†æ•£å¼æœå°‹å¼•æ“

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Elasticsearch Cluster                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Master     â”‚  â”‚  Master     â”‚  â”‚  Master     â”‚         â”‚
â”‚  â”‚  Node 1     â”‚  â”‚  Node 2     â”‚  â”‚  Node 3     â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚         â”‚               â”‚               â”‚                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚              Cluster State                   â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Data       â”‚  â”‚  Data       â”‚  â”‚  Data       â”‚         â”‚
â”‚  â”‚  Node 1     â”‚  â”‚  Node 2     â”‚  â”‚  Node N     â”‚         â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚
â”‚  â”‚  â”‚Shard 0â”‚  â”‚  â”‚  â”‚Shard 1â”‚  â”‚  â”‚  â”‚Shard 2â”‚  â”‚         â”‚
â”‚  â”‚  â”‚Primaryâ”‚  â”‚  â”‚  â”‚Primaryâ”‚  â”‚  â”‚  â”‚Primaryâ”‚  â”‚         â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚
â”‚  â”‚  â”‚Shard 1â”‚  â”‚  â”‚  â”‚Shard 2â”‚  â”‚  â”‚  â”‚Shard 0â”‚  â”‚         â”‚
â”‚  â”‚  â”‚Replicaâ”‚  â”‚  â”‚  â”‚Replicaâ”‚  â”‚  â”‚  â”‚Replicaâ”‚  â”‚         â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ¸å¿ƒæ¦‚å¿µ**ï¼š
| æ¦‚å¿µ | èªªæ˜ |
|------|------|
| **Cluster** | å¤šå€‹ Node çµ„æˆçš„å¢é›† |
| **Node** | å–®ä¸€ Elasticsearch å¯¦ä¾‹ |
| **Index** | é¡ä¼¼è³‡æ–™åº«çš„ Table |
| **Shard** | Index çš„æ°´å¹³åˆ†å‰²å–®ä½ |
| **Replica** | Shard çš„å‰¯æœ¬ï¼Œæä¾› HA |

#### Kibana - è¦–è¦ºåŒ–å¹³å°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Kibana åŠŸèƒ½æ¨¡çµ„                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Discover   â”‚  Visualize   â”‚  Dashboard   â”‚   Management  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Log æŸ¥è©¢   â”‚ â€¢ åœ–è¡¨å»ºç«‹   â”‚ â€¢ å„€è¡¨æ¿çµ„åˆ â”‚ â€¢ Index ç®¡ç†  â”‚
â”‚ â€¢ å…¨æ–‡æª¢ç´¢   â”‚ â€¢ å¤šç¨®åœ–å‹   â”‚ â€¢ å³æ™‚æ›´æ–°   â”‚ â€¢ ä½¿ç”¨è€…ç®¡ç†  â”‚
â”‚ â€¢ æ™‚é–“ç¯©é¸   â”‚ â€¢ èšåˆåˆ†æ   â”‚ â€¢ åˆ†äº«åŒ¯å‡º   â”‚ â€¢ ç©ºé–“ç®¡ç†    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2.3 å–®ç¯€é» vs å¤šç¯€é»æ¶æ§‹

#### æ¶æ§‹é¸æ“‡æ±ºç­–è¡¨

| é¢å‘ | å–®ç¯€é» | å¤šç¯€é»å¢é›† |
|------|--------|-----------|
| **é©ç”¨å ´æ™¯** | é–‹ç™¼ã€æ¸¬è©¦ã€POC | æ­£å¼ç’°å¢ƒã€å¤§æµé‡ |
| **Log é‡** | < 10 GB/å¤© | > 10 GB/å¤© |
| **å¯ç”¨æ€§** | ç„¡ HA | æ”¯æ´ HA |
| **æ“´å±•æ€§** | æœ‰é™ | æ°´å¹³æ“´å±• |
| **æˆæœ¬** | ä½ | è¼ƒé«˜ |
| **ç¶­é‹è¤‡é›œåº¦** | ç°¡å–® | è¼ƒè¤‡é›œ |

#### ç¯€é»é¡å‹èªªæ˜

```mermaid
graph TB
    subgraph "Elasticsearch Node Types"
        M[Master Node<br/>å¢é›†ç®¡ç†]
        D[Data Node<br/>è³‡æ–™å„²å­˜]
        I[Ingest Node<br/>è³‡æ–™è™•ç†]
        C[Coordinating Node<br/>è«‹æ±‚è·¯ç”±]
    end
    
    M -->|"ç®¡ç†"| D
    C -->|"è·¯ç”±æŸ¥è©¢"| D
    I -->|"è™•ç†å¾Œå¯«å…¥"| D
```

| ç¯€é»é¡å‹ | è·è²¬ | å»ºè­°æ•¸é‡ |
|---------|------|---------|
| **Master** | å¢é›†ç‹€æ…‹ç®¡ç†ã€Index å»ºç«‹/åˆªé™¤ | 3ï¼ˆå¥‡æ•¸ï¼Œé¿å…è…¦è£‚ï¼‰ |
| **Data** | å„²å­˜è³‡æ–™ã€åŸ·è¡Œ CRUD | ä¾è³‡æ–™é‡èª¿æ•´ |
| **Ingest** | è³‡æ–™å‰è™•ç†ï¼ˆé¡ä¼¼è¼•é‡ Logstashï¼‰ | é¸é… |
| **Coordinating** | è«‹æ±‚è·¯ç”±ã€çµæœèšåˆ | é¸é…ï¼ˆé«˜æŸ¥è©¢é‡æ™‚ï¼‰ |

---

### 2.4 Production å»ºè­°æ¶æ§‹

#### ä¸­å‹ä¼æ¥­å»ºè­°æ¶æ§‹ï¼ˆæ—¥èªŒé‡ 50-200 GB/å¤©ï¼‰

```mermaid
graph TB
    subgraph "Collection"
        FB[Filebeat x N]
    end
    
    subgraph "Processing"
        LS1[Logstash 1]
        LS2[Logstash 2]
    end
    
    subgraph "Elasticsearch Cluster"
        subgraph "Master Nodes"
            M1[Master 1<br/>4 CPU / 8 GB]
            M2[Master 2<br/>4 CPU / 8 GB]
            M3[Master 3<br/>4 CPU / 8 GB]
        end
        
        subgraph "Data Nodes"
            D1[Data 1<br/>8 CPU / 32 GB / 1TB SSD]
            D2[Data 2<br/>8 CPU / 32 GB / 1TB SSD]
            D3[Data 3<br/>8 CPU / 32 GB / 1TB SSD]
        end
    end
    
    subgraph "Visualization"
        K1[Kibana 1]
        K2[Kibana 2]
    end
    
    FB --> LS1
    FB --> LS2
    LS1 --> D1
    LS2 --> D2
    D1 --- D2 --- D3
    M1 --- M2 --- M3
    D1 --> K1
    D2 --> K2
```

#### ç¡¬é«”è¦æ ¼å»ºè­°

| å…ƒä»¶ | CPU | Memory | Disk | æ•¸é‡ |
|------|-----|--------|------|------|
| **ES Master** | 4 cores | 8 GB | 50 GB SSD | 3 |
| **ES Data** | 8 cores | 32 GB | 1 TB SSD | 3+ |
| **Logstash** | 4 cores | 8 GB | 100 GB | 2 |
| **Kibana** | 2 cores | 4 GB | 20 GB | 2 |

#### é—œéµè¨­è¨ˆåŸå‰‡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Production æ¶æ§‹è¨­è¨ˆåŸå‰‡                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Master Node ç¨ç«‹éƒ¨ç½²ï¼Œä¸èˆ‡ Data Node æ··ç”¨               â”‚
â”‚  2. è‡³å°‘ 3 å€‹ Master Nodeï¼ˆé¿å…è…¦è£‚ï¼‰                       â”‚
â”‚  3. Data Node ä½¿ç”¨ SSDï¼Œæå‡ I/O æ•ˆèƒ½                       â”‚
â”‚  4. Logstash éƒ¨ç½² 2+ å°ï¼Œé¿å…å–®é»æ•…éšœ                       â”‚
â”‚  5. è€ƒæ…®åŠ å…¥ Kafka ä½œç‚º Buffer Layer                        â”‚
â”‚  6. Kibana å‰ç«¯åŠ  Load Balancer                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2.5 èˆ‡ Prometheus / Grafana ä¸¦å­˜æ¶æ§‹

```mermaid
graph TB
    subgraph "Application"
        App[Java / Spring Boot]
    end
    
    subgraph "Metrics Pipeline"
        Prom[Prometheus]
        Graf[Grafana]
    end
    
    subgraph "Logs Pipeline"
        FB[Filebeat]
        LS[Logstash]
        ES[Elasticsearch]
        Kib[Kibana]
    end
    
    subgraph "Alerting"
        AM[Alertmanager]
        Teams[MS Teams / Email]
    end
    
    App -->|"/actuator/prometheus"| Prom
    App -->|"Log File"| FB
    
    Prom --> Graf
    FB --> LS --> ES --> Kib
    
    Prom --> AM --> Teams
    ES -->|"Watcher Alert"| Teams
    
    Graf -.->|"Link to Kibana"| Kib
```

#### æ•´åˆç­–ç•¥

| ç­–ç•¥ | èªªæ˜ |
|------|------|
| **çµ±ä¸€æ™‚é–“è»¸** | Grafana èˆ‡ Kibana ä½¿ç”¨ç›¸åŒæ™‚å€è¨­å®š |
| **Deep Link æ•´åˆ** | Grafana Alert é€£çµè·³è½‰è‡³ Kibana æŸ¥è©¢ |
| **å…±ç”¨å‘Šè­¦é€šé“** | çµ±ä¸€ä½¿ç”¨ Alertmanager æˆ– MS Teams |
| **Correlation ID** | Log èˆ‡ Metrics ä½¿ç”¨ç›¸åŒ Trace ID é—œè¯ |

#### Grafana é€£çµ Kibana ç¯„ä¾‹

```yaml
# Grafana Alert é€šçŸ¥ç¯„æœ¬
alerting:
  notification_channels:
    - name: elk-deep-link
      type: webhook
      settings:
        url: "https://kibana.company.com/app/discover#/?_a=(query:(query_string:(query:'traceId:${traceId}')))"
```

---

> ğŸ’¡ **æœ¬ç« é‡é»**
> - å°å‹ç’°å¢ƒå¯ç”¨å–®ç¯€é»ï¼Œæ­£å¼ç’°å¢ƒå»ºè­°å¤šç¯€é»å¢é›†
> - Master Node è‡³å°‘ 3 å°ï¼ŒData Node ä½¿ç”¨ SSD
> - å¤§æµé‡ç’°å¢ƒåŠ å…¥ Kafka ä½œç‚º Buffer Layer
> - èˆ‡ Prometheus/Grafana æ•´åˆä½¿ç”¨ Deep Link èˆ‡ Correlation ID

---

## ç¬¬ä¸‰ç« ï¼šç³»çµ±å®‰è£

### 3.1 ç’°å¢ƒéœ€æ±‚ç¸½è¦½

#### ä½œæ¥­ç³»çµ±æ”¯æ´

| OS | æ”¯æ´ç‹€æ…‹ | å»ºè­° |
|----|---------|------|
| RHEL / CentOS 7, 8 | âœ… å®Œæ•´æ”¯æ´ | ä¼æ¥­é¦–é¸ |
| Ubuntu 18.04, 20.04, 22.04 | âœ… å®Œæ•´æ”¯æ´ | é–‹ç™¼ç’°å¢ƒ |
| Debian 10, 11 | âœ… å®Œæ•´æ”¯æ´ | |
| Windows Server | âš ï¸ æ”¯æ´ä½†ä¸å»ºè­° | åƒ…é™é–‹ç™¼æ¸¬è©¦ |

#### ç‰ˆæœ¬é¸æ“‡åŸå‰‡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ç‰ˆæœ¬é¸æ“‡å»ºè­°                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ… ä¸‰å€‹å…ƒä»¶ä½¿ç”¨ã€Œç›¸åŒä¸»ç‰ˆæœ¬è™Ÿã€ï¼ˆå¦‚éƒ½ç”¨ 8.xï¼‰               â”‚
â”‚  âœ… å„ªå…ˆé¸æ“‡æœ€æ–°çš„ç©©å®šç‰ˆï¼ˆé RC / Betaï¼‰                    â”‚
â”‚  âœ… åƒè€ƒ Elastic å®˜æ–¹ Support Matrix                        â”‚
â”‚  âš ï¸  é¿å…è·¨å¤§ç‰ˆæœ¬æ··ç”¨ï¼ˆå¦‚ ES 8.x + Kibana 7.xï¼‰            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç›®å‰å»ºè­°ç‰ˆæœ¬ï¼š8.12.xï¼ˆ2026 å¹´ 1 æœˆï¼‰
```

#### ç¡¬é«”éœ€æ±‚ï¼ˆå–®ç¯€é»æœ€ä½é…ç½®ï¼‰

| å…ƒä»¶ | CPU | Memory | Disk |
|------|-----|--------|------|
| **Elasticsearch** | 2 cores | 4 GB | 50 GB SSD |
| **Logstash** | 2 cores | 4 GB | 20 GB |
| **Kibana** | 1 core | 2 GB | 10 GB |

#### ç¶²è·¯åŸ éœ€æ±‚

| å…ƒä»¶ | é è¨­åŸ  | ç”¨é€” |
|------|--------|------|
| Elasticsearch | 9200 | HTTP API |
| Elasticsearch | 9300 | ç¯€é»é–“é€šè¨Š |
| Logstash | 5044 | Beats Input |
| Logstash | 9600 | Monitoring API |
| Kibana | 5601 | Web UI |

---

### 3.2 Elasticsearch å®‰è£

#### æ–¹æ³•ä¸€ï¼šRPM å®‰è£ï¼ˆRHEL / CentOSï¼‰

```bash
# 1. åŒ¯å…¥ GPG Key
sudo rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch

# 2. å»ºç«‹ Repo æª”æ¡ˆ
sudo tee /etc/yum.repos.d/elasticsearch.repo << EOF
[elasticsearch]
name=Elasticsearch repository for 8.x packages
baseurl=https://artifacts.elastic.co/packages/8.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md
EOF

# 3. å®‰è£ Elasticsearch
sudo yum install -y elasticsearch

# 4. å•Ÿå‹•æœå‹™
sudo systemctl daemon-reload
sudo systemctl enable elasticsearch
sudo systemctl start elasticsearch

# 5. é©—è­‰å®‰è£ï¼ˆéœ€ç­‰å¾…ç´„ 30 ç§’ï¼‰
curl -X GET "localhost:9200" -u elastic:your_password
```

#### æ–¹æ³•äºŒï¼šDEB å®‰è£ï¼ˆUbuntu / Debianï¼‰

```bash
# 1. å®‰è£å¿…è¦å¥—ä»¶
sudo apt-get install -y apt-transport-https

# 2. åŒ¯å…¥ GPG Key
wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg

# 3. æ–°å¢ Repository
echo "deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/8.x/apt stable main" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list

# 4. å®‰è£
sudo apt-get update && sudo apt-get install -y elasticsearch

# 5. å•Ÿå‹•æœå‹™
sudo systemctl daemon-reload
sudo systemctl enable elasticsearch
sudo systemctl start elasticsearch
```

#### æ–¹æ³•ä¸‰ï¼šDocker å®‰è£

```bash
# å»ºç«‹ç¶²è·¯
docker network create elastic

# å•Ÿå‹• Elasticsearch
docker run -d \
  --name elasticsearch \
  --net elastic \
  -p 9200:9200 \
  -p 9300:9300 \
  -e "discovery.type=single-node" \
  -e "xpack.security.enabled=false" \
  -e "ES_JAVA_OPTS=-Xms2g -Xmx2g" \
  docker.elastic.co/elasticsearch/elasticsearch:8.12.0
```

#### å®‰è£å¾Œé©—è­‰

```bash
# æª¢æŸ¥æœå‹™ç‹€æ…‹
sudo systemctl status elasticsearch

# æª¢æŸ¥ Cluster Health
curl -X GET "localhost:9200/_cluster/health?pretty"

# é æœŸè¼¸å‡º
{
  "cluster_name" : "elasticsearch",
  "status" : "green",
  "number_of_nodes" : 1,
  ...
}
```

---

### 3.3 Logstash å®‰è£

#### RPM å®‰è£

```bash
# 1. å®‰è£ Logstashï¼ˆä½¿ç”¨å‰é¢å»ºç«‹çš„ Repoï¼‰
sudo yum install -y logstash

# 2. å»ºç«‹åŸºæœ¬è¨­å®šæª”
sudo tee /etc/logstash/conf.d/basic.conf << 'EOF'
input {
  beats {
    port => 5044
  }
}

filter {
  # åŸºæœ¬è™•ç†
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
}
EOF

# 3. æ¸¬è©¦è¨­å®šæª”èªæ³•
sudo /usr/share/logstash/bin/logstash --config.test_and_exit -f /etc/logstash/conf.d/basic.conf

# 4. å•Ÿå‹•æœå‹™
sudo systemctl enable logstash
sudo systemctl start logstash
```

#### DEB å®‰è£

```bash
# å®‰è£ï¼ˆä½¿ç”¨å‰é¢å»ºç«‹çš„ Repositoryï¼‰
sudo apt-get install -y logstash

# è¨­å®šèˆ‡å•Ÿå‹•åŒä¸Š
```

#### Docker å®‰è£

```bash
# å»ºç«‹è¨­å®šæª”ç›®éŒ„
mkdir -p ~/logstash/pipeline

# å»ºç«‹ Pipeline è¨­å®š
cat > ~/logstash/pipeline/logstash.conf << 'EOF'
input {
  beats {
    port => 5044
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
}
EOF

# å•Ÿå‹• Logstash
docker run -d \
  --name logstash \
  --net elastic \
  -p 5044:5044 \
  -v ~/logstash/pipeline:/usr/share/logstash/pipeline \
  docker.elastic.co/logstash/logstash:8.12.0
```

#### å®‰è£é©—è­‰

```bash
# æª¢æŸ¥æœå‹™ç‹€æ…‹
sudo systemctl status logstash

# æª¢æŸ¥ Log
sudo tail -f /var/log/logstash/logstash-plain.log

# æª¢æŸ¥ APIï¼ˆé è¨­ 9600ï¼‰
curl -X GET "localhost:9600/_node/stats?pretty"
```

---

### 3.4 Kibana å®‰è£

#### RPM å®‰è£

```bash
# 1. å®‰è£ Kibana
sudo yum install -y kibana

# 2. è¨­å®šé€£ç·š Elasticsearch
sudo tee -a /etc/kibana/kibana.yml << EOF
server.host: "0.0.0.0"
server.port: 5601
elasticsearch.hosts: ["http://localhost:9200"]
EOF

# 3. å•Ÿå‹•æœå‹™
sudo systemctl enable kibana
sudo systemctl start kibana
```

#### DEB å®‰è£

```bash
sudo apt-get install -y kibana

# è¨­å®šèˆ‡å•Ÿå‹•åŒä¸Š
```

#### Docker å®‰è£

```bash
docker run -d \
  --name kibana \
  --net elastic \
  -p 5601:5601 \
  -e "ELASTICSEARCH_HOSTS=http://elasticsearch:9200" \
  docker.elastic.co/kibana/kibana:8.12.0
```

#### å®‰è£é©—è­‰

```bash
# æª¢æŸ¥æœå‹™ç‹€æ…‹
sudo systemctl status kibana

# ç­‰å¾…ç´„ 1-2 åˆ†é˜å¾Œï¼Œé–‹å•Ÿç€è¦½å™¨
# http://your-server-ip:5601
```

---

### 3.5 å¸¸è¦‹å®‰è£å•é¡Œæ’é™¤

#### å•é¡Œä¸€ï¼šElasticsearch ç„¡æ³•å•Ÿå‹•

```bash
# æŸ¥çœ‹éŒ¯èª¤æ—¥èªŒ
sudo journalctl -u elasticsearch -f

# å¸¸è¦‹åŸå›  1ï¼šè¨˜æ†¶é«”ä¸è¶³
# è§£æ±ºï¼šèª¿æ•´ JVM Heap
sudo vi /etc/elasticsearch/jvm.options
# ä¿®æ”¹ -Xms å’Œ -Xmxï¼ˆå»ºè­°è¨­ç‚ºå¯¦é«”è¨˜æ†¶é«”çš„ 50%ï¼Œä½†ä¸è¶…é 31GBï¼‰
-Xms2g
-Xmx2g

# å¸¸è¦‹åŸå›  2ï¼šæª”æ¡ˆæè¿°ç¬¦é™åˆ¶
# è§£æ±ºï¼šå¢åŠ é™åˆ¶
sudo tee -a /etc/security/limits.conf << EOF
elasticsearch soft nofile 65536
elasticsearch hard nofile 65536
EOF

# å¸¸è¦‹åŸå›  3ï¼šè™›æ“¬è¨˜æ†¶é«”é™åˆ¶
# è§£æ±ºï¼šå¢åŠ  mmap æ•¸é‡
sudo sysctl -w vm.max_map_count=262144
# æ°¸ä¹…ç”Ÿæ•ˆ
echo "vm.max_map_count=262144" | sudo tee -a /etc/sysctl.conf
```

#### å•é¡ŒäºŒï¼šLogstash Pipeline éŒ¯èª¤

```bash
# æ¸¬è©¦è¨­å®šæª”èªæ³•
sudo /usr/share/logstash/bin/logstash --config.test_and_exit -f /etc/logstash/conf.d/

# æŸ¥çœ‹è©³ç´°éŒ¯èª¤
sudo /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/ --log.level=debug
```

#### å•é¡Œä¸‰ï¼šKibana ç„¡æ³•é€£ç·š Elasticsearch

```bash
# æª¢æŸ¥ Elasticsearch æ˜¯å¦é‹ä½œ
curl -X GET "localhost:9200"

# æª¢æŸ¥ Kibana Log
sudo tail -f /var/log/kibana/kibana.log

# ç¢ºèªè¨­å®š
sudo cat /etc/kibana/kibana.yml | grep elasticsearch

# å¸¸è¦‹åŸå› ï¼šSecurity å•Ÿç”¨ä½†æœªè¨­å®šèªè­‰
# è§£æ±ºï¼šåœ¨ kibana.yml åŠ å…¥
elasticsearch.username: "kibana_system"
elasticsearch.password: "your_password"
```

#### å•é¡Œå››ï¼šç£ç¢Ÿç©ºé–“ä¸è¶³

```bash
# æª¢æŸ¥ç£ç¢Ÿä½¿ç”¨
df -h

# æ¸…ç†èˆŠ Indexï¼ˆè¬¹æ…æ“ä½œï¼‰
curl -X DELETE "localhost:9200/logs-2026.01.01"

# è¨­å®šè‡ªå‹•æ¸…ç†ï¼ˆå¾ŒçºŒç« ç¯€è©³è¿°ï¼‰
```

#### å¿«é€Ÿè¨ºæ–·è…³æœ¬

```bash
#!/bin/bash
# elk-health-check.sh

echo "=== Elasticsearch Status ==="
systemctl status elasticsearch --no-pager
curl -s -X GET "localhost:9200/_cluster/health?pretty" 2>/dev/null || echo "ES not responding"

echo ""
echo "=== Logstash Status ==="
systemctl status logstash --no-pager
curl -s -X GET "localhost:9600/?pretty" 2>/dev/null || echo "Logstash not responding"

echo ""
echo "=== Kibana Status ==="
systemctl status kibana --no-pager
curl -s -X GET "localhost:5601/api/status" 2>/dev/null | grep -o '"state":"[^"]*"' || echo "Kibana not responding"

echo ""
echo "=== Disk Usage ==="
df -h | grep -E "Filesystem|/dev/"

echo ""
echo "=== Memory Usage ==="
free -h
```

---

> ğŸ’¡ **æœ¬ç« é‡é»**
> - ä¸‰å€‹å…ƒä»¶å‹™å¿…ä½¿ç”¨ç›¸åŒä¸»ç‰ˆæœ¬è™Ÿ
> - JVM Heap è¨­ç‚ºå¯¦é«”è¨˜æ†¶é«”çš„ 50%ï¼ˆä¸è¶…é 31 GBï¼‰
> - æ³¨æ„ç³»çµ±åƒæ•¸ï¼š`vm.max_map_count`ã€`nofile` é™åˆ¶
> - ä½¿ç”¨ Docker å®‰è£å¯å¿«é€Ÿé©—è­‰ï¼Œä½†æ­£å¼ç’°å¢ƒå»ºè­°ç›´æ¥å®‰è£

---

## ç¬¬å››ç« ï¼šç³»çµ±è¨­å®š

### 4.1 Elasticsearch è¨­å®š

#### ä¸»è¨­å®šæª”ä½ç½®

```
/etc/elasticsearch/elasticsearch.yml    # ä¸»è¨­å®šæª”
/etc/elasticsearch/jvm.options          # JVM è¨­å®š
/etc/elasticsearch/log4j2.properties    # Log è¨­å®š
```

#### åŸºæœ¬è¨­å®šç¯„ä¾‹

```yaml
# /etc/elasticsearch/elasticsearch.yml

# ======================== å¢é›†è¨­å®š ========================
cluster.name: prod-elk-cluster
node.name: es-node-01

# ======================== ç¯€é»è§’è‰² ========================
# å–®ç¯€é»æ¨¡å¼
node.roles: [ master, data, ingest ]

# å¤šç¯€é»æ¨¡å¼ï¼ˆä¾è§’è‰²è¨­å®šï¼‰
# Master Node: node.roles: [ master ]
# Data Node: node.roles: [ data ]

# ======================== è·¯å¾‘è¨­å®š ========================
path.data: /var/lib/elasticsearch
path.logs: /var/log/elasticsearch

# ======================== ç¶²è·¯è¨­å®š ========================
network.host: 0.0.0.0
http.port: 9200
transport.port: 9300

# ======================== å¢é›†ç™¼ç¾ ========================
# å–®ç¯€é»
discovery.type: single-node

# å¤šç¯€é»ï¼ˆåˆ—å‡ºæ‰€æœ‰ Master å€™é¸ç¯€é»ï¼‰
# discovery.seed_hosts:
#   - 192.168.1.10:9300
#   - 192.168.1.11:9300
#   - 192.168.1.12:9300
# cluster.initial_master_nodes:
#   - es-master-01
#   - es-master-02
#   - es-master-03

# ======================== å®‰å…¨æ€§è¨­å®š ========================
xpack.security.enabled: true
xpack.security.enrollment.enabled: true
xpack.security.http.ssl.enabled: false  # å…§ç¶²å¯é—œé–‰
xpack.security.transport.ssl.enabled: true
```

#### JVM è¨­å®š

```bash
# /etc/elasticsearch/jvm.options.d/heap.options

# Heap Sizeï¼ˆè¨­ç‚ºå¯¦é«”è¨˜æ†¶é«”çš„ 50%ï¼Œä½†ä¸è¶…é 31GBï¼‰
-Xms16g
-Xmx16g

# GC è¨­å®šï¼ˆES 8.x é è¨­ä½¿ç”¨ G1GCï¼‰
# é€šå¸¸ä¸éœ€ä¿®æ”¹
```

#### Memory è¨­å®šå»ºè­°

| å¯¦é«”è¨˜æ†¶é«” | Heap Size | èªªæ˜ |
|-----------|-----------|------|
| 8 GB | 4 GB | é–‹ç™¼ç’°å¢ƒ |
| 16 GB | 8 GB | å°å‹ç”Ÿç”¢ |
| 32 GB | 16 GB | ä¸­å‹ç”Ÿç”¢ |
| 64 GB | 31 GB | å¤§å‹ç”Ÿç”¢ï¼ˆä¸è¶…é 31 GBï¼‰ |

> âš ï¸ **é‡è¦**ï¼šHeap ä¸è¦è¶…é 31 GBï¼Œå¦å‰‡ç„¡æ³•ä½¿ç”¨ Compressed OOPsï¼Œæ•ˆèƒ½åè€Œä¸‹é™ã€‚

#### Index åŸºæœ¬æ¦‚å¿µèˆ‡è¨­å®š

```bash
# å»ºç«‹ Index Templateï¼ˆå»ºè­°ä½¿ç”¨ï¼‰
curl -X PUT "localhost:9200/_index_template/logs-template" -H 'Content-Type: application/json' -d'
{
  "index_patterns": ["logs-*"],
  "template": {
    "settings": {
      "number_of_shards": 3,
      "number_of_replicas": 1,
      "index.lifecycle.name": "logs-policy",
      "index.lifecycle.rollover_alias": "logs"
    },
    "mappings": {
      "properties": {
        "@timestamp": { "type": "date" },
        "message": { "type": "text" },
        "level": { "type": "keyword" },
        "service": { "type": "keyword" },
        "traceId": { "type": "keyword" }
      }
    }
  }
}'
```

#### Shard æ•¸é‡è¦åŠƒ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Shard è¦åŠƒå»ºè­°                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ å–®ä¸€ Shard å»ºè­°å¤§å°ï¼š10-50 GB                            â”‚
â”‚  â€¢ æ¯å€‹ Data Node å»ºè­° Shard æ•¸ï¼š< 1000                     â”‚
â”‚  â€¢ Shard æ•¸ = (é ä¼° Index å¤§å°) / (å–®ä¸€ Shard å¤§å°)          â”‚
â”‚                                                             â”‚
â”‚  ç¯„ä¾‹ï¼šæ—¥èªŒé‡ 100 GB/å¤©                                     â”‚
â”‚  â†’ å»ºè­° Shard æ•¸ï¼š100 / 30 â‰ˆ 3-4 å€‹ Primary Shard          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 4.2 Logstash è¨­å®š

#### è¨­å®šæª”çµæ§‹

```
/etc/logstash/
â”œâ”€â”€ logstash.yml           # ä¸»è¨­å®š
â”œâ”€â”€ pipelines.yml          # Pipeline å®šç¾©
â”œâ”€â”€ jvm.options            # JVM è¨­å®š
â””â”€â”€ conf.d/                # Pipeline è¨­å®šæª”ç›®éŒ„
    â”œâ”€â”€ 01-input.conf
    â”œâ”€â”€ 02-filter.conf
    â””â”€â”€ 03-output.conf
```

#### Pipeline æ¶æ§‹

```mermaid
graph LR
    subgraph "Logstash Pipeline"
        I[Input]
        F[Filter]
        O[Output]
    end
    
    Source[è³‡æ–™ä¾†æº] --> I
    I --> F
    F --> O
    O --> Dest[ç›®çš„åœ°]
    
    style I fill:#74b9ff
    style F fill:#fdcb6e
    style O fill:#55efc4
```

#### Input è¨­å®šç¯„ä¾‹

```ruby
# /etc/logstash/conf.d/01-input.conf

# å¾ Filebeat æ¥æ”¶
input {
  beats {
    port => 5044
    ssl => false
  }
}

# å¾ Kafka æ¥æ”¶
input {
  kafka {
    bootstrap_servers => "kafka01:9092,kafka02:9092"
    topics => ["app-logs"]
    group_id => "logstash-consumer"
    codec => json
  }
}

# å¾ TCP æ¥æ”¶ï¼ˆLog4j2ï¼‰
input {
  tcp {
    port => 5000
    codec => json_lines
  }
}
```

#### Filter è¨­å®šç¯„ä¾‹

```ruby
# /etc/logstash/conf.d/02-filter.conf

filter {
  # è§£æ Java Logï¼ˆLog4j2 Patternï¼‰
  if [type] == "java-app" {
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} \[%{DATA:thread}\] %{JAVACLASS:class} - %{GREEDYDATA:logMessage}"
      }
    }
    
    # è§£ææ™‚é–“
    date {
      match => [ "timestamp", "yyyy-MM-dd HH:mm:ss.SSS" ]
      target => "@timestamp"
      timezone => "Asia/Taipei"
    }
    
    # ç§»é™¤åŸå§‹ timestamp æ¬„ä½
    mutate {
      remove_field => [ "timestamp" ]
    }
  }
  
  # è§£æ JSON æ ¼å¼ Log
  if [type] == "json-log" {
    json {
      source => "message"
    }
  }
  
  # æ–°å¢æ¬„ä½
  mutate {
    add_field => { "environment" => "production" }
  }
  
  # æ•æ„Ÿè³‡æ–™é®è”½
  mutate {
    gsub => [
      # é®è”½èº«åˆ†è­‰å­—è™Ÿ
      "message", "[A-Z][12]\d{8}", "***MASKED***",
      # é®è”½ä¿¡ç”¨å¡è™Ÿ
      "message", "\d{4}-\d{4}-\d{4}-\d{4}", "****-****-****-****"
    ]
  }
}
```

#### Output è¨­å®šç¯„ä¾‹

```ruby
# /etc/logstash/conf.d/03-output.conf

output {
  # è¼¸å‡ºè‡³ Elasticsearch
  elasticsearch {
    hosts => ["http://es-node-01:9200", "http://es-node-02:9200"]
    index => "logs-%{[service]}-%{+YYYY.MM.dd}"
    user => "logstash_writer"
    password => "${ES_PASSWORD}"
    
    # æ•ˆèƒ½èª¿æ ¡
    action => "index"
    document_type => "_doc"
  }
  
  # é™¤éŒ¯ç”¨ï¼šåŒæ™‚è¼¸å‡ºè‡³ Console
  if [level] == "ERROR" {
    stdout {
      codec => rubydebug
    }
  }
}
```

#### å®Œæ•´ Java æ‡‰ç”¨ç¨‹å¼ Log Pipeline

```ruby
# /etc/logstash/conf.d/java-app-pipeline.conf

input {
  beats {
    port => 5044
  }
}

filter {
  # è§£æ Spring Boot Log
  grok {
    match => {
      "message" => "%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:level}\s+%{NUMBER:pid}\s+---\s+\[%{DATA:thread}\]\s+%{JAVACLASS:logger}\s+:\s+%{GREEDYDATA:logMessage}"
    }
  }
  
  # è§£ææ™‚é–“
  date {
    match => ["timestamp", "yyyy-MM-dd HH:mm:ss.SSS"]
    target => "@timestamp"
    timezone => "Asia/Taipei"
  }
  
  # è™•ç†å¤šè¡Œ Exception Stack Trace
  if [logMessage] =~ /Exception|Error/ {
    mutate {
      add_tag => ["exception"]
    }
  }
  
  # æ–°å¢ Metadata
  mutate {
    add_field => {
      "app_name" => "%{[fields][app_name]}"
      "env" => "%{[fields][env]}"
    }
    remove_field => ["timestamp", "host", "agent", "ecs", "input", "log"]
  }
}

output {
  elasticsearch {
    hosts => ["http://localhost:9200"]
    index => "app-logs-%{[app_name]}-%{+YYYY.MM.dd}"
  }
}
```

---

### 4.3 Kibana è¨­å®š

#### ä¸»è¨­å®šæª”

```yaml
# /etc/kibana/kibana.yml

# ======================== Server è¨­å®š ========================
server.port: 5601
server.host: "0.0.0.0"
server.name: "kibana-prod"

# ======================== Elasticsearch é€£ç·š ========================
elasticsearch.hosts: ["http://es-node-01:9200", "http://es-node-02:9200"]
elasticsearch.username: "kibana_system"
elasticsearch.password: "your_secure_password"

# ======================== å®‰å…¨æ€§è¨­å®š ========================
# å•Ÿç”¨åŠ å¯†é€šè¨Šï¼ˆæ­£å¼ç’°å¢ƒå»ºè­°ï¼‰
# server.ssl.enabled: true
# server.ssl.certificate: /path/to/kibana.crt
# server.ssl.key: /path/to/kibana.key

# ======================== æ—¥èªŒè¨­å®š ========================
logging.root.level: info
logging.appenders.default:
  type: file
  fileName: /var/log/kibana/kibana.log
  layout:
    type: json

# ======================== æœ¬åœ°åŒ–è¨­å®š ========================
i18n.locale: "zh-TW"

# ======================== å…¶ä»–è¨­å®š ========================
# é è¨­é¦–é 
server.defaultRoute: "/app/discover"

# æŸ¥è©¢ Timeout
elasticsearch.requestTimeout: 30000
```

#### Index Pattern è¨­å®š

é€é Kibana UI è¨­å®šï¼š

1. é€²å…¥ **Stack Management** â†’ **Data Views**
2. é»æ“Š **Create data view**
3. è¨­å®šï¼š
   - Name: `app-logs-*`
   - Index pattern: `app-logs-*`
   - Timestamp field: `@timestamp`
4. é»æ“Š **Save data view to Kibana**

æˆ–ä½¿ç”¨ APIï¼š

```bash
curl -X POST "localhost:5601/api/data_views/data_view" \
  -H "kbn-xsrf: true" \
  -H "Content-Type: application/json" \
  -d '{
    "data_view": {
      "title": "app-logs-*",
      "timeFieldName": "@timestamp"
    }
  }'
```

---

> ğŸ’¡ **æœ¬ç« é‡é»**
> - Elasticsearch Heap è¨­ç‚ºå¯¦é«”è¨˜æ†¶é«” 50%ï¼Œä¸è¶…é 31 GB
> - Logstash Pipeline åˆ†ç‚º Input â†’ Filter â†’ Output ä¸‰æ®µ
> - ä½¿ç”¨ Grok è§£æéçµæ§‹åŒ– Logï¼Œæ³¨æ„æ•ˆèƒ½å½±éŸ¿
> - Kibana è¨­å®š Index Pattern æ™‚å‹™å¿…æŒ‡å®š Timestamp æ¬„ä½

---

## ç¬¬äº”ç« ï¼šä¸‰è€…å¦‚ä½•ä¸²æ¥

### 5.1 End-to-End è³‡æ–™æµ

```mermaid
sequenceDiagram
    participant App as Application
    participant FB as Filebeat
    participant LS as Logstash
    participant ES as Elasticsearch
    participant K as Kibana
    participant User as ä½¿ç”¨è€…
    
    App->>App: å¯«å…¥ Log æª”æ¡ˆ
    FB->>App: ç›£æ§ Log æª”æ¡ˆè®ŠåŒ–
    FB->>LS: å‚³é€ Log äº‹ä»¶ (TCP 5044)
    LS->>LS: è§£æã€è½‰æ›ã€enrichment
    LS->>ES: å¯«å…¥ Index (HTTP 9200)
    ES->>ES: å»ºç«‹å€’æ’ç´¢å¼•
    User->>K: é–‹å•Ÿ Kibana (HTTP 5601)
    K->>ES: æŸ¥è©¢ Log (HTTP 9200)
    ES->>K: è¿”å›çµæœ
    K->>User: é¡¯ç¤ºè¦–è¦ºåŒ–çµæœ
```

### 5.2 å¯¦éš›ä¸²æ¥ç¯„ä¾‹

#### å ´æ™¯ï¼šSpring Boot æ‡‰ç”¨ç¨‹å¼ Log æ”¶é›†

**æ­¥é©Ÿ 1ï¼šæ‡‰ç”¨ç¨‹å¼ Log è¨­å®š**

```xml
<!-- logback-spring.xml -->
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property name="LOG_PATH" value="/var/log/myapp"/>
    <property name="APP_NAME" value="order-service"/>
    
    <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>${LOG_PATH}/${APP_NAME}.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${LOG_PATH}/${APP_NAME}.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>7</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} %level [%thread] %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>
    
    <!-- JSON æ ¼å¼ï¼ˆæ¨è–¦ï¼‰ -->
    <appender name="JSON_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>${LOG_PATH}/${APP_NAME}-json.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${LOG_PATH}/${APP_NAME}-json.%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>7</maxHistory>
        </rollingPolicy>
        <encoder class="net.logstash.logback.encoder.LogstashEncoder">
            <customFields>{"app_name":"${APP_NAME}","env":"prod"}</customFields>
        </encoder>
    </appender>
    
    <root level="INFO">
        <appender-ref ref="JSON_FILE"/>
    </root>
</configuration>
```

**æ­¥é©Ÿ 2ï¼šFilebeat è¨­å®š**

```yaml
# /etc/filebeat/filebeat.yml

filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /var/log/myapp/*-json.log
    json.keys_under_root: true
    json.add_error_key: true
    fields:
      app_name: order-service
      env: production
    fields_under_root: true

output.logstash:
  hosts: ["logstash-server:5044"]
  loadbalance: true

# ç›£æ§è¨­å®š
monitoring.enabled: true
monitoring.elasticsearch:
  hosts: ["http://es-server:9200"]
```

**æ­¥é©Ÿ 3ï¼šLogstash Pipeline è¨­å®š**

```ruby
# /etc/logstash/conf.d/spring-boot.conf

input {
  beats {
    port => 5044
  }
}

filter {
  # JSON Log å·²ç¶“ç”± Filebeat è§£æï¼Œåªéœ€è¦é¡å¤–è™•ç†
  
  # ç¢ºä¿ @timestamp æ­£ç¢º
  if [timestamp] {
    date {
      match => ["timestamp", "ISO8601"]
      target => "@timestamp"
      timezone => "Asia/Taipei"
    }
    mutate {
      remove_field => ["timestamp"]
    }
  }
  
  # è§£æ Stack Traceï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
  if [stack_trace] {
    mutate {
      add_tag => ["has_stacktrace"]
    }
  }
  
  # ç§»é™¤ä¸éœ€è¦çš„æ¬„ä½
  mutate {
    remove_field => ["agent", "ecs", "host", "input", "log"]
  }
}

output {
  elasticsearch {
    hosts => ["http://es-node-01:9200", "http://es-node-02:9200"]
    index => "app-logs-%{[app_name]}-%{+YYYY.MM.dd}"
    user => "logstash_writer"
    password => "${ES_PASSWORD}"
  }
}
```

**æ­¥é©Ÿ 4ï¼šé©—è­‰è³‡æ–™æµ**

```bash
# 1. æª¢æŸ¥ Filebeat æ˜¯å¦è®€å–åˆ° Log
sudo filebeat test output

# 2. æª¢æŸ¥ Logstash æ˜¯å¦æ”¶åˆ°è³‡æ–™
curl -s localhost:9600/_node/stats/pipelines | jq '.pipelines.main.events'

# 3. æª¢æŸ¥ Elasticsearch æ˜¯å¦æœ‰è³‡æ–™
curl -s "localhost:9200/app-logs-order-service-*/_count" | jq '.count'

# 4. åœ¨ Kibana æŸ¥è©¢
# é–‹å•Ÿ Discoverï¼Œé¸æ“‡å°æ‡‰çš„ Index Pattern
```

### 5.3 Filebeat æ•´åˆ

#### Filebeat vs Logstash æ¯”è¼ƒ

| é¢å‘ | Filebeat | Logstash |
|------|----------|----------|
| **å®šä½** | è¼•é‡ç´šè³‡æ–™æ”¶é›†å™¨ | é‡é‡ç´šè³‡æ–™è™•ç†å¼•æ“ |
| **è³‡æºæ¶ˆè€—** | ä½ï¼ˆ~10MB RAMï¼‰ | é«˜ï¼ˆ~1GB RAMï¼‰ |
| **è™•ç†èƒ½åŠ›** | åŸºæœ¬ï¼ˆModuleã€Processorï¼‰ | å¼·å¤§ï¼ˆå®Œæ•´ Filterï¼‰ |
| **éƒ¨ç½²ä½ç½®** | Application Server | é›†ä¸­è™•ç† Server |
| **ä½¿ç”¨å ´æ™¯** | æ”¶é›† + è½‰ç™¼ | è¤‡é›œè§£æ + è½‰æ› |

#### æ¨è–¦æ¶æ§‹

```mermaid
graph LR
    subgraph "App Servers"
        A1[App 1 + Filebeat]
        A2[App 2 + Filebeat]
        A3[App N + Filebeat]
    end
    
    subgraph "Processing Layer"
        LS1[Logstash 1]
        LS2[Logstash 2]
    end
    
    subgraph "Storage"
        ES[Elasticsearch Cluster]
    end
    
    A1 --> LS1
    A2 --> LS1
    A3 --> LS2
    LS1 --> ES
    LS2 --> ES
```

#### Filebeat Module ä½¿ç”¨

```bash
# å•Ÿç”¨ System Module
sudo filebeat modules enable system

# å•Ÿç”¨ Nginx Module
sudo filebeat modules enable nginx

# è¨­å®š Module
sudo vi /etc/filebeat/modules.d/nginx.yml

# è¼‰å…¥ Dashboard
sudo filebeat setup -e

# é‡å•Ÿ Filebeat
sudo systemctl restart filebeat
```

---

> ğŸ’¡ **æœ¬ç« é‡é»**
> - æ¨è–¦ä½¿ç”¨ JSON æ ¼å¼ Logï¼Œæ¸›å°‘è§£ææˆæœ¬
> - Filebeat éƒ¨ç½²åœ¨ App Serverï¼ŒLogstash é›†ä¸­è™•ç†
> - é©—è­‰è³‡æ–™æµï¼šFilebeat â†’ Logstash â†’ Elasticsearch â†’ Kibana
> - Filebeat Module å¯å¿«é€Ÿæ•´åˆå¸¸è¦‹ Log æ ¼å¼

---

## ç¬¬å…­ç« ï¼šç³»çµ±ä½¿ç”¨

### 6.1 Kibana æ“ä½œæ•™å­¸

#### Discover - Log æŸ¥è©¢

```mermaid
graph TB
    subgraph "Discover ä»‹é¢"
        A[æ™‚é–“é¸æ“‡å™¨]
        B[æœå°‹åˆ— - KQL]
        C[æ¬„ä½åˆ—è¡¨]
        D[Log åˆ—è¡¨]
        E[Log è©³æƒ…]
    end
    
    A --> D
    B --> D
    C --> D
    D --> E
```

**åŸºæœ¬æ“ä½œ**ï¼š

1. **é¸æ“‡ Data View**ï¼šå·¦ä¸Šè§’ä¸‹æ‹‰é¸å–®
2. **è¨­å®šæ™‚é–“ç¯„åœ**ï¼šå³ä¸Šè§’æ™‚é–“é¸æ“‡å™¨
3. **è¼¸å…¥æœå°‹æ¢ä»¶**ï¼šæœå°‹åˆ—è¼¸å…¥ KQL
4. **æ–°å¢é¡¯ç¤ºæ¬„ä½**ï¼šå¾å·¦å´æ¬„ä½åˆ—è¡¨é»æ“Š `+`
5. **æª¢è¦– Log è©³æƒ…**ï¼šé»æ“Šä»»ä¸€ç­† Log å±•é–‹

#### Dashboard - è¦–è¦ºåŒ–å„€è¡¨æ¿

**å»ºç«‹ Dashboard æ­¥é©Ÿ**ï¼š

1. é€²å…¥ **Analytics** â†’ **Dashboard**
2. é»æ“Š **Create dashboard**
3. é»æ“Š **Create visualization**
4. é¸æ“‡åœ–è¡¨é¡å‹ï¼š
   - **Bar / Line Chart**ï¼šè¶¨å‹¢åˆ†æ
   - **Pie Chart**ï¼šæ¯”ä¾‹åˆ†å¸ƒ
   - **Metric**ï¼šå–®ä¸€æ•¸å€¼
   - **Data Table**ï¼šæ˜ç´°è¡¨æ ¼
5. è¨­å®š Data View èˆ‡èšåˆæ¢ä»¶
6. å„²å­˜ Visualization ä¸¦åŠ å…¥ Dashboard

**ç¯„ä¾‹ï¼šå»ºç«‹ Error Rate Dashboard**

```
1. å»ºç«‹ Line Chartï¼š
   - Data View: app-logs-*
   - Y-axis: Count
   - X-axis: @timestamp (Date Histogram, 1 minute)
   - Breakdown: level (Top values)

2. å»ºç«‹ Metricï¼š
   - Data View: app-logs-*
   - Filter: level: ERROR
   - Aggregation: Count
   - Time range: Last 1 hour

3. å»ºç«‹ Data Tableï¼š
   - Data View: app-logs-*
   - Filter: level: ERROR
   - Columns: @timestamp, service, message
   - Sort: @timestamp DESC
```

### 6.2 æŸ¥è©¢èªæ³•è©³è§£

#### KQLï¼ˆKibana Query Languageï¼‰

```
# åŸºæœ¬èªæ³•
æ¬„ä½åç¨±: å€¼

# ç¯„ä¾‹
level: ERROR
service: order-service
message: "timeout"
```

**å¸¸ç”¨æŸ¥è©¢ç¯„ä¾‹**ï¼š

| éœ€æ±‚ | KQL èªæ³• |
|------|----------|
| ç²¾ç¢ºåŒ¹é… | `level: "ERROR"` |
| æ¨¡ç³ŠåŒ¹é… | `message: *timeout*` |
| å¤šå€¼åŒ¹é… | `level: (ERROR OR WARN)` |
| ç¯„åœæŸ¥è©¢ | `response_time >= 1000` |
| å­˜åœ¨æª¢æŸ¥ | `stack_trace: *` |
| çµ„åˆæ¢ä»¶ | `service: order* AND level: ERROR` |
| æ’é™¤æ¢ä»¶ | `NOT level: DEBUG` |

#### Lucene Query Syntaxï¼ˆé€²éšï¼‰

```
# è¬ç”¨å­—å…ƒ
message: timeout*
message: time?ut

# æ­£è¦è¡¨é”å¼
message: /[Ee]rror.*/

# æ¨¡ç³Šæœå°‹
message: tiemout~2

# ç¯„åœæœå°‹
response_time: [100 TO 500]
@timestamp: [2026-01-01 TO 2026-01-31]

# æ¬Šé‡
message: error^2 OR warning^1
```

#### å¯¦ç”¨æŸ¥è©¢ç¯„ä¾‹

```bash
# 1. æŸ¥è©¢ç‰¹å®šäº¤æ˜“çš„å®Œæ•´éˆè·¯
traceId: "abc123" AND (service: order-service OR service: payment-service)

# 2. æŸ¥è©¢ä»Šæ—¥æ‰€æœ‰ 5xx éŒ¯èª¤
response_code: [500 TO 599] AND @timestamp >= now/d

# 3. æŸ¥è©¢ç‰¹å®šæ™‚æ®µçš„æ…¢æŸ¥è©¢
response_time >= 3000 AND @timestamp >= "2026-01-27T10:00:00" AND @timestamp <= "2026-01-27T11:00:00"

# 4. æ’é™¤å¥åº·æª¢æŸ¥ Log
NOT (request_path: "/health" OR request_path: "/actuator/*")

# 5. æŸ¥è©¢å«æœ‰ Exception çš„ Log
message: *Exception* OR stack_trace: *
```

### 6.3 å¯¦å‹™ä½¿ç”¨æƒ…å¢ƒ

#### æƒ…å¢ƒä¸€ï¼šå•é¡Œè¿½è¹¤

**å ´æ™¯**ï¼šä½¿ç”¨è€…å›å ±è¨‚å–®å¤±æ•—ï¼Œæä¾›è¨‚å–®ç·¨è™Ÿ `ORD-20260127-001`

```mermaid
sequenceDiagram
    participant User as ä½¿ç”¨è€…
    participant K as Kibana
    participant ES as Elasticsearch
    
    User->>K: 1. è¼¸å…¥æŸ¥è©¢ï¼šorderId: "ORD-20260127-001"
    K->>ES: 2. åŸ·è¡ŒæŸ¥è©¢
    ES->>K: 3. è¿”å›ç›¸é—œ Log
    K->>User: 4. é¡¯ç¤ºå®Œæ•´äº¤æ˜“éˆè·¯
    User->>User: 5. å®šä½ ERROR Log
    User->>User: 6. åˆ†æ Stack Trace
```

**æŸ¥è©¢æ­¥é©Ÿ**ï¼š

```
1. åœ¨ Discover è¼¸å…¥ï¼šorderId: "ORD-20260127-001"
2. å±•é–‹æ™‚é–“ç¯„åœç¢ºä¿æ¶µè“‹äº¤æ˜“æ™‚é–“
3. æ’åºï¼š@timestamp ASCï¼ˆä¾æ™‚é–“æ­£åºï¼‰
4. æ–°å¢é¡¯ç¤ºæ¬„ä½ï¼šservice, level, message, duration
5. æ‰¾åˆ° ERROR Logï¼Œæª¢è¦– Stack Trace
```

#### æƒ…å¢ƒäºŒï¼šéŒ¯èª¤åˆ†æ

**å ´æ™¯**ï¼šGrafana å‘Šè­¦é¡¯ç¤º Error Rate ä¸Šå‡

```
æŸ¥è©¢æ­¥é©Ÿï¼š

1. è¨­å®šæ™‚é–“ç¯„åœç‚ºå‘Šè­¦è§¸ç™¼å‰å¾Œ 30 åˆ†é˜

2. æŸ¥è©¢ Error Logï¼š
   level: ERROR AND @timestamp >= "2026-01-27T10:00:00"

3. èšåˆåˆ†æ Error é¡å‹ï¼š
   - é–‹å•Ÿ Lens
   - X-axis: @timestamp
   - Breakdown: error_type
   - è­˜åˆ¥æœ€å¤šçš„ Error é¡å‹

4. æ·±å…¥ç‰¹å®š Errorï¼š
   error_type: "ConnectionTimeoutException" AND service: payment-service

5. é—œè¯ Metricsï¼š
   åœ¨ Grafana æª¢æŸ¥åŒæ™‚æ®µ payment-service çš„ Connection Pool ä½¿ç”¨ç‡
```

#### æƒ…å¢ƒä¸‰ï¼šç³»çµ±è¡Œç‚ºå›æº¯

**å ´æ™¯**ï¼šç¨½æ ¸è¦æ±‚æä¾›ç‰¹å®šä½¿ç”¨è€…éå» 7 å¤©çš„æ“ä½œç´€éŒ„

```
æŸ¥è©¢èªæ³•ï¼š
userId: "U12345" AND action: * AND @timestamp >= now-7d

é¡¯ç¤ºæ¬„ä½ï¼š
- @timestamp
- action
- ip_address
- request_path
- response_code

åŒ¯å‡ºæ­¥é©Ÿï¼š
1. åœ¨ Discover åŸ·è¡ŒæŸ¥è©¢
2. è¨­å®šé¡¯ç¤ºæ¬„ä½
3. é»æ“Š Share â†’ CSV Reports
4. é¸æ“‡ Generate CSV
5. ä¸‹è¼‰ä¸¦æä¾›ç¨½æ ¸å–®ä½
```

#### æƒ…å¢ƒå››ï¼šèˆ‡ Grafana Metrics æ­é…åˆ†æ

```mermaid
graph LR
    subgraph "Grafana"
        A[ç™¼ç¾ CPU é£†é«˜]
        B[æª¢è¦– Alert è©³æƒ…]
    end
    
    subgraph "Kibana"
        C[æŸ¥è©¢åŒæ™‚æ®µ Log]
        D[è­˜åˆ¥ç•°å¸¸ Pattern]
    end
    
    A --> B
    B -->|"Deep Link"| C
    C --> D
    D -->|"Correlation"| A
```

**æ•´åˆæŸ¥è©¢ç¯„ä¾‹**ï¼š

```
# Grafana Alert è§¸ç™¼æ™‚é–“ï¼š2026-01-27 14:30:00
# ç›®æ¨™æœå‹™ï¼šinventory-service
# è§€å¯Ÿåˆ° CPU > 90%

Kibana æŸ¥è©¢ï¼š
service: inventory-service AND @timestamp >= "2026-01-27T14:25:00" AND @timestamp <= "2026-01-27T14:35:00"

å¯èƒ½ç™¼ç¾ï¼š
- å¤§é‡ DEBUG Log è¼¸å‡º
- é‡è¤‡çš„ SQL Query
- GC Log é »ç¹
- Exception å¤§é‡æ‹‹å‡º
```

---

> ğŸ’¡ **æœ¬ç« é‡é»**
> - KQL èªæ³•ç°¡æ½”ç›´è§€ï¼Œé©åˆæ—¥å¸¸æŸ¥è©¢
> - è¤‡é›œæŸ¥è©¢å¯ä½¿ç”¨ Lucene èªæ³•
> - å»ºç«‹å¸¸ç”¨æŸ¥è©¢çš„ Saved Search
> - Dashboard ä¾è§’è‰²è¨­è¨ˆï¼ˆDev / Ops / Businessï¼‰

---

## ç¬¬ä¸ƒç« ï¼šç³»çµ±ç¶­è­·

### 7.1 Index ç®¡ç†ç­–ç•¥

#### Index Lifecycle Management (ILM)

```mermaid
graph LR
    subgraph "Index Lifecycle"
        H[Hot Phase<br/>å¯«å…¥ & æŸ¥è©¢]
        W[Warm Phase<br/>å”¯è®€ & æŸ¥è©¢]
        C[Cold Phase<br/>ä½é »æŸ¥è©¢]
        D[Delete Phase<br/>åˆªé™¤]
    end
    
    H -->|"7 å¤©å¾Œ"| W
    W -->|"30 å¤©å¾Œ"| C
    C -->|"90 å¤©å¾Œ"| D
    
    style H fill:#ff6b6b
    style W fill:#feca57
    style C fill:#54a0ff
    style D fill:#576574
```

#### ILM Policy è¨­å®š

```bash
# å»ºç«‹ ILM Policy
curl -X PUT "localhost:9200/_ilm/policy/logs-policy" -H 'Content-Type: application/json' -d'
{
  "policy": {
    "phases": {
      "hot": {
        "min_age": "0ms",
        "actions": {
          "rollover": {
            "max_age": "1d",
            "max_size": "50gb"
          },
          "set_priority": {
            "priority": 100
          }
        }
      },
      "warm": {
        "min_age": "7d",
        "actions": {
          "forcemerge": {
            "max_num_segments": 1
          },
          "shrink": {
            "number_of_shards": 1
          },
          "set_priority": {
            "priority": 50
          }
        }
      },
      "cold": {
        "min_age": "30d",
        "actions": {
          "freeze": {},
          "set_priority": {
            "priority": 0
          }
        }
      },
      "delete": {
        "min_age": "90d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}'
```

#### æ‰‹å‹•æ¸…ç† Index

```bash
# åˆ—å‡ºæ‰€æœ‰ Index
curl -s "localhost:9200/_cat/indices?v&s=index"

# åˆªé™¤ç‰¹å®šæ—¥æœŸçš„ Index
curl -X DELETE "localhost:9200/app-logs-*-2026.01.01"

# åˆªé™¤ 30 å¤©å‰çš„ Indexï¼ˆä½¿ç”¨ Curator æˆ–è…³æœ¬ï¼‰
#!/bin/bash
# cleanup-old-indices.sh

DAYS_TO_KEEP=30
DATE_THRESHOLD=$(date -d "-${DAYS_TO_KEEP} days" +%Y.%m.%d)

curl -s "localhost:9200/_cat/indices/logs-*?h=index" | while read index; do
  index_date=$(echo $index | grep -oP '\d{4}\.\d{2}\.\d{2}')
  if [[ "$index_date" < "$DATE_THRESHOLD" ]]; then
    echo "Deleting $index"
    curl -X DELETE "localhost:9200/$index"
  fi
done
```

### 7.2 æ•ˆèƒ½èª¿æ ¡

#### Elasticsearch æ•ˆèƒ½å„ªåŒ–

```yaml
# elasticsearch.yml æ•ˆèƒ½ç›¸é—œè¨­å®š

# ç´¢å¼• Refresh é–“éš”ï¼ˆå¯«å…¥é‡å¤§æ™‚å¯å¢åŠ ï¼‰
index.refresh_interval: 30s

# ç´¢å¼• Buffer å¤§å°
indices.memory.index_buffer_size: 20%

# Thread Pool è¨­å®š
thread_pool.write.queue_size: 1000
thread_pool.search.queue_size: 1000
```

#### æŸ¥è©¢æ•ˆèƒ½å„ªåŒ–

```bash
# 1. é¿å… wildcard é–‹é ­æŸ¥è©¢
âŒ message: *error*
âœ… message: error*

# 2. ä½¿ç”¨ filter è€Œé queryï¼ˆä¸è¨ˆç®—åˆ†æ•¸ï¼Œå¯å¿«å–ï¼‰
curl -X GET "localhost:9200/logs-*/_search" -H 'Content-Type: application/json' -d'
{
  "query": {
    "bool": {
      "filter": [
        { "term": { "level": "ERROR" } },
        { "range": { "@timestamp": { "gte": "now-1h" } } }
      ]
    }
  }
}'

# 3. é™åˆ¶è¿”å›æ¬„ä½
curl -X GET "localhost:9200/logs-*/_search" -H 'Content-Type: application/json' -d'
{
  "_source": ["@timestamp", "level", "message"],
  "query": { "match_all": {} },
  "size": 100
}'
```

#### Logstash æ•ˆèƒ½å„ªåŒ–

```yaml
# logstash.yml

# Pipeline è¨­å®š
pipeline.workers: 4          # é€šå¸¸è¨­ç‚º CPU æ ¸å¿ƒæ•¸
pipeline.batch.size: 250     # æ‰¹æ¬¡å¤§å°
pipeline.batch.delay: 50     # æ‰¹æ¬¡ç­‰å¾…æ™‚é–“ (ms)

# è¼¸å‡º Buffer
output.elasticsearch.bulk_max_size: 5000
```

### 7.3 å¥åº·æª¢æŸ¥èˆ‡ç›£æ§

#### Elasticsearch å¥åº·æª¢æŸ¥

```bash
# Cluster Health
curl -s "localhost:9200/_cluster/health?pretty"

# é æœŸçµæœ
{
  "status": "green",        # green/yellow/red
  "number_of_nodes": 3,
  "active_primary_shards": 50,
  "active_shards": 100,
  "unassigned_shards": 0    # æ‡‰ç‚º 0
}

# Node Stats
curl -s "localhost:9200/_nodes/stats?pretty" | jq '.nodes | to_entries[] | {
  name: .value.name,
  heap_used_percent: .value.jvm.mem.heap_used_percent,
  disk_available: .value.fs.total.available_in_bytes
}'

# Index Stats
curl -s "localhost:9200/_cat/indices?v&h=index,health,docs.count,store.size"
```

#### ç›£æ§æŒ‡æ¨™æ¸…å–®

| æŒ‡æ¨™ | æ­£å¸¸ç¯„åœ | å‘Šè­¦é–¾å€¼ |
|------|---------|---------|
| Cluster Status | green | yellow/red |
| JVM Heap Used | < 75% | > 85% |
| Disk Available | > 20% | < 15% |
| Indexing Rate | ç©©å®š | çªé™ 50% |
| Search Latency | < 200ms | > 500ms |
| Unassigned Shards | 0 | > 0 |

#### æ•´åˆ Prometheus ç›£æ§

```yaml
# ä½¿ç”¨ elasticsearch_exporter
# docker-compose.yml

version: '3'
services:
  elasticsearch-exporter:
    image: prometheuscommunity/elasticsearch-exporter:latest
    command:
      - '--es.uri=http://elasticsearch:9200'
    ports:
      - "9114:9114"
```

```yaml
# prometheus.yml

scrape_configs:
  - job_name: 'elasticsearch'
    static_configs:
      - targets: ['elasticsearch-exporter:9114']
```

---

> ğŸ’¡ **æœ¬ç« é‡é»**
> - ä½¿ç”¨ ILM è‡ªå‹•ç®¡ç† Index ç”Ÿå‘½é€±æœŸ
> - Hot-Warm-Cold æ¶æ§‹å„ªåŒ–å„²å­˜æˆæœ¬
> - å®šæœŸæª¢æŸ¥ Cluster Health èˆ‡ JVM Heap
> - æ•´åˆ Prometheus ç›£æ§ ELK æœ¬èº«

---

## ç¬¬å…«ç« ï¼šç³»çµ±å‡ç´š

### 8.1 å‡ç´šå‰æº–å‚™

#### å‡ç´šæª¢æŸ¥æ¸…å–®

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å‡ç´šå‰æª¢æŸ¥æ¸…å–®                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â–¡ 1. ç¢ºèªç›®æ¨™ç‰ˆæœ¬èˆ‡ç¾æœ‰ç‰ˆæœ¬çš„ç›¸å®¹æ€§                        â”‚
â”‚  â–¡ 2. é–±è®€ Release Notes èˆ‡ Breaking Changes                â”‚
â”‚  â–¡ 3. å‚™ä»½ Elasticsearch Index                              â”‚
â”‚  â–¡ 4. å‚™ä»½è¨­å®šæª”ï¼ˆelasticsearch.ymlã€logstash.conf ç­‰ï¼‰     â”‚
â”‚  â–¡ 5. å‚™ä»½ Kibana Saved Objectsï¼ˆDashboardã€Visualizationï¼‰ â”‚
â”‚  â–¡ 6. æ¸¬è©¦ç’°å¢ƒå…ˆè¡Œå‡ç´šé©—è­‰                                  â”‚
â”‚  â–¡ 7. æº–å‚™å›æ»¾è¨ˆç•«                                          â”‚
â”‚  â–¡ 8. é€šçŸ¥ç›¸é—œäººå“¡ç¶­è­·æ™‚é–“                                  â”‚
â”‚  â–¡ 9. ç¢ºèªæœ‰è¶³å¤ ç£ç¢Ÿç©ºé–“                                    â”‚
â”‚  â–¡ 10. ç¢ºèª Cluster Health ç‚º green                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ç‰ˆæœ¬ç›¸å®¹æ€§æª¢æŸ¥

```bash
# æŸ¥çœ‹ç›®å‰ç‰ˆæœ¬
curl -s "localhost:9200" | jq '.version.number'
curl -s "localhost:5601/api/status" | jq '.version.number'

# æª¢æŸ¥ Elastic å®˜æ–¹ Support Matrix
# https://www.elastic.co/support/matrix

# é‡è¦ï¼šä¸‰å€‹å…ƒä»¶æ‡‰ä½¿ç”¨ç›¸åŒä¸»ç‰ˆæœ¬
# âœ… ES 8.12 + Logstash 8.12 + Kibana 8.12
# âŒ ES 8.12 + Logstash 7.17 + Kibana 8.12
```

#### å‚™ä»½ç­–ç•¥

```bash
# 1. å»ºç«‹ Snapshot Repository
curl -X PUT "localhost:9200/_snapshot/backup_repo" -H 'Content-Type: application/json' -d'
{
  "type": "fs",
  "settings": {
    "location": "/mnt/backup/elasticsearch"
  }
}'

# 2. å»ºç«‹ Snapshot
curl -X PUT "localhost:9200/_snapshot/backup_repo/pre-upgrade-$(date +%Y%m%d)?wait_for_completion=true"

# 3. å‚™ä»½ Kibana Saved Objects
curl -X POST "localhost:5601/api/saved_objects/_export" \
  -H "kbn-xsrf: true" \
  -H "Content-Type: application/json" \
  -d '{"type": ["dashboard", "visualization", "index-pattern", "search"]}' \
  > kibana-saved-objects-backup.ndjson

# 4. å‚™ä»½è¨­å®šæª”
tar -czvf elk-config-backup-$(date +%Y%m%d).tar.gz \
  /etc/elasticsearch \
  /etc/logstash \
  /etc/kibana \
  /etc/filebeat
```

### 8.2 å„å…ƒä»¶å‡ç´šæµç¨‹

#### Elasticsearch å‡ç´šï¼ˆRolling Upgradeï¼‰

```bash
# é©ç”¨æ–¼åŒä¸€å¤§ç‰ˆæœ¬å‡ç´šï¼ˆå¦‚ 8.10 â†’ 8.12ï¼‰

# 1. é—œé–‰ Shard Allocation
curl -X PUT "localhost:9200/_cluster/settings" -H 'Content-Type: application/json' -d'
{
  "persistent": {
    "cluster.routing.allocation.enable": "primaries"
  }
}'

# 2. åœæ­¢éå¿…è¦ç´¢å¼•ï¼ˆå¯é¸ï¼‰
curl -X POST "localhost:9200/_flush/synced"

# 3. åœæ­¢ç¯€é»
sudo systemctl stop elasticsearch

# 4. å‡ç´šå¥—ä»¶
sudo yum update elasticsearch
# æˆ–
sudo apt-get update && sudo apt-get install elasticsearch

# 5. å•Ÿå‹•ç¯€é»
sudo systemctl start elasticsearch

# 6. ç­‰å¾…ç¯€é»åŠ å…¥å¢é›†
curl -s "localhost:9200/_cat/nodes?v"

# 7. é‡æ–°å•Ÿç”¨ Shard Allocation
curl -X PUT "localhost:9200/_cluster/settings" -H 'Content-Type: application/json' -d'
{
  "persistent": {
    "cluster.routing.allocation.enable": null
  }
}'

# 8. ç­‰å¾… Cluster å›åˆ° green
watch -n 5 'curl -s localhost:9200/_cluster/health | jq .'

# 9. å°ä¸‹ä¸€å€‹ç¯€é»é‡è¤‡æ­¥é©Ÿ 1-8
```

#### Logstash å‡ç´š

```bash
# 1. åœæ­¢ Logstash
sudo systemctl stop logstash

# 2. å‡ç´šå¥—ä»¶
sudo yum update logstash

# 3. é©—è­‰è¨­å®šæª”ç›¸å®¹æ€§
sudo /usr/share/logstash/bin/logstash --config.test_and_exit -f /etc/logstash/conf.d/

# 4. å•Ÿå‹• Logstash
sudo systemctl start logstash

# 5. é©—è­‰
curl -s localhost:9600 | jq '.version'
```

#### Kibana å‡ç´š

```bash
# 1. åœæ­¢ Kibana
sudo systemctl stop kibana

# 2. å‡ç´šå¥—ä»¶
sudo yum update kibana

# 3. å•Ÿå‹• Kibana
sudo systemctl start kibana

# 4. ç­‰å¾…å•Ÿå‹•å®Œæˆï¼ˆå¯èƒ½éœ€è¦ 1-2 åˆ†é˜ï¼‰
tail -f /var/log/kibana/kibana.log

# 5. é©—è­‰
curl -s localhost:5601/api/status | jq '.version'
```

### 8.3 å›å¾©ç­–ç•¥

#### å›å¾© Elasticsearch

```bash
# 1. åœæ­¢ç¯€é»
sudo systemctl stop elasticsearch

# 2. é™ç´šå¥—ä»¶
sudo yum downgrade elasticsearch-8.10.0

# 3. é‚„åŸè¨­å®šæª”
sudo tar -xzvf elk-config-backup-*.tar.gz -C /

# 4. é‚„åŸè³‡æ–™ï¼ˆå¦‚æœéœ€è¦ï¼‰
curl -X POST "localhost:9200/_snapshot/backup_repo/pre-upgrade-*/_restore?wait_for_completion=true"

# 5. å•Ÿå‹•æœå‹™
sudo systemctl start elasticsearch
```

#### å›å¾© Kibana Saved Objects

```bash
# åŒ¯å…¥å‚™ä»½çš„ Saved Objects
curl -X POST "localhost:5601/api/saved_objects/_import?overwrite=true" \
  -H "kbn-xsrf: true" \
  --form file=@kibana-saved-objects-backup.ndjson
```

---

> ğŸ’¡ **æœ¬ç« é‡é»**
> - å‡ç´šå‰å‹™å¿…å‚™ä»½ï¼šIndex Snapshot + è¨­å®šæª” + Kibana Objects
> - Rolling Upgrade å¯é¿å…æœå‹™ä¸­æ–·
> - ä¸‰å€‹å…ƒä»¶ä½¿ç”¨ç›¸åŒç‰ˆæœ¬
> - æº–å‚™å®Œæ•´å›æ»¾è¨ˆç•«

---

## ç¬¬ä¹ç« ï¼šå®‰å…¨æ€§èˆ‡æ¬Šé™ç®¡ç†

### 9.1 Security åŸºæœ¬æ¦‚å¿µ

#### Elastic Security æ¶æ§‹

```mermaid
graph TB
    subgraph "Security Layer"
        Auth[Authentication<br/>èº«ä»½é©—è­‰]
        Authz[Authorization<br/>æ¬Šé™æ§ç®¡]
        Enc[Encryption<br/>å‚³è¼¸åŠ å¯†]
        Audit[Audit Logging<br/>ç¨½æ ¸æ—¥èªŒ]
    end
    
    User[ä½¿ç”¨è€…] --> Auth
    Auth --> Authz
    Authz --> ES[Elasticsearch]
    Enc -.->|"TLS"| ES
    ES --> Audit
```

#### å•Ÿç”¨ Security

```yaml
# elasticsearch.yml

xpack.security.enabled: true
xpack.security.enrollment.enabled: true

# å‚³è¼¸å±¤åŠ å¯†
xpack.security.transport.ssl.enabled: true
xpack.security.transport.ssl.verification_mode: certificate
xpack.security.transport.ssl.keystore.path: elastic-certificates.p12
xpack.security.transport.ssl.truststore.path: elastic-certificates.p12

# HTTP å±¤åŠ å¯†ï¼ˆæ­£å¼ç’°å¢ƒå»ºè­°ï¼‰
xpack.security.http.ssl.enabled: true
xpack.security.http.ssl.keystore.path: http.p12
```

### 9.2 ä½¿ç”¨è€…èˆ‡è§’è‰²ç®¡ç†

#### å…§å»ºè§’è‰²

| è§’è‰² | èªªæ˜ | é©ç”¨å°è±¡ |
|------|------|---------|
| `superuser` | å®Œæ•´æ¬Šé™ | ç³»çµ±ç®¡ç†å“¡ |
| `kibana_admin` | Kibana ç®¡ç†æ¬Šé™ | Kibana ç®¡ç†å“¡ |
| `monitoring_user` | ç›£æ§å”¯è®€ | ç›£æ§äººå“¡ |
| `logstash_system` | Logstash ç›£æ§ | Logstash |
| `beats_system` | Beats ç›£æ§ | Filebeat |

#### å»ºç«‹è‡ªè¨‚è§’è‰²

```bash
# å»ºç«‹æ‡‰ç”¨ç¨‹å¼é–‹ç™¼è€…è§’è‰²
curl -X PUT "localhost:9200/_security/role/app_developer" -H 'Content-Type: application/json' -d'
{
  "indices": [
    {
      "names": ["app-logs-*"],
      "privileges": ["read", "view_index_metadata"],
      "query": {
        "bool": {
          "filter": [
            { "term": { "env": "dev" } }
          ]
        }
      }
    }
  ],
  "applications": [
    {
      "application": "kibana-.kibana",
      "privileges": ["feature_discover.read", "feature_dashboard.read"],
      "resources": ["*"]
    }
  ]
}'

# å»ºç«‹ä½¿ç”¨è€…
curl -X POST "localhost:9200/_security/user/dev_user" -H 'Content-Type: application/json' -d'
{
  "password": "secure_password",
  "roles": ["app_developer"],
  "full_name": "é–‹ç™¼äººå“¡",
  "email": "dev@company.com"
}'
```

#### å¯¦å‹™è§’è‰²è¨­è¨ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    è§’è‰²è¨­è¨ˆå»ºè­°                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è§’è‰²         â”‚ æ¬Šé™ç¯„åœ                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ elk_admin    â”‚ å®Œæ•´ç®¡ç†æ¬Šé™ï¼Œå¯å»ºç«‹/åˆªé™¤ Index              â”‚
â”‚ ops_team     â”‚ å¯æŸ¥çœ‹æ‰€æœ‰ç’°å¢ƒ Logï¼Œå¯å»ºç«‹ Dashboard         â”‚
â”‚ dev_team     â”‚ åªèƒ½æŸ¥çœ‹ dev/sit ç’°å¢ƒ Log                    â”‚
â”‚ security     â”‚ å¯æŸ¥çœ‹æ‰€æœ‰ Logï¼Œç‰¹åˆ¥æ˜¯ Security ç›¸é—œ         â”‚
â”‚ auditor      â”‚ å”¯è®€æ¬Šé™ï¼Œå¯åŒ¯å‡ºå ±è¡¨                         â”‚
â”‚ business     â”‚ åªèƒ½æŸ¥çœ‹ç‰¹å®š Dashboardï¼Œç„¡ Discover æ¬Šé™     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.3 ä¼æ¥­è³‡å®‰è€ƒé‡

#### æ•æ„Ÿè³‡æ–™è™•ç†

```ruby
# Logstash - æ•æ„Ÿè³‡æ–™é®è”½

filter {
  # é®è”½èº«åˆ†è­‰å­—è™Ÿ
  mutate {
    gsub => [
      "message", "[A-Z][12]\d{8}", "[ID_MASKED]"
    ]
  }
  
  # é®è”½ä¿¡ç”¨å¡è™Ÿ
  mutate {
    gsub => [
      "message", "\b\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}\b", "[CARD_MASKED]"
    ]
  }
  
  # é®è”½ Email
  mutate {
    gsub => [
      "message", "[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}", "[EMAIL_MASKED]"
    ]
  }
  
  # ç§»é™¤æ•æ„Ÿæ¬„ä½
  mutate {
    remove_field => ["password", "token", "secret"]
  }
}
```

#### ç¨½æ ¸æ—¥èªŒ

```yaml
# elasticsearch.yml

xpack.security.audit.enabled: true
xpack.security.audit.logfile.events.include:
  - access_denied
  - authentication_failed
  - connection_denied
  - tampered_request
  - run_as_denied
  - anonymous_access_denied
```

#### ç¶²è·¯å®‰å…¨å»ºè­°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ç¶²è·¯å®‰å…¨å»ºè­°                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Elasticsearch 9200/9300 port ä¸å°å¤–é–‹æ”¾                 â”‚
â”‚  2. Kibana é€é Reverse Proxyï¼ˆNginxï¼‰å°å¤–                  â”‚
â”‚  3. å•Ÿç”¨ HTTPSï¼ˆTLS 1.2+ï¼‰                                  â”‚
â”‚  4. ä½¿ç”¨é˜²ç«ç‰†é™åˆ¶ä¾†æº IP                                   â”‚
â”‚  5. å®šæœŸæ›´æ› Password èˆ‡ Token                              â”‚
â”‚  6. å•Ÿç”¨ Audit Log ä¸¦ä¿å­˜è‡³å°‘ 1 å¹´                          â”‚
â”‚  7. æ•æ„Ÿè³‡æ–™åœ¨å¯«å…¥å‰é®è”½ï¼Œä¸ä¾è³´æŸ¥è©¢æ™‚éæ¿¾                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

> ğŸ’¡ **æœ¬ç« é‡é»**
> - æ­£å¼ç’°å¢ƒå‹™å¿…å•Ÿç”¨ Security
> - ä¾è§’è‰²è¨­è¨ˆæ¬Šé™ï¼Œéµå¾ªæœ€å°æ¬Šé™åŸå‰‡
> - æ•æ„Ÿè³‡æ–™åœ¨ Logstash éšæ®µé®è”½
> - å•Ÿç”¨ Audit Log æ»¿è¶³æ³•è¦è¦æ±‚

---

## ç¬¬åç« ï¼šæœ€ä½³å¯¦å‹™èˆ‡å°å…¥å»ºè­°

### 10.1 å°å…¥å¸¸è¦‹è¸©é›·é»

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ELK å°å…¥å¸¸è¦‹è¸©é›·é»                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  1. âŒ Log é‡ä¼°ç®—éŒ¯èª¤                                       â”‚
â”‚     â†’ æ­£ç¢ºåšæ³•ï¼šå…ˆåœ¨æ¸¬è©¦ç’°å¢ƒæ¸¬é‡å¯¦éš› Log é‡                  â”‚
â”‚                                                             â”‚
â”‚  2. âŒ æ²’æœ‰è¨­å®š ILMï¼ŒIndex ç„¡é™æˆé•·                         â”‚
â”‚     â†’ æ­£ç¢ºåšæ³•ï¼šä¸Šç·šå‰å°±è¨­å®š ILM Policy                      â”‚
â”‚                                                             â”‚
â”‚  3. âŒ æ‰€æœ‰ Log éƒ½æ”¶é›†                                      â”‚
â”‚     â†’ æ­£ç¢ºåšæ³•ï¼šéæ¿¾ç„¡ç”¨ Logï¼ˆDEBUGã€Health Checkï¼‰          â”‚
â”‚                                                             â”‚
â”‚  4. âŒ Shard æ•¸é‡è¨­å®šä¸ç•¶                                   â”‚
â”‚     â†’ æ­£ç¢ºåšæ³•ï¼šå–®ä¸€ Shard 10-50 GBï¼Œä¾è³‡æ–™é‡è¨ˆç®—            â”‚
â”‚                                                             â”‚
â”‚  5. âŒ æ²’æœ‰ç›£æ§ ELK æœ¬èº«                                    â”‚
â”‚     â†’ æ­£ç¢ºåšæ³•ï¼šç”¨ Prometheus ç›£æ§ ELK å…ƒä»¶                  â”‚
â”‚                                                             â”‚
â”‚  6. âŒ ç›´æ¥åœ¨ Production èª¿æ•´è¨­å®š                           â”‚
â”‚     â†’ æ­£ç¢ºåšæ³•ï¼šå…ˆåœ¨æ¸¬è©¦ç’°å¢ƒé©—è­‰                             â”‚
â”‚                                                             â”‚
â”‚  7. âŒ å¿½ç•¥ Security è¨­å®š                                   â”‚
â”‚     â†’ æ­£ç¢ºåšæ³•ï¼šä¸Šç·šå‰å•Ÿç”¨èªè­‰èˆ‡åŠ å¯†                         â”‚
â”‚                                                             â”‚
â”‚  8. âŒ æ²’æœ‰å‚™ä»½ç­–ç•¥                                         â”‚
â”‚     â†’ æ­£ç¢ºåšæ³•ï¼šå®šæœŸ Snapshot + è¨­å®šæª”ç‰ˆæ§                   â”‚
â”‚                                                             â”‚
â”‚  9. âŒ Logstash èˆ‡ Application éƒ¨ç½²åœ¨åŒä¸€å°                 â”‚
â”‚     â†’ æ­£ç¢ºåšæ³•ï¼šFilebeat åœ¨ App Serverï¼ŒLogstash ç¨ç«‹éƒ¨ç½²    â”‚
â”‚                                                             â”‚
â”‚  10. âŒ ä½¿ç”¨ç´”æ–‡å­— Log æ ¼å¼                                 â”‚
â”‚      â†’ æ­£ç¢ºåšæ³•ï¼šä½¿ç”¨ JSON æ ¼å¼ï¼Œæ¸›å°‘è§£ææˆæœ¬                â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 10.2 çµæ§‹åŒ– Log è¨­è¨ˆåŸå‰‡

#### æ¨è–¦çš„ Log æ ¼å¼

```json
{
  "@timestamp": "2026-01-27T14:30:00.123+08:00",
  "level": "INFO",
  "logger": "com.company.order.OrderService",
  "thread": "http-nio-8080-exec-1",
  "message": "Order created successfully",
  "service": "order-service",
  "env": "prod",
  "host": "app-server-01",
  "traceId": "abc123def456",
  "spanId": "789xyz",
  "userId": "U12345",
  "orderId": "ORD-20260127-001",
  "duration": 150,
  "extra": {
    "orderAmount": 1500,
    "productCount": 3
  }
}
```

#### Log è¨­è¨ˆåŸå‰‡

| åŸå‰‡ | èªªæ˜ |
|------|------|
| **çµæ§‹åŒ–** | ä½¿ç”¨ JSON æ ¼å¼ï¼Œéç´”æ–‡å­— |
| **æ¨™æº–æ¬„ä½** | `@timestamp`ã€`level`ã€`service`ã€`traceId` å¿…å‚™ |
| **æ¥­å‹™æ¬„ä½** | åŒ…å«è¶³å¤ çš„æ¥­å‹™ Contextï¼ˆuserIdã€orderId ç­‰ï¼‰ |
| **å¯æœå°‹** | é—œéµæ¬„ä½è¨­ç‚º `keyword` é¡å‹ |
| **é©é‡** | é¿å… DEBUG Log é€²å…¥ Production |
| **å¯é—œè¯** | åŒ…å« traceId å¯èˆ‡å…¶ä»–ç³»çµ±é—œè¯ |

### 10.3 èˆ‡ AI åˆ†æçµåˆ

#### AI è¼”åŠ©æŸ¥è©¢ç¯„ä¾‹

```
ğŸ‘¤ ä½¿ç”¨è€…å•ï¼šã€Œæ˜¨å¤©æœ‰å¤šå°‘è¨‚å–®å¤±æ•—ï¼Ÿå¤±æ•—çš„ä¸»è¦åŸå› æ˜¯ä»€éº¼ï¼Ÿã€

ğŸ¤– AI è½‰æ›ç‚ºæŸ¥è©¢ï¼š
1. è¨ˆç®—å¤±æ•—è¨‚å–®æ•¸ï¼š
   service: order-service AND level: ERROR AND @timestamp >= now-1d/d AND @timestamp < now/d
   â†’ Aggregation: Count

2. åˆ†æå¤±æ•—åŸå› ï¼š
   service: order-service AND level: ERROR AND @timestamp >= now-1d/d
   â†’ Aggregation: Terms on error_type field

ğŸ“Š AI æ•´ç†çµæœï¼š
ã€Œæ˜¨å¤©å…±æœ‰ 156 ç­†è¨‚å–®å¤±æ•—ï¼Œä¸»è¦åŸå› ï¼š
  1. PaymentTimeout: 78 ç­† (50%)
  2. InventoryNotAvailable: 45 ç­† (29%)
  3. UserNotFound: 33 ç­† (21%)
  
  å»ºè­°ï¼šå„ªå…ˆèª¿æŸ¥ Payment Service çš„é€£ç·šå•é¡Œã€
```

#### å°‡ Log ä½œç‚º AI Prompt è¼¸å…¥

```
# ç¯„ä¾‹ï¼šå°‡ Error Log æ•´ç†çµ¦ AI åˆ†æ

Promptï¼š
"""
ä»¥ä¸‹æ˜¯ç³»çµ±éŒ¯èª¤æ—¥èªŒï¼Œè«‹åˆ†æå¯èƒ½çš„æ ¹å› ä¸¦æä¾›è§£æ±ºå»ºè­°ï¼š

[2026-01-27 14:30:01] ERROR OrderService - Failed to process order
  java.net.SocketTimeoutException: Read timed out
    at java.net.SocketInputStream.read(SocketInputStream.java:123)
    at com.company.payment.PaymentClient.charge(PaymentClient.java:89)
    at com.company.order.OrderService.processOrder(OrderService.java:45)

[2026-01-27 14:30:02] WARN ConnectionPool - Pool exhausted, waiting for connection
  pool_size: 10, active: 10, waiting: 5

[2026-01-27 14:30:03] ERROR PaymentClient - Connection refused to payment-service:8080
"""

AI å›æ‡‰ï¼š
ã€Œæ ¹æ“šæ—¥èªŒåˆ†æï¼Œå•é¡Œå¯èƒ½æ˜¯ï¼š
1. Payment Service é€£ç·šæ± è€—ç›¡ï¼ˆ10 å€‹é€£ç·šå…¨éƒ¨ç”¨å®Œï¼‰
2. Payment Service å¯èƒ½éè¼‰æˆ–ç•¶æ©Ÿ

å»ºè­°è¡Œå‹•ï¼š
1. æª¢æŸ¥ payment-service å¥åº·ç‹€æ…‹
2. å¢åŠ  Connection Pool å¤§å°
3. è¨­å®šé©ç•¶çš„ Connection Timeout
4. è€ƒæ…®åŠ å…¥ Circuit Breaker æ©Ÿåˆ¶ã€
```

### 10.4 èˆ‡ Prometheus / Grafana åˆ†å·¥

```mermaid
graph TB
    subgraph "å•é¡Œç™¼ç¾"
        P[Prometheus + Grafana]
        Note1[ç›£æ§æŒ‡æ¨™ç•°å¸¸<br/>Error Rate â†‘ / Latency â†‘]
    end
    
    subgraph "å•é¡Œåˆ†æ"
        E[ELK Stack]
        Note2[æŸ¥è©¢è©³ç´° Log<br/>æ‰¾åˆ° Error Message]
    end
    
    subgraph "æ ¹å› å®šä½"
        T[Tracing - Jaeger]
        Note3[è¿½è¹¤å®Œæ•´è«‹æ±‚éˆè·¯<br/>å®šä½ç“¶é ¸æœå‹™]
    end
    
    P --> E --> T
```

#### åˆ†å·¥å»ºè­°

| å•é¡Œé¡å‹ | ä½¿ç”¨å·¥å…· | èªªæ˜ |
|---------|---------|------|
| ç³»çµ±å±¤ç´šç›£æ§ | Prometheus + Grafana | CPUã€Memoryã€Diskã€Network |
| æ‡‰ç”¨å±¤ç´šç›£æ§ | Prometheus + Grafana | Request Rateã€Error Rateã€Latency |
| å•é¡Œè©³ç´°åˆ†æ | ELK | Error Logã€Stack Traceã€æ¥­å‹™ Log |
| è«‹æ±‚éˆè·¯è¿½è¹¤ | Jaeger / Zipkin | å¾®æœå‹™å‘¼å«éˆè·¯ã€æ•ˆèƒ½ç“¶é ¸ |
| å³æ™‚å‘Šè­¦ | Alertmanager | çµ±ä¸€å‘Šè­¦ç®¡é“ |
| æ­·å²è¶¨å‹¢åˆ†æ | Grafana + Kibana | é•·æœŸè¶¨å‹¢ã€å®¹é‡è¦åŠƒ |

---

> ğŸ’¡ **æœ¬ç« é‡é»**
> - é¿å…å¸¸è¦‹è¸©é›·é»ï¼šLog é‡ä¼°ç®—ã€ILM è¨­å®šã€Security å•Ÿç”¨
> - ä½¿ç”¨ JSON çµæ§‹åŒ– Logï¼ŒåŒ…å«è¶³å¤ æ¥­å‹™ Context
> - AI å¯è¼”åŠ© Log æŸ¥è©¢èˆ‡åˆ†æï¼Œä½†äººå·¥åˆ¤æ–·ä»ä¸å¯å°‘
> - ELK èˆ‡ Prometheus/Grafana äº’è£œä½¿ç”¨ï¼Œå„å¸å…¶è·

---

## é™„éŒ„ï¼šæª¢æŸ¥æ¸…å–®

### å®‰è£æª¢æŸ¥æ¸…å–®

- [ ] ä½œæ¥­ç³»çµ±ç¬¦åˆéœ€æ±‚ï¼ˆRHEL 7+ã€Ubuntu 18.04+ï¼‰
- [ ] ä¸‰å€‹å…ƒä»¶ç‰ˆæœ¬ä¸€è‡´ï¼ˆåŒä¸»ç‰ˆæœ¬è™Ÿï¼‰
- [ ] JVM Heap è¨­å®šæ­£ç¢ºï¼ˆå¯¦é«”è¨˜æ†¶é«” 50%ï¼Œä¸è¶…é 31 GBï¼‰
- [ ] `vm.max_map_count` è¨­å®šç‚º 262144
- [ ] `nofile` é™åˆ¶è¨­å®šç‚º 65536
- [ ] é˜²ç«ç‰†é–‹æ”¾å¿…è¦ Portï¼ˆ9200ã€9300ã€5044ã€5601ï¼‰
- [ ] ç£ç¢Ÿç©ºé–“å……è¶³ï¼ˆå»ºè­° > 100 GBï¼‰
- [ ] æœå‹™è¨­å®šé–‹æ©Ÿè‡ªå‹•å•Ÿå‹•

### è¨­å®šæª¢æŸ¥æ¸…å–®

- [ ] Elasticsearch cluster.name å·²è¨­å®š
- [ ] Elasticsearch discovery è¨­å®šæ­£ç¢ºï¼ˆå–®ç¯€é» / å¤šç¯€é»ï¼‰
- [ ] Logstash Pipeline èªæ³•é©—è­‰é€šé
- [ ] Logstash è¼¸å‡ºè‡³ Elasticsearch é€£ç·šæ­£å¸¸
- [ ] Kibana å¯é€£ç·šè‡³ Elasticsearch
- [ ] Index Template å·²å»ºç«‹
- [ ] ILM Policy å·²è¨­å®š
- [ ] æ•æ„Ÿè³‡æ–™é®è”½è¦å‰‡å·²è¨­å®š

### ä¸Šç·šæª¢æŸ¥æ¸…å–®

- [ ] Cluster Health ç‚º green
- [ ] Filebeat å·²éƒ¨ç½²è‡³ App Server
- [ ] è³‡æ–™å¯æ­£å¸¸æµå…¥ Elasticsearch
- [ ] Kibana å¯æŸ¥è©¢åˆ°è³‡æ–™
- [ ] Index Pattern / Data View å·²å»ºç«‹
- [ ] åŸºæœ¬ Dashboard å·²å»ºç«‹
- [ ] å‘Šè­¦è¦å‰‡å·²è¨­å®š
- [ ] å‚™ä»½ç­–ç•¥å·²è¨­å®šï¼ˆSnapshot + è¨­å®šæª”ï¼‰
- [ ] ç›£æ§å·²å•Ÿç”¨ï¼ˆPrometheus + Elasticsearch Exporterï¼‰
- [ ] Security å·²å•Ÿç”¨ï¼ˆèªè­‰ + TLSï¼‰

### ç¶­é‹æª¢æŸ¥æ¸…å–®ï¼ˆæ¯æ—¥ï¼‰

- [ ] Cluster Health ç‹€æ…‹
- [ ] Disk ä½¿ç”¨ç‡ < 80%
- [ ] JVM Heap ä½¿ç”¨ç‡ < 75%
- [ ] ç„¡ Unassigned Shards
- [ ] Logstash Pipeline ç„¡ç©å£“
- [ ] ç„¡å‘Šè­¦è§¸ç™¼

### å‡ç´šæª¢æŸ¥æ¸…å–®

- [ ] å‚™ä»½ Index Snapshot
- [ ] å‚™ä»½è¨­å®šæª”
- [ ] å‚™ä»½ Kibana Saved Objects
- [ ] é–±è®€ Release Notes
- [ ] æ¸¬è©¦ç’°å¢ƒé©—è­‰é€šé
- [ ] æº–å‚™å›æ»¾è¨ˆç•«
- [ ] é€šçŸ¥ç›¸é—œäººå“¡ç¶­è­·æ™‚é–“

---

## åƒè€ƒè³‡æº

- [Elastic å®˜æ–¹æ–‡ä»¶](https://www.elastic.co/guide/index.html)
- [Elasticsearch Reference](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html)
- [Logstash Reference](https://www.elastic.co/guide/en/logstash/current/index.html)
- [Kibana Guide](https://www.elastic.co/guide/en/kibana/current/index.html)
- [Filebeat Reference](https://www.elastic.co/guide/en/beats/filebeat/current/index.html)
- [Elastic Support Matrix](https://www.elastic.co/support/matrix)


