+++
date = '2026-01-30T19:37:57+08:00'
draft = false
title = 'Apache Kafka æ•™å­¸æ‰‹å†Š'
tags = ['æ•™å­¸', 'å·¥å…·', 'Kafka']
categories = ['æ•™å­¸']
+++

# Apache Kafka æ•™å­¸æ‰‹å†Š

> **ç‰ˆæœ¬**ï¼š1.0  
> **æœ€å¾Œæ›´æ–°**ï¼š2026 å¹´ 1 æœˆ  
> **é©ç”¨å°è±¡**ï¼šå¾Œç«¯å·¥ç¨‹å¸«ã€ç³»çµ±æ¶æ§‹å¸«ã€SREã€DevOps    
> **å®šä½**ï¼šä¼æ¥­å…§éƒ¨æ¨™æº–æ•™æ 
> **æ–‡ä»¶ç¶­è­·**ï¼šå…§éƒ¨æŠ€è¡“åœ˜éšŠ
> **ä½¿ç”¨æƒ…å¢ƒ**ï¼šå¤§å‹ä¼æ¥­ / éŠ€è¡Œå…§éƒ¨ç³»çµ±
> **æœ€å¾Œæ›´æ–°**: 2026å¹´1æœˆ29æ—¥  
> **é©ç”¨æ–¼**: é©ç”¨æ–¼ Kafka 3.xï¼ˆå« KRaft æ¶æ§‹ï¼‰
> **Created by**: Eric Cheng


## ç›®éŒ„

1. [Apache Kafka ç°¡ä»‹](#1-apache-kafka-ç°¡ä»‹)
   - [1.1 Kafka æ˜¯ä»€éº¼ï¼Ÿè§£æ±ºä»€éº¼å•é¡Œï¼Ÿ](#11-kafka-æ˜¯ä»€éº¼è§£æ±ºä»€éº¼å•é¡Œ)
   - [1.2 èˆ‡å‚³çµ± Message Queue çš„å·®ç•°](#12-èˆ‡å‚³çµ±-message-queue-çš„å·®ç•°)
   - [1.3 é©åˆèˆ‡ä¸é©åˆçš„ä½¿ç”¨æƒ…å¢ƒ](#13-é©åˆèˆ‡ä¸é©åˆçš„ä½¿ç”¨æƒ…å¢ƒ)
2. [Kafka ç³»çµ±æ¶æ§‹ç¸½è¦½](#2-kafka-ç³»çµ±æ¶æ§‹ç¸½è¦½)
   - [2.1 Kafka æ ¸å¿ƒå…ƒä»¶èªªæ˜](#21-kafka-æ ¸å¿ƒå…ƒä»¶èªªæ˜)
   - [2.2 é«˜å¯ç”¨ï¼ˆHAï¼‰èˆ‡æ°´å¹³æ“´å……è¨­è¨ˆåŸå‰‡](#22-é«˜å¯ç”¨haèˆ‡æ°´å¹³æ“´å……è¨­è¨ˆåŸå‰‡)
3. [Kafka å®‰è£èˆ‡éƒ¨ç½²](#3-kafka-å®‰è£èˆ‡éƒ¨ç½²)
   - [3.1 ç’°å¢ƒéœ€æ±‚](#31-ç’°å¢ƒéœ€æ±‚)
   - [3.2 å–®æ©Ÿç’°å¢ƒå®‰è£ï¼ˆKRaft æ¨¡å¼ï¼‰](#32-å–®æ©Ÿç’°å¢ƒå®‰è£kraft-æ¨¡å¼)
   - [3.3 å¤šç¯€é»å¢é›†å®‰è£ï¼ˆæ­£å¼ç’°å¢ƒï¼‰](#33-å¤šç¯€é»å¢é›†å®‰è£æ­£å¼ç’°å¢ƒ)
   - [3.4 ZooKeeper èˆ‡ KRaft æ¶æ§‹æ¯”è¼ƒ](#34-zookeeper-èˆ‡-kraft-æ¶æ§‹æ¯”è¼ƒ)
   - [3.5 å¸¸è¦‹å®‰è£éŒ¯èª¤èˆ‡æ’é™¤æ–¹å¼](#35-å¸¸è¦‹å®‰è£éŒ¯èª¤èˆ‡æ’é™¤æ–¹å¼)
4. [Kafka åŸºæœ¬è¨­å®šèªªæ˜](#4-kafka-åŸºæœ¬è¨­å®šèªªæ˜)
   - [4.1 Broker é‡è¦è¨­å®šåƒæ•¸](#41-broker-é‡è¦è¨­å®šåƒæ•¸)
   - [4.2 Topic è¨­è¨ˆåŸå‰‡](#42-topic-è¨­è¨ˆåŸå‰‡)
   - [4.3 Producer é‡è¦è¨­å®š](#43-producer-é‡è¦è¨­å®š)
   - [4.4 Consumer é‡è¦è¨­å®š](#44-consumer-é‡è¦è¨­å®š)
   - [4.5 è³‡æ–™ä¿ç•™ç­–ç•¥ï¼ˆRetention Policyï¼‰](#45-è³‡æ–™ä¿ç•™ç­–ç•¥retention-policy)
5. [Kafka ç³»çµ±ä½¿ç”¨æ•™å­¸](#5-kafka-ç³»çµ±ä½¿ç”¨æ•™å­¸)
   - [5.1 Topic ç®¡ç†](#51-topic-ç®¡ç†)
   - [5.2 Producer ç™¼é€è¨Šæ¯](#52-producer-ç™¼é€è¨Šæ¯)
   - [5.3 Consumer æ¶ˆè²»è¨Šæ¯](#53-consumer-æ¶ˆè²»è¨Šæ¯)
   - [5.4 Offset ç®¡ç†](#54-offset-ç®¡ç†)
   - [5.5 è¨Šæ¯é †åºæ€§èˆ‡é‡è¤‡æ¶ˆè²»](#55-è¨Šæ¯é †åºæ€§èˆ‡é‡è¤‡æ¶ˆè²»)
6. [Kafka èˆ‡æ‡‰ç”¨ç³»çµ±ä¸²æ¥æ–¹å¼](#6-kafka-èˆ‡æ‡‰ç”¨ç³»çµ±ä¸²æ¥æ–¹å¼)
   - [6.1 èˆ‡ Spring Boot æ•´åˆ](#61-èˆ‡-spring-boot-æ•´åˆ)
   - [6.2 ç³»çµ±è§£è€¦æ¶æ§‹è¨­è¨ˆ](#62-ç³»çµ±è§£è€¦æ¶æ§‹è¨­è¨ˆ)
   - [6.3 åŒæ­¥ç³»çµ± vs äº‹ä»¶é©…å‹•æ¶æ§‹](#63-åŒæ­¥ç³»çµ±-vs-äº‹ä»¶é©…å‹•æ¶æ§‹)
   - [6.4 å¸¸è¦‹æ•´åˆæ¶æ§‹æ¨¡å¼](#64-å¸¸è¦‹æ•´åˆæ¶æ§‹æ¨¡å¼)
7. [Kafka ç³»çµ±ç¶­é‹èˆ‡ç›£æ§](#7-kafka-ç³»çµ±ç¶­é‹èˆ‡ç›£æ§)
   - [7.1 å¸¸è¦‹ç›£æ§æŒ‡æ¨™](#71-å¸¸è¦‹ç›£æ§æŒ‡æ¨™)
   - [7.2 Consumer Lag ç›£æ§èˆ‡è™•ç†](#72-consumer-lag-ç›£æ§èˆ‡è™•ç†)
   - [7.3 ç³»çµ±ç›£æ§è¨­å®š](#73-ç³»çµ±ç›£æ§è¨­å®š)
   - [7.4 å¸¸è¦‹ç‡Ÿé‹å•é¡Œèˆ‡æ’æŸ¥](#74-å¸¸è¦‹ç‡Ÿé‹å•é¡Œèˆ‡æ’æŸ¥)
8. [Kafka ç³»çµ±å‡ç´šèˆ‡ç‰ˆæœ¬æ§ç®¡](#8-kafka-ç³»çµ±å‡ç´šèˆ‡ç‰ˆæœ¬æ§ç®¡)
   - [8.1 å‡ç´šç­–ç•¥ï¼ˆRolling Upgradeï¼‰](#81-å‡ç´šç­–ç•¥rolling-upgrade)
   - [8.2 å‡ç´šå‰æª¢æŸ¥æ¸…å–®](#82-å‡ç´šå‰æª¢æŸ¥æ¸…å–®)
   - [8.3 å‡ç´šé¢¨éšªèˆ‡å›å¾©æ©Ÿåˆ¶](#83-å‡ç´šé¢¨éšªèˆ‡å›å¾©æ©Ÿåˆ¶)
   - [8.4 Client ç›¸å®¹æ€§](#84-client-ç›¸å®¹æ€§)
9. [å®‰å…¨æ€§èˆ‡æ¬Šé™æ§ç®¡](#9-å®‰å…¨æ€§èˆ‡æ¬Šé™æ§ç®¡)
   - [9.1 SSL/TLS åŠ å¯†](#91-ssltls-åŠ å¯†)
   - [9.2 SASL èªè­‰](#92-sasl-èªè­‰)
   - [9.3 ACL æ¬Šé™æ§ç®¡](#93-acl-æ¬Šé™æ§ç®¡)
   - [9.4 ä¼æ¥­å®‰å…¨è¨­è¨ˆå»ºè­°](#94-ä¼æ¥­å®‰å…¨è¨­è¨ˆå»ºè­°)
10. [æœ€ä½³å¯¦å‹™èˆ‡å¸¸è¦‹åœ°é›·](#10-æœ€ä½³å¯¦å‹™èˆ‡å¸¸è¦‹åœ°é›·)
    - [10.1 Topic å‘½åå»ºè­°](#101-topic-å‘½åå»ºè­°)
    - [10.2 Partition è¨­è¨ˆåœ°é›·](#102-partition-è¨­è¨ˆåœ°é›·)
    - [10.3 Consumer Group éŒ¯èª¤æ¡ˆä¾‹](#103-consumer-group-éŒ¯èª¤æ¡ˆä¾‹)
    - [10.4 çœŸå¯¦å°ˆæ¡ˆå¸¸è¦‹èª¤ç”¨æƒ…å¢ƒ](#104-çœŸå¯¦å°ˆæ¡ˆå¸¸è¦‹èª¤ç”¨æƒ…å¢ƒ)
    - [10.5 æœ€ä½³å¯¦å‹™ç¸½çµ](#105-æœ€ä½³å¯¦å‹™ç¸½çµ)
11. [æª¢æŸ¥æ¸…å–®ï¼ˆChecklistï¼‰](#11-æª¢æŸ¥æ¸…å–®checklist)
    - [11.1 æ–°å°ˆæ¡ˆå°å…¥ Checklist](#111-æ–°å°ˆæ¡ˆå°å…¥-checklist)
    - [11.2 æ—¥å¸¸ç¶­é‹ Checklist](#112-æ—¥å¸¸ç¶­é‹-checklist)
    - [11.3 æ•…éšœæ’é™¤ Checklist](#113-æ•…éšœæ’é™¤-checklist)
    - [11.4 å‡ç´š Checklist](#114-å‡ç´š-checklist)
12. [é™„éŒ„](#é™„éŒ„)
    - [é™„éŒ„ Aï¼šå¸¸ç”¨æŒ‡ä»¤é€ŸæŸ¥](#é™„éŒ„-aå¸¸ç”¨æŒ‡ä»¤é€ŸæŸ¥)
    - [é™„éŒ„ Bï¼šè¨­å®šåƒæ•¸é€ŸæŸ¥](#é™„éŒ„-bè¨­å®šåƒæ•¸é€ŸæŸ¥)
    - [é™„éŒ„ Cï¼šåƒè€ƒè³‡æº](#é™„éŒ„-cåƒè€ƒè³‡æº)

---

## 1. Apache Kafka ç°¡ä»‹

### 1.1 Kafka æ˜¯ä»€éº¼ï¼Ÿè§£æ±ºä»€éº¼å•é¡Œï¼Ÿ

Apache Kafka æ˜¯ä¸€å€‹**åˆ†æ•£å¼äº‹ä»¶ä¸²æµå¹³å°ï¼ˆDistributed Event Streaming Platformï¼‰**ï¼Œç”± LinkedIn æ–¼ 2011 å¹´é–‹æºï¼Œç¾ç”± Apache è»Ÿé«”åŸºé‡‘æœƒç¶­è­·ã€‚

#### Kafka çš„æ ¸å¿ƒèƒ½åŠ›

| èƒ½åŠ› | èªªæ˜ |
|------|------|
| **ç™¼å¸ƒ/è¨‚é–±ï¼ˆPub/Subï¼‰** | è®“å¤šå€‹ç³»çµ±èƒ½å¤ ç™¼å¸ƒèˆ‡è¨‚é–±äº‹ä»¶è¨Šæ¯ |
| **æŒä¹…åŒ–å„²å­˜** | è¨Šæ¯å¯æŒä¹…åŒ–åˆ°ç£ç¢Ÿï¼Œæ”¯æ´é‡æ’­ï¼ˆReplayï¼‰ |
| **ä¸²æµè™•ç†** | æ”¯æ´å³æ™‚è³‡æ–™è™•ç†èˆ‡è½‰æ› |
| **é«˜ååé‡** | æ¯ç§’å¯è™•ç†ç™¾è¬ç´šè¨Šæ¯ |
| **æ°´å¹³æ“´å±•** | é€éå¢åŠ ç¯€é»è¼•é¬†æ“´å±•å®¹é‡ |

#### Kafka è§£æ±ºçš„å•é¡Œ

```mermaid
flowchart LR
    subgraph å‚³çµ±æ¶æ§‹
        A1[ç³»çµ± A] --> B1[ç³»çµ± B]
        A1 --> C1[ç³»çµ± C]
        A1 --> D1[ç³»çµ± D]
        B1 --> C1
        B1 --> D1
        C1 --> D1
    end
```

**å•é¡Œï¼šé»å°é»æ•´åˆå°è‡´ç³»çµ±è€¦åˆåš´é‡ï¼Œé›£ä»¥ç¶­è­·**

```mermaid
flowchart LR
    subgraph Kafka æ¶æ§‹
        A2[ç³»çµ± A] --> K[Kafka]
        B2[ç³»çµ± B] --> K
        K --> C2[ç³»çµ± C]
        K --> D2[ç³»çµ± D]
        K --> E2[ç³»çµ± E]
    end
```

**è§£æ±ºæ–¹æ¡ˆï¼šé€é Kafka ä½œç‚ºä¸­å¤®äº‹ä»¶åŒ¯æµæ’ï¼Œå¯¦ç¾ç³»çµ±è§£è€¦**

### 1.2 èˆ‡å‚³çµ± Message Queue çš„å·®ç•°

| ç‰¹æ€§ | Kafka | RabbitMQ | ActiveMQ |
|------|-------|----------|----------|
| **è¨Šæ¯æ¨¡å‹** | Log-basedï¼ˆæ—¥èªŒå‹ï¼‰ | Queue-basedï¼ˆä½‡åˆ—å‹ï¼‰ | Queue-based |
| **è¨Šæ¯ä¿ç•™** | ä¾æ™‚é–“/å¤§å°ä¿ç•™ï¼Œå¯é‡æ’­ | æ¶ˆè²»å¾Œåˆªé™¤ | æ¶ˆè²»å¾Œåˆªé™¤ |
| **ååé‡** | æ¥µé«˜ï¼ˆç™¾è¬/ç§’ï¼‰ | ä¸­ç­‰ï¼ˆè¬/ç§’ï¼‰ | ä¸­ç­‰ |
| **é †åºä¿è­‰** | Partition å…§ä¿è­‰é †åº | å–®ä¸€ Queue ä¿è­‰ | å–®ä¸€ Queue ä¿è­‰ |
| **Consumer æ¨¡å¼** | Pullï¼ˆæ‹‰å–ï¼‰ | Push/Pull | Push/Pull |
| **å¢é›†æ¶æ§‹** | åŸç”Ÿåˆ†æ•£å¼ | éœ€é¡å¤–é…ç½® | éœ€é¡å¤–é…ç½® |
| **é©ç”¨å ´æ™¯** | å¤§æ•¸æ“šã€äº‹ä»¶ä¸²æµã€æ—¥èªŒæ”¶é›† | ä»»å‹™ä½‡åˆ—ã€RPC | ä¼æ¥­æ•´åˆ |

#### é—œéµå·®ç•°èªªæ˜

1. **Log-based vs Queue-based**
   - Kafkaï¼šè¨Šæ¯å¯«å…¥å¾Œä¿ç•™åœ¨ Log ä¸­ï¼Œå¤šå€‹ Consumer å¯ç¨ç«‹æ¶ˆè²»åŒä¸€ä»½è³‡æ–™
   - RabbitMQï¼šè¨Šæ¯è¢«æ¶ˆè²»å¾Œå³å¾ Queue ç§»é™¤

2. **Consumer Group æ©Ÿåˆ¶**
   - Kafka çš„ Consumer Group å…è¨±åŒä¸€ç¾¤çµ„å…§çš„ Consumer åˆ†æ“” Partition
   - ä¸åŒç¾¤çµ„å¯ç¨ç«‹æ¶ˆè²»ç›¸åŒè¨Šæ¯ï¼Œå¯¦ç¾å»£æ’­æ•ˆæœ

### 1.3 é©åˆèˆ‡ä¸é©åˆçš„ä½¿ç”¨æƒ…å¢ƒ

#### âœ… é©åˆ Kafka çš„æƒ…å¢ƒ

| æƒ…å¢ƒ | èªªæ˜ |
|------|------|
| **äº‹ä»¶é©…å‹•æ¶æ§‹ï¼ˆEDAï¼‰** | ç³»çµ±é–“é€éäº‹ä»¶é€šè¨Šï¼Œå¯¦ç¾é¬†è€¦åˆ |
| **æ—¥èªŒæ”¶é›†èˆ‡èšåˆ** | é›†ä¸­æ”¶é›†å¤šç³»çµ±æ—¥èªŒï¼Œä¾›åˆ†æå¹³å°ä½¿ç”¨ |
| **å³æ™‚è³‡æ–™ç®¡é“** | å°‡è³‡æ–™å¾ä¾†æºç³»çµ±å³æ™‚å‚³è¼¸è‡³ç›®æ¨™ç³»çµ± |
| **CDCï¼ˆChange Data Captureï¼‰** | æ•æ‰è³‡æ–™åº«è®Šæ›´ï¼ŒåŒæ­¥è‡³å…¶ä»–ç³»çµ± |
| **æŒ‡æ¨™æ”¶é›†** | æ”¶é›†æ‡‰ç”¨ç¨‹å¼æŒ‡æ¨™ï¼Œä¾›ç›£æ§ç³»çµ±ä½¿ç”¨ |
| **å¾®æœå‹™é€šè¨Š** | ä½œç‚ºå¾®æœå‹™é–“çš„éåŒæ­¥é€šè¨Šç®¡é“ |

#### âŒ ä¸é©åˆ Kafka çš„æƒ…å¢ƒ

| æƒ…å¢ƒ | åŸå›  | æ›¿ä»£æ–¹æ¡ˆ |
|------|------|----------|
| **éœ€è¦å³æ™‚å›æ‡‰çš„ Request-Response** | Kafka æ˜¯éåŒæ­¥è¨­è¨ˆï¼Œå»¶é²è¼ƒé«˜ | REST APIã€gRPC |
| **å°é‡è¨Šæ¯ä¸”éœ€ä½å»¶é²** | Kafka çš„æ‰¹æ¬¡è™•ç†è¨­è¨ˆä¸é©åˆ | RabbitMQã€Redis |
| **è¤‡é›œçš„è¨Šæ¯è·¯ç”±** | Kafka è·¯ç”±èƒ½åŠ›è¼ƒå¼± | RabbitMQï¼ˆExchangeï¼‰ |
| **äº‹å‹™æ€§è¨Šæ¯è™•ç†** | é›–æ”¯æ´ä½†è¼ƒè¤‡é›œ | å‚³çµ± MQ + è³‡æ–™åº«äº‹å‹™ |
| **è¨Šæ¯éœ€è¦å„ªå…ˆç´š** | Kafka ä¸æ”¯æ´è¨Šæ¯å„ªå…ˆç´š | RabbitMQ |

### ğŸ’¡ å¯¦å‹™å»ºè­°

> **é‡‘èæ¥­å°å…¥è€ƒé‡**ï¼š
> - Kafka éå¸¸é©åˆç”¨æ–¼äº¤æ˜“æ—¥èªŒã€é¢¨æ§äº‹ä»¶ã€è·¨ç³»çµ±è³‡æ–™åŒæ­¥
> - ä½†å°æ–¼éœ€è¦åš´æ ¼äº‹å‹™ä¿è­‰çš„æ ¸å¿ƒäº¤æ˜“ç³»çµ±ï¼Œå»ºè­°æ­é…å…¶ä»–æ©Ÿåˆ¶ç¢ºä¿ä¸€è‡´æ€§
> - å»ºè­°å…ˆåœ¨éæ ¸å¿ƒç³»çµ±é©—è­‰ï¼Œå†é€æ­¥æ“´å±•è‡³é—œéµç³»çµ±

---

## 2. Kafka ç³»çµ±æ¶æ§‹ç¸½è¦½

### 2.1 Kafka æ ¸å¿ƒå…ƒä»¶èªªæ˜

```mermaid
flowchart TB
    subgraph Kafka Cluster
        subgraph Broker1[Broker 1]
            T1P0[Topic1-P0 Leader]
            T1P1[Topic1-P1 Follower]
            T2P0[Topic2-P0 Follower]
        end
        subgraph Broker2[Broker 2]
            T1P0F[Topic1-P0 Follower]
            T1P1L[Topic1-P1 Leader]
            T2P0L[Topic2-P0 Leader]
        end
        subgraph Broker3[Broker 3]
            T1P0F2[Topic1-P0 Follower]
            T1P1F[Topic1-P1 Follower]
            T2P0F[Topic2-P0 Follower]
        end
    end
    
    subgraph Producers
        P1[Producer 1]
        P2[Producer 2]
    end
    
    subgraph Consumers
        CG1[Consumer Group 1]
        CG2[Consumer Group 2]
    end
    
    P1 --> Broker1
    P2 --> Broker2
    Broker1 --> CG1
    Broker2 --> CG1
    Broker1 --> CG2
    Broker2 --> CG2
    
    Controller[Controller<br/>Broker 1] -.-> Broker2
    Controller -.-> Broker3
```

#### 2.1.1 Broker

**Broker** æ˜¯ Kafka å¢é›†ä¸­çš„ä¼ºæœå™¨ç¯€é»ï¼Œè² è²¬ï¼š
- æ¥æ”¶ Producer ç™¼é€çš„è¨Šæ¯
- å°‡è¨Šæ¯æŒä¹…åŒ–è‡³ç£ç¢Ÿ
- æä¾›è¨Šæ¯çµ¦ Consumer è®€å–
- èˆ‡å…¶ä»– Broker é€²è¡Œè³‡æ–™è¤‡å¯«

```
ä¸€å€‹ Kafka å¢é›†é€šå¸¸ç”± 3 å€‹ä»¥ä¸Šçš„ Broker çµ„æˆ
```

#### 2.1.2 Topic èˆ‡ Partition

**Topicï¼ˆä¸»é¡Œï¼‰**ï¼š
- è¨Šæ¯çš„é‚è¼¯åˆ†é¡ï¼Œé¡ä¼¼è³‡æ–™åº«çš„ Table
- ä¾‹å¦‚ï¼š`order-events`ã€`user-activities`ã€`system-logs`

**Partitionï¼ˆåˆ†å€ï¼‰**ï¼š
- Topic çš„ç‰©ç†åˆ†å‰²å–®ä½
- æ¯å€‹ Partition æ˜¯ä¸€å€‹æœ‰åºçš„ã€ä¸å¯è®Šçš„è¨Šæ¯åºåˆ—
- Partition å…§çš„è¨Šæ¯æœ‰å”¯ä¸€çš„ Offsetï¼ˆåç§»é‡ï¼‰

```mermaid
flowchart LR
    subgraph Topic: order-events
        subgraph P0[Partition 0]
            M0[Offset 0] --> M1[Offset 1] --> M2[Offset 2] --> M3[Offset 3]
        end
        subgraph P1[Partition 1]
            M4[Offset 0] --> M5[Offset 1] --> M6[Offset 2]
        end
        subgraph P2[Partition 2]
            M7[Offset 0] --> M8[Offset 1]
        end
    end
```

**Partition è¨­è¨ˆåŸå‰‡**ï¼š
- æ±ºå®šäº† Consumer çš„æœ€å¤§ä¸¦è¡Œåº¦
- ä¸€å€‹ Partition åªèƒ½è¢«åŒä¸€ Consumer Group ä¸­çš„ä¸€å€‹ Consumer æ¶ˆè²»
- Partition æ•¸é‡ä¸€æ—¦è¨­å®šï¼Œå¢åŠ å®¹æ˜“ä½†æ¸›å°‘å›°é›£

#### 2.1.3 Producer

**Producerï¼ˆç”Ÿç”¢è€…ï¼‰** è² è²¬å°‡è¨Šæ¯ç™¼é€è‡³ Kafkaï¼š

```mermaid
flowchart LR
    P[Producer] -->|1. é¸æ“‡ Partition| PL[Partitioner]
    PL -->|2. ç™¼é€è‡³ Leader| B[Broker Leader]
    B -->|3. è¤‡å¯«è‡³ Follower| BF[Broker Follower]
    B -->|4. å›å‚³ ACK| P
```

**Partition é¸æ“‡ç­–ç•¥**ï¼š
| ç­–ç•¥ | èªªæ˜ |
|------|------|
| **æŒ‡å®š Partition** | ç›´æ¥æŒ‡å®šç›®æ¨™ Partition |
| **Key Hash** | æ ¹æ“šè¨Šæ¯ Key çš„ Hash å€¼é¸æ“‡ï¼ˆåŒ Key åŒ Partitionï¼‰ |
| **Round Robin** | è¼ªè©¢åˆ†é…ï¼ˆç„¡ Key æ™‚çš„é è¨­è¡Œç‚ºï¼‰ |
| **Sticky** | é»æ€§åˆ†å€ï¼Œæ¸›å°‘æ‰¹æ¬¡æ•¸é‡ |

#### 2.1.4 Consumer èˆ‡ Consumer Group

**Consumerï¼ˆæ¶ˆè²»è€…ï¼‰**ï¼šå¾ Kafka è®€å–è¨Šæ¯çš„å®¢æˆ¶ç«¯

**Consumer Groupï¼ˆæ¶ˆè²»è€…ç¾¤çµ„ï¼‰**ï¼š
- ä¸€çµ„ Consumer çš„é‚è¼¯é›†åˆ
- ç¾¤çµ„å…§çš„ Consumer å…±åŒæ¶ˆè²»ä¸€å€‹ Topic
- æ¯å€‹ Partition åªæœƒè¢«ç¾¤çµ„å…§çš„ä¸€å€‹ Consumer æ¶ˆè²»

```mermaid
flowchart TB
    subgraph Topic
        P0[Partition 0]
        P1[Partition 1]
        P2[Partition 2]
        P3[Partition 3]
    end
    
    subgraph Consumer Group A
        CA1[Consumer A1]
        CA2[Consumer A2]
    end
    
    subgraph Consumer Group B
        CB1[Consumer B1]
    end
    
    P0 --> CA1
    P1 --> CA1
    P2 --> CA2
    P3 --> CA2
    
    P0 --> CB1
    P1 --> CB1
    P2 --> CB1
    P3 --> CB1
```

**é‡é»**ï¼š
- Consumer Group A æœ‰ 2 å€‹ Consumerï¼Œåˆ†æ“” 4 å€‹ Partition
- Consumer Group B æœ‰ 1 å€‹ Consumerï¼Œæ¶ˆè²»å…¨éƒ¨ 4 å€‹ Partition
- å…©å€‹ Group ç¨ç«‹æ¶ˆè²»ï¼Œäº’ä¸å½±éŸ¿

#### 2.1.5 Controller

**Controller** æ˜¯å¢é›†ä¸­è² è²¬ç®¡ç†å·¥ä½œçš„ç‰¹æ®Š Brokerï¼š
- ç®¡ç† Partition çš„ Leader é¸èˆ‰
- è™•ç† Broker çš„åŠ å…¥èˆ‡é›¢é–‹
- ç›£æ§å¢é›†ç‹€æ…‹
- åŒä¸€æ™‚é–“åªæœ‰ä¸€å€‹ Controllerï¼ˆé€é ZooKeeper/KRaft é¸èˆ‰ï¼‰

### 2.2 é«˜å¯ç”¨ï¼ˆHAï¼‰èˆ‡æ°´å¹³æ“´å……è¨­è¨ˆåŸå‰‡

#### 2.2.1 å‰¯æœ¬æ©Ÿåˆ¶ï¼ˆReplicationï¼‰

```mermaid
flowchart LR
    subgraph Partition å‰¯æœ¬
        L[Leader Replica<br/>è™•ç†è®€å¯«]
        F1[Follower Replica 1<br/>åŒæ­¥è¤‡å¯«]
        F2[Follower Replica 2<br/>åŒæ­¥è¤‡å¯«]
    end
    
    P[Producer] -->|å¯«å…¥| L
    L -->|è¤‡å¯«| F1
    L -->|è¤‡å¯«| F2
    C[Consumer] -->|è®€å–| L
```

**é—œéµåè©**ï¼š

| åè© | èªªæ˜ |
|------|------|
| **Replication Factor** | å‰¯æœ¬æ•¸é‡ï¼ˆå»ºè­° â‰¥ 3ï¼‰ |
| **Leader Replica** | è™•ç†æ‰€æœ‰è®€å¯«è«‹æ±‚çš„å‰¯æœ¬ |
| **Follower Replica** | å¾ Leader åŒæ­¥è³‡æ–™çš„å‰¯æœ¬ |
| **ISRï¼ˆIn-Sync Replicasï¼‰** | èˆ‡ Leader ä¿æŒåŒæ­¥çš„å‰¯æœ¬é›†åˆ |
| **Min ISR** | æœ€å°åŒæ­¥å‰¯æœ¬æ•¸ï¼Œä½æ–¼æ­¤å€¼å°‡æ‹’çµ•å¯«å…¥ |

#### 2.2.2 é«˜å¯ç”¨è¨­è¨ˆåŸå‰‡

```
å»ºè­°é…ç½®ï¼š
- Broker æ•¸é‡ â‰¥ 3
- Replication Factor = 3
- Min ISR = 2
- acks = allï¼ˆæˆ– -1ï¼‰
```

**æ•…éšœå®¹å¿è¨ˆç®—**ï¼š
```
å¯å®¹å¿æ•…éšœæ•¸ = Replication Factor - Min ISR
ä¾‹å¦‚ï¼šRF=3, Min ISR=2 â†’ å¯å®¹å¿ 1 å° Broker æ•…éšœ
```

#### 2.2.3 æ°´å¹³æ“´å……åŸå‰‡

| æ“´å……æ–¹å¼ | å½±éŸ¿ | æ³¨æ„äº‹é … |
|----------|------|----------|
| **å¢åŠ  Broker** | æå‡æ•´é«”å®¹é‡èˆ‡ååé‡ | éœ€é€²è¡Œ Partition é‡æ–°åˆ†é… |
| **å¢åŠ  Partition** | æå‡ Consumer ä¸¦è¡Œåº¦ | Partition åªèƒ½å¢åŠ ä¸èƒ½æ¸›å°‘ |
| **å¢åŠ  Consumer** | æå‡æ¶ˆè²»é€Ÿåº¦ | Consumer æ•¸é‡ â‰¤ Partition æ•¸é‡ |

### ğŸ’¡ å¯¦å‹™å»ºè­°

> **æ¶æ§‹è¨­è¨ˆå»ºè­°**ï¼š
> 1. ç”Ÿç”¢ç’°å¢ƒè‡³å°‘ 3 å€‹ Brokerï¼Œåˆ†å¸ƒåœ¨ä¸åŒæ©Ÿæ¶æˆ–å¯ç”¨å€
> 2. Partition æ•¸é‡å»ºè­°ç‚º Broker æ•¸é‡çš„å€æ•¸
> 3. é ä¼°æœªä¾†æµé‡æˆé•·ï¼Œé ç•™è¶³å¤ çš„ Partition æ•¸é‡
> 4. ç›£æ§ ISR æ•¸é‡ï¼Œä½æ–¼ Min ISR æ‡‰ç«‹å³è™•ç†

---

## 3. Kafka å®‰è£èˆ‡éƒ¨ç½²

### 3.1 ç’°å¢ƒéœ€æ±‚

#### 3.1.1 ç¡¬é«”éœ€æ±‚

| ç’°å¢ƒ | CPU | è¨˜æ†¶é«” | ç£ç¢Ÿ | ç¶²è·¯ |
|------|-----|--------|------|------|
| **é–‹ç™¼/æ¸¬è©¦** | 2 cores | 4 GB | 50 GB SSD | 1 Gbps |
| **å°å‹ç”Ÿç”¢** | 4 cores | 8 GB | 500 GB SSD | 1 Gbps |
| **å¤§å‹ç”Ÿç”¢** | 16+ cores | 32+ GB | å¤šé¡† SSDï¼ˆRAID 10ï¼‰ | 10 Gbps |

#### 3.1.2 è»Ÿé«”éœ€æ±‚

| è»Ÿé«” | ç‰ˆæœ¬è¦æ±‚ | èªªæ˜ |
|------|----------|------|
| **ä½œæ¥­ç³»çµ±** | Linuxï¼ˆCentOS 7+ã€Ubuntu 18.04+ï¼‰ | å»ºè­°ä½¿ç”¨ Linux |
| **JDK** | Java 11 æˆ– Java 17ï¼ˆLTSï¼‰ | Kafka 3.x å»ºè­° Java 17 |
| **ZooKeeper** | 3.6+ | Kafka 3.4+ å¯ä½¿ç”¨ KRaft æ¨¡å¼ |

#### 3.1.3 ç¶²è·¯éœ€æ±‚

```
Kafka é è¨­ä½¿ç”¨ä»¥ä¸‹ Portï¼š
- 9092ï¼šBroker å°å¤–æœå‹™ï¼ˆPLAINTEXTï¼‰
- 9093ï¼šBroker å°å¤–æœå‹™ï¼ˆSSLï¼‰
- 9094ï¼šBroker é–“é€šè¨Š
- 2181ï¼šZooKeeperï¼ˆè‹¥ä½¿ç”¨ï¼‰
```

### 3.2 å–®æ©Ÿç’°å¢ƒå®‰è£ï¼ˆKRaft æ¨¡å¼ï¼‰

> **èªªæ˜**ï¼šKafka 3.4+ æ”¯æ´ KRaft æ¨¡å¼ï¼Œç„¡éœ€ ZooKeeper

#### æ­¥é©Ÿ 1ï¼šä¸‹è¼‰ä¸¦è§£å£“ç¸®

```bash
# ä¸‹è¼‰ Kafka
cd /opt
wget https://downloads.apache.org/kafka/3.7.0/kafka_2.13-3.7.0.tgz

# è§£å£“ç¸®
tar -xzf kafka_2.13-3.7.0.tgz
ln -s kafka_2.13-3.7.0 kafka

# è¨­å®šç’°å¢ƒè®Šæ•¸
echo 'export KAFKA_HOME=/opt/kafka' >> ~/.bashrc
echo 'export PATH=$PATH:$KAFKA_HOME/bin' >> ~/.bashrc
source ~/.bashrc
```

#### æ­¥é©Ÿ 2ï¼šç”¢ç”Ÿ Cluster ID

```bash
# ç”¢ç”Ÿå”¯ä¸€çš„ Cluster ID
KAFKA_CLUSTER_ID="$(kafka-storage.sh random-uuid)"
echo $KAFKA_CLUSTER_ID
# è¼¸å‡ºç¯„ä¾‹ï¼šMkU3OEVBNTcwNTJENDM2Qk
```

#### æ­¥é©Ÿ 3ï¼šè¨­å®š KRaft æ¨¡å¼

ç·¨è¼¯ `/opt/kafka/config/kraft/server.properties`ï¼š

```properties
# ç¯€é» IDï¼ˆæ¯å€‹ç¯€é»å¿…é ˆå”¯ä¸€ï¼‰
node.id=1

# ç¯€é»è§’è‰²ï¼šbroker, controller, æˆ–å…©è€…
process.roles=broker,controller

# Controller é¸èˆ‰æŠ•ç¥¨è€…
controller.quorum.voters=1@localhost:9093

# ç›£è½ä½å€
listeners=PLAINTEXT://:9092,CONTROLLER://:9093
advertised.listeners=PLAINTEXT://localhost:9092

# æ—¥èªŒç›®éŒ„
log.dirs=/var/kafka-logs

# Controller ç›£è½è¨­å®š
controller.listener.names=CONTROLLER

# é è¨­ Partition æ•¸é‡
num.partitions=3

# é è¨­å‰¯æœ¬å› å­
default.replication.factor=1

# æ—¥èªŒä¿ç•™æ™‚é–“ï¼ˆ168 å°æ™‚ = 7 å¤©ï¼‰
log.retention.hours=168
```

#### æ­¥é©Ÿ 4ï¼šæ ¼å¼åŒ–å„²å­˜ç›®éŒ„

```bash
# å»ºç«‹æ—¥èªŒç›®éŒ„
sudo mkdir -p /var/kafka-logs
sudo chown -R $USER:$USER /var/kafka-logs

# æ ¼å¼åŒ–å„²å­˜ç›®éŒ„
kafka-storage.sh format -t $KAFKA_CLUSTER_ID \
  -c /opt/kafka/config/kraft/server.properties
```

#### æ­¥é©Ÿ 5ï¼šå•Ÿå‹• Kafka

```bash
# å‰æ™¯åŸ·è¡Œï¼ˆé™¤éŒ¯ç”¨ï¼‰
kafka-server-start.sh /opt/kafka/config/kraft/server.properties

# èƒŒæ™¯åŸ·è¡Œ
kafka-server-start.sh -daemon /opt/kafka/config/kraft/server.properties

# æª¢æŸ¥æ˜¯å¦å•Ÿå‹•æˆåŠŸ
jps -l | grep kafka
# æ‡‰è©²çœ‹åˆ°ï¼škafka.Kafka
```

#### æ­¥é©Ÿ 6ï¼šé©—è­‰å®‰è£

```bash
# å»ºç«‹æ¸¬è©¦ Topic
kafka-topics.sh --create \
  --topic test-topic \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 1

# åˆ—å‡º Topic
kafka-topics.sh --list --bootstrap-server localhost:9092

# ç™¼é€æ¸¬è©¦è¨Šæ¯
echo "Hello Kafka" | kafka-console-producer.sh \
  --topic test-topic \
  --bootstrap-server localhost:9092

# æ¶ˆè²»æ¸¬è©¦è¨Šæ¯
kafka-console-consumer.sh \
  --topic test-topic \
  --from-beginning \
  --bootstrap-server localhost:9092
```

### 3.3 å¤šç¯€é»å¢é›†å®‰è£ï¼ˆæ­£å¼ç’°å¢ƒï¼‰

#### 3.3.1 å¢é›†è¦åŠƒ

å‡è¨­ 3 ç¯€é»å¢é›†ï¼š

| ç¯€é» | Hostname | IP | è§’è‰² |
|------|----------|-----|------|
| Node 1 | kafka-1 | 192.168.1.101 | broker, controller |
| Node 2 | kafka-2 | 192.168.1.102 | broker, controller |
| Node 3 | kafka-3 | 192.168.1.103 | broker, controller |

#### 3.3.2 å„ç¯€é»è¨­å®š

**Node 1 è¨­å®š** (`/opt/kafka/config/kraft/server.properties`)ï¼š

```properties
node.id=1
process.roles=broker,controller
controller.quorum.voters=1@192.168.1.101:9093,2@192.168.1.102:9093,3@192.168.1.103:9093

listeners=PLAINTEXT://:9092,CONTROLLER://:9093
advertised.listeners=PLAINTEXT://192.168.1.101:9092

controller.listener.names=CONTROLLER
inter.broker.listener.name=PLAINTEXT

log.dirs=/var/kafka-logs

num.partitions=6
default.replication.factor=3
min.insync.replicas=2

log.retention.hours=168
log.segment.bytes=1073741824
```

**Node 2 è¨­å®š**ï¼ˆä¿®æ”¹ä»¥ä¸‹é …ç›®ï¼‰ï¼š

```properties
node.id=2
advertised.listeners=PLAINTEXT://192.168.1.102:9092
```

**Node 3 è¨­å®š**ï¼ˆä¿®æ”¹ä»¥ä¸‹é …ç›®ï¼‰ï¼š

```properties
node.id=3
advertised.listeners=PLAINTEXT://192.168.1.103:9092
```

#### 3.3.3 å¢é›†å•Ÿå‹•æ­¥é©Ÿ

```bash
# 1. åœ¨ä»»ä¸€ç¯€é»ç”¢ç”Ÿ Cluster ID
KAFKA_CLUSTER_ID="$(kafka-storage.sh random-uuid)"
echo $KAFKA_CLUSTER_ID
# è¨˜ä¸‹æ­¤ IDï¼Œæ‰€æœ‰ç¯€é»ä½¿ç”¨ç›¸åŒçš„ ID

# 2. åœ¨æ¯å€‹ç¯€é»åŸ·è¡Œæ ¼å¼åŒ–ï¼ˆä½¿ç”¨ç›¸åŒçš„ CLUSTER_IDï¼‰
kafka-storage.sh format -t $KAFKA_CLUSTER_ID \
  -c /opt/kafka/config/kraft/server.properties

# 3. åœ¨æ¯å€‹ç¯€é»å•Ÿå‹• Kafka
kafka-server-start.sh -daemon /opt/kafka/config/kraft/server.properties

# 4. é©—è­‰å¢é›†ç‹€æ…‹
kafka-metadata.sh --snapshot /var/kafka-logs/__cluster_metadata-0/00000000000000000000.log \
  --command "describe"
```

### 3.4 ZooKeeper èˆ‡ KRaft æ¶æ§‹æ¯”è¼ƒ

```mermaid
flowchart TB
    subgraph ZooKeeper æ¨¡å¼
        ZK[ZooKeeper Ensemble]
        B1[Broker 1]
        B2[Broker 2]
        B3[Broker 3]
        ZK <--> B1
        ZK <--> B2
        ZK <--> B3
    end
    
    subgraph KRaft æ¨¡å¼
        C1[Controller 1]
        C2[Controller 2]
        C3[Controller 3]
        KB1[Broker 1]
        KB2[Broker 2]
        KB3[Broker 3]
        C1 <--> C2
        C2 <--> C3
        C1 <--> C3
        C1 --> KB1
        C2 --> KB2
        C3 --> KB3
    end
```

| ç‰¹æ€§ | ZooKeeper æ¨¡å¼ | KRaft æ¨¡å¼ |
|------|---------------|------------|
| **å…ƒä»¶æ•¸é‡** | éœ€è¦é¡å¤–çš„ ZK å¢é›† | å…§å»ºï¼Œç„¡éœ€é¡å¤–å…ƒä»¶ |
| **é‹ç¶­è¤‡é›œåº¦** | è¼ƒé«˜ï¼ˆéœ€ç¶­è­·å…©å¥—ç³»çµ±ï¼‰ | è¼ƒä½ |
| **æ“´å±•æ€§** | å— ZK é™åˆ¶ï¼ˆ~æ•¸è¬ Partitionï¼‰ | å¯æ”¯æ´æ›´å¤š Partition |
| **æ•…éšœæ¢å¾©** | è¼ƒæ…¢ | è¼ƒå¿« |
| **ç‰ˆæœ¬è¦æ±‚** | æ‰€æœ‰ç‰ˆæœ¬ | Kafka 3.4+ï¼ˆæ­£å¼æ”¯æ´ï¼‰ |

**å»ºè­°**ï¼šæ–°å°ˆæ¡ˆå»ºè­°ç›´æ¥ä½¿ç”¨ KRaft æ¨¡å¼

### 3.5 å¸¸è¦‹å®‰è£éŒ¯èª¤èˆ‡æ’é™¤æ–¹å¼

| éŒ¯èª¤è¨Šæ¯ | åŸå›  | è§£æ±ºæ–¹å¼ |
|----------|------|----------|
| `Error: JVM not found` | Java æœªå®‰è£æˆ–ç’°å¢ƒè®Šæ•¸æœªè¨­å®š | å®‰è£ JDK ä¸¦è¨­å®š `JAVA_HOME` |
| `Address already in use` | Port è¢«ä½”ç”¨ | æª¢æŸ¥ä¸¦é‡‹æ”¾ 9092/9093 Port |
| `No space left on device` | ç£ç¢Ÿç©ºé–“ä¸è¶³ | æ¸…ç†ç£ç¢Ÿæˆ–æ“´å……ç©ºé–“ |
| `Connection refused` | Kafka æœªå•Ÿå‹•æˆ–é˜²ç«ç‰†é˜»æ“‹ | æª¢æŸ¥æœå‹™ç‹€æ…‹èˆ‡é˜²ç«ç‰†è¦å‰‡ |
| `Cluster ID doesn't match` | Cluster ID ä¸ä¸€è‡´ | æ¸…ç©º log.dirs é‡æ–°æ ¼å¼åŒ– |
| `Not enough replicas` | å‰¯æœ¬æ•¸å¤§æ–¼ Broker æ•¸ | é™ä½ replication.factor |

### ğŸ’¡ å¯¦å‹™å»ºè­°

> **éƒ¨ç½²æª¢æŸ¥æ¸…å–®**ï¼š
> 1. ç¢ºèªæ‰€æœ‰ç¯€é»æ™‚é–“åŒæ­¥ï¼ˆä½¿ç”¨ NTPï¼‰
> 2. è¨­å®šé©ç•¶çš„ JVM è¨˜æ†¶é«”ï¼ˆå»ºè­° Heap 6-8 GBï¼‰
> 3. ä½¿ç”¨ç¨ç«‹çš„ç£ç¢Ÿå­˜æ”¾ Kafka æ—¥èªŒ
> 4. é–‹å•Ÿå¿…è¦çš„é˜²ç«ç‰† Port
> 5. è¨­å®š Systemd Service å¯¦ç¾é–‹æ©Ÿè‡ªå‹•å•Ÿå‹•

---

## 4. Kafka åŸºæœ¬è¨­å®šèªªæ˜

### 4.1 Broker é‡è¦è¨­å®šåƒæ•¸

#### 4.1.1 æ ¸å¿ƒè¨­å®š

```properties
############################# Server Basics #############################

# Broker å”¯ä¸€è­˜åˆ¥ç¢¼
broker.id=1

# ç¯€é»è§’è‰²ï¼ˆKRaft æ¨¡å¼ï¼‰
process.roles=broker,controller

# æ—¥èªŒå„²å­˜ç›®éŒ„ï¼ˆå¯è¨­å®šå¤šå€‹ï¼Œä»¥é€—è™Ÿåˆ†éš”ï¼‰
log.dirs=/var/kafka-logs,/var/kafka-logs2

# ç›£è½ä½å€
listeners=PLAINTEXT://0.0.0.0:9092
advertised.listeners=PLAINTEXT://kafka-broker-1:9092
```

#### 4.1.2 æ•ˆèƒ½ç›¸é—œè¨­å®š

```properties
############################# Performance #############################

# ç¶²è·¯è«‹æ±‚è™•ç†åŸ·è¡Œç·’æ•¸
num.network.threads=8

# I/O è™•ç†åŸ·è¡Œç·’æ•¸
num.io.threads=16

# Socket ç·©è¡å€å¤§å°
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400

# è«‹æ±‚æœ€å¤§å¤§å°
socket.request.max.bytes=104857600

# æ‰¹æ¬¡æ¥æ”¶çš„è¨Šæ¯æ•¸é‡
queued.max.requests=500
```

#### 4.1.3 æ—¥èªŒè¨­å®š

```properties
############################# Log Configuration #############################

# é è¨­ Partition æ•¸é‡
num.partitions=6

# é è¨­å‰¯æœ¬å› å­
default.replication.factor=3

# æœ€å°åŒæ­¥å‰¯æœ¬æ•¸
min.insync.replicas=2

# æ—¥èªŒå€æ®µå¤§å°ï¼ˆ1 GBï¼‰
log.segment.bytes=1073741824

# æ—¥èªŒä¿ç•™æ™‚é–“ï¼ˆ7 å¤©ï¼‰
log.retention.hours=168

# æ—¥èªŒä¿ç•™å¤§å°ï¼ˆ-1 è¡¨ç¤ºç„¡é™åˆ¶ï¼‰
log.retention.bytes=-1

# æ—¥èªŒæ¸…ç†æª¢æŸ¥é–“éš”ï¼ˆ5 åˆ†é˜ï¼‰
log.retention.check.interval.ms=300000

# æ—¥èªŒæ¸…ç†ç­–ç•¥ï¼šdelete æˆ– compact
log.cleanup.policy=delete
```

### 4.2 Topic è¨­è¨ˆåŸå‰‡

#### 4.2.1 Partition æ•¸é‡è¨­è¨ˆ

```mermaid
flowchart LR
    subgraph è€ƒé‡å› ç´ 
        A[é æœŸååé‡]
        B[Consumer æ•¸é‡]
        C[Broker æ•¸é‡]
        D[è¨Šæ¯é †åºéœ€æ±‚]
    end
    
    A --> E[Partition æ•¸é‡]
    B --> E
    C --> E
    D --> E
```

**è¨ˆç®—å…¬å¼**ï¼š
```
Partition æ•¸é‡ â‰¥ max(é æœŸååé‡ / å–®ä¸€ Partition ååé‡, Consumer æ•¸é‡)
```

**å»ºè­°åŸå‰‡**ï¼š

| å ´æ™¯ | å»ºè­° Partition æ•¸ | èªªæ˜ |
|------|-------------------|------|
| **ä½æµé‡ï¼ˆ< 1MB/sï¼‰** | 3-6 | ä¿æŒç°¡å–® |
| **ä¸­æµé‡ï¼ˆ1-10 MB/sï¼‰** | 6-12 | å¹³è¡¡æ•ˆèƒ½èˆ‡ç®¡ç† |
| **é«˜æµé‡ï¼ˆ> 10 MB/sï¼‰** | 12-50+ | è¦– Consumer æ•¸é‡èª¿æ•´ |
| **éœ€è¦é †åº** | 1 | å–®ä¸€ Partition ä¿è­‰é †åº |

âš ï¸ **æ³¨æ„**ï¼šPartition åªèƒ½å¢åŠ ä¸èƒ½æ¸›å°‘ï¼Œå»ºè­°ä¸€é–‹å§‹å°±é ç•™è¶³å¤ æ•¸é‡

#### 4.2.2 Replication Factor è¨­è¨ˆ

```properties
# å»ºè­°è¨­å®š
default.replication.factor=3
min.insync.replicas=2
```

| Replication Factor | Min ISR | å¯å®¹å¿æ•…éšœæ•¸ | èªªæ˜ |
|--------------------|---------|-------------|------|
| 1 | 1 | 0 | åƒ…é©ç”¨é–‹ç™¼ç’°å¢ƒ |
| 2 | 1 | 1 | æœ€ä½ç”Ÿç”¢è¦æ±‚ |
| 3 | 2 | 1 | **å»ºè­°çš„ç”Ÿç”¢é…ç½®** |
| 5 | 3 | 2 | é«˜å¯ç”¨è¦æ±‚ |

### 4.3 Producer é‡è¦è¨­å®š

```java
Properties props = new Properties();

// åŸºæœ¬é€£ç·šè¨­å®š
props.put("bootstrap.servers", "kafka-1:9092,kafka-2:9092,kafka-3:9092");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

// acks è¨­å®šï¼šè¨Šæ¯ç¢ºèªæ©Ÿåˆ¶
// 0: ä¸ç­‰å¾…ç¢ºèªï¼ˆæœ€å¿«ï¼Œå¯èƒ½éºå¤±ï¼‰
// 1: Leader ç¢ºèªï¼ˆå¹³è¡¡ï¼‰
// all/-1: æ‰€æœ‰ ISR ç¢ºèªï¼ˆæœ€å®‰å…¨ï¼‰
props.put("acks", "all");

// é‡è©¦è¨­å®š
props.put("retries", 3);
props.put("retry.backoff.ms", 1000);

// æ‰¹æ¬¡è¨­å®š
props.put("batch.size", 16384);           // æ‰¹æ¬¡å¤§å°ï¼ˆbytesï¼‰
props.put("linger.ms", 5);                // æ‰¹æ¬¡ç­‰å¾…æ™‚é–“ï¼ˆmsï¼‰
props.put("buffer.memory", 33554432);     // ç·©è¡å€å¤§å°ï¼ˆ32 MBï¼‰

// å†ªç­‰æ€§è¨­å®šï¼ˆé¿å…é‡è¤‡ï¼‰
props.put("enable.idempotence", true);

// å£“ç¸®è¨­å®š
props.put("compression.type", "snappy");  // none, gzip, snappy, lz4, zstd
```

**acks è¨­å®šæ¯”è¼ƒ**ï¼š

| acks | èªªæ˜ | å»¶é² | å¯é æ€§ | é©ç”¨å ´æ™¯ |
|------|------|------|--------|----------|
| 0 | ä¸ç­‰å¾… | æœ€ä½ | æœ€ä½ | æ—¥èªŒã€ç›£æ§ |
| 1 | Leader ç¢ºèª | ä¸­ | ä¸­ | ä¸€èˆ¬æ‡‰ç”¨ |
| all | æ‰€æœ‰ ISR ç¢ºèª | æœ€é«˜ | æœ€é«˜ | é‡‘èäº¤æ˜“ |

### 4.4 Consumer é‡è¦è¨­å®š

```java
Properties props = new Properties();

// åŸºæœ¬é€£ç·šè¨­å®š
props.put("bootstrap.servers", "kafka-1:9092,kafka-2:9092,kafka-3:9092");
props.put("group.id", "order-processing-group");
props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

// Offset é‡ç½®ç­–ç•¥
// earliest: å¾æœ€æ—©çš„è¨Šæ¯é–‹å§‹
// latest: å¾æœ€æ–°çš„è¨Šæ¯é–‹å§‹
// none: å¦‚æœæ²’æœ‰ offset å‰‡æ‹‹å‡ºä¾‹å¤–
props.put("auto.offset.reset", "earliest");

// è‡ªå‹•æäº¤è¨­å®š
props.put("enable.auto.commit", false);   // å»ºè­°æ‰‹å‹•æäº¤
props.put("auto.commit.interval.ms", 5000);

// æ‹‰å–è¨­å®š
props.put("max.poll.records", 500);       // å–®æ¬¡æ‹‰å–æœ€å¤§ç­†æ•¸
props.put("max.poll.interval.ms", 300000); // å…©æ¬¡ poll æœ€å¤§é–“éš”
props.put("fetch.min.bytes", 1);          // æœ€å°æ‹‰å–è³‡æ–™é‡
props.put("fetch.max.wait.ms", 500);      // æœ€å¤§ç­‰å¾…æ™‚é–“

// Session è¨­å®š
props.put("session.timeout.ms", 45000);   // Session è¶…æ™‚
props.put("heartbeat.interval.ms", 15000); // å¿ƒè·³é–“éš”

// éš”é›¢ç´šåˆ¥ï¼ˆé…åˆ Exactly Onceï¼‰
props.put("isolation.level", "read_committed");
```

### 4.5 è³‡æ–™ä¿ç•™ç­–ç•¥ï¼ˆRetention Policyï¼‰

```properties
############################# Retention Policy #############################

# ä¾æ™‚é–“ä¿ç•™ï¼ˆé è¨­ 7 å¤©ï¼‰
log.retention.hours=168
# æˆ–æ›´ç²¾ç¢ºçš„è¨­å®š
log.retention.ms=604800000

# ä¾å¤§å°ä¿ç•™ï¼ˆ-1 è¡¨ç¤ºç„¡é™åˆ¶ï¼‰
log.retention.bytes=-1

# æ—¥èªŒå€æ®µå¤§å°ï¼ˆè§¸ç™¼ Rollingï¼‰
log.segment.bytes=1073741824

# æ—¥èªŒå€æ®µæ™‚é–“ï¼ˆ7 å¤©ï¼‰
log.roll.hours=168

# æ¸…ç†ç­–ç•¥
# delete: åˆªé™¤éæœŸè³‡æ–™
# compact: å£“ç¸®ï¼ˆä¿ç•™æ¯å€‹ Key çš„æœ€æ–°å€¼ï¼‰
# compact,delete: å…ˆå£“ç¸®å¾Œåˆªé™¤
log.cleanup.policy=delete
```

**Log Compaction èªªæ˜**ï¼š

```mermaid
flowchart LR
    subgraph åŸå§‹ Log
        A1[Key:A, V:1]
        B1[Key:B, V:1]
        A2[Key:A, V:2]
        C1[Key:C, V:1]
        B2[Key:B, V:2]
        A3[Key:A, V:3]
    end
    
    subgraph Compacted Log
        AC[Key:A, V:3]
        BC[Key:B, V:2]
        CC[Key:C, V:1]
    end
    
    åŸå§‹ Log -->|Compaction| Compacted Log
```

### ğŸ’¡ å¯¦å‹™å»ºè­°

> **è¨­å®šæœ€ä½³å¯¦å‹™**ï¼š
> 1. ç”Ÿç”¢ç’°å¢ƒå‹™å¿…è¨­å®š `acks=all` èˆ‡ `min.insync.replicas=2`
> 2. æ ¹æ“šæ¥­å‹™éœ€æ±‚è¨­å®šåˆç†çš„ä¿ç•™æ™‚é–“ï¼Œé¿å…ç£ç¢Ÿç©ºé–“ä¸è¶³
> 3. Consumer å»ºè­°ä½¿ç”¨æ‰‹å‹•æäº¤ Offsetï¼Œé¿å…è³‡æ–™éºå¤±
> 4. å•Ÿç”¨ `enable.idempotence=true` ç¢ºä¿ Producer å†ªç­‰æ€§

---

## 5. Kafka ç³»çµ±ä½¿ç”¨æ•™å­¸

### 5.1 Topic ç®¡ç†

#### 5.1.1 å»ºç«‹ Topic

```bash
# åŸºæœ¬å»ºç«‹
kafka-topics.sh --create \
  --topic order-events \
  --bootstrap-server localhost:9092 \
  --partitions 6 \
  --replication-factor 3

# å»ºç«‹æ™‚æŒ‡å®šé¡å¤–è¨­å®š
kafka-topics.sh --create \
  --topic user-activities \
  --bootstrap-server localhost:9092 \
  --partitions 12 \
  --replication-factor 3 \
  --config retention.ms=259200000 \
  --config cleanup.policy=compact
```

#### 5.1.2 æŸ¥è©¢ Topic

```bash
# åˆ—å‡ºæ‰€æœ‰ Topic
kafka-topics.sh --list --bootstrap-server localhost:9092

# æŸ¥çœ‹ Topic è©³ç´°è³‡è¨Š
kafka-topics.sh --describe \
  --topic order-events \
  --bootstrap-server localhost:9092

# è¼¸å‡ºç¯„ä¾‹ï¼š
# Topic: order-events	TopicId: ABC123...
# PartitionCount: 6	ReplicationFactor: 3
# Configs: retention.ms=604800000
#   Partition: 0	Leader: 1	Replicas: 1,2,3	Isr: 1,2,3
#   Partition: 1	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1
#   ...
```

#### 5.1.3 ä¿®æ”¹ Topic

```bash
# å¢åŠ  Partitionï¼ˆåªèƒ½å¢åŠ ï¼Œä¸èƒ½æ¸›å°‘ï¼‰
kafka-topics.sh --alter \
  --topic order-events \
  --partitions 12 \
  --bootstrap-server localhost:9092

# ä¿®æ”¹ Topic è¨­å®š
kafka-configs.sh --alter \
  --entity-type topics \
  --entity-name order-events \
  --add-config retention.ms=172800000 \
  --bootstrap-server localhost:9092

# æŸ¥çœ‹ Topic è¨­å®š
kafka-configs.sh --describe \
  --entity-type topics \
  --entity-name order-events \
  --bootstrap-server localhost:9092
```

#### 5.1.4 åˆªé™¤ Topic

```bash
# åˆªé™¤ Topic
kafka-topics.sh --delete \
  --topic test-topic \
  --bootstrap-server localhost:9092

# æ³¨æ„ï¼šç¢ºä¿ delete.topic.enable=trueï¼ˆé è¨­ç‚º trueï¼‰
```

### 5.2 Producer ç™¼é€è¨Šæ¯

#### 5.2.1 ä½¿ç”¨ Console Producer

```bash
# åŸºæœ¬ç™¼é€
kafka-console-producer.sh \
  --topic order-events \
  --bootstrap-server localhost:9092

# å¸¶ Key ç™¼é€
kafka-console-producer.sh \
  --topic order-events \
  --bootstrap-server localhost:9092 \
  --property "parse.key=true" \
  --property "key.separator=:"

# è¼¸å…¥æ ¼å¼ï¼škey:value
# order-001:{"orderId":"001","amount":100}
```

#### 5.2.2 Java Producer ç¯„ä¾‹

```java
import org.apache.kafka.clients.producer.*;
import java.util.Properties;

public class OrderProducer {
    
    public static void main(String[] args) {
        // è¨­å®š Producer åƒæ•¸
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, 
                  "kafka-1:9092,kafka-2:9092,kafka-3:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, 
                  "org.apache.kafka.common.serialization.StringSerializer");
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, 
                  "org.apache.kafka.common.serialization.StringSerializer");
        props.put(ProducerConfig.ACKS_CONFIG, "all");
        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
        props.put(ProducerConfig.RETRIES_CONFIG, 3);
        
        // å»ºç«‹ Producer
        try (KafkaProducer<String, String> producer = new KafkaProducer<>(props)) {
            
            // å»ºç«‹è¨Šæ¯
            String topic = "order-events";
            String key = "order-001";
            String value = "{\"orderId\":\"001\",\"amount\":100}";
            
            ProducerRecord<String, String> record = 
                new ProducerRecord<>(topic, key, value);
            
            // åŒæ­¥ç™¼é€
            RecordMetadata metadata = producer.send(record).get();
            System.out.printf("Sent to partition %d, offset %d%n", 
                              metadata.partition(), metadata.offset());
            
            // éåŒæ­¥ç™¼é€ï¼ˆå»ºè­°ï¼‰
            producer.send(record, (meta, exception) -> {
                if (exception != null) {
                    exception.printStackTrace();
                } else {
                    System.out.printf("Sent to partition %d, offset %d%n", 
                                      meta.partition(), meta.offset());
                }
            });
            
            // ç¢ºä¿æ‰€æœ‰è¨Šæ¯ç™¼é€å®Œæˆ
            producer.flush();
        }
    }
}
```

### 5.3 Consumer æ¶ˆè²»è¨Šæ¯

#### 5.3.1 ä½¿ç”¨ Console Consumer

```bash
# å¾æœ€æ–°è¨Šæ¯é–‹å§‹æ¶ˆè²»
kafka-console-consumer.sh \
  --topic order-events \
  --bootstrap-server localhost:9092

# å¾æœ€æ—©è¨Šæ¯é–‹å§‹æ¶ˆè²»
kafka-console-consumer.sh \
  --topic order-events \
  --from-beginning \
  --bootstrap-server localhost:9092

# æŒ‡å®š Consumer Group
kafka-console-consumer.sh \
  --topic order-events \
  --group order-processor \
  --bootstrap-server localhost:9092

# é¡¯ç¤º Key å’Œ Timestamp
kafka-console-consumer.sh \
  --topic order-events \
  --bootstrap-server localhost:9092 \
  --property print.key=true \
  --property print.timestamp=true
```

#### 5.3.2 Java Consumer ç¯„ä¾‹

```java
import org.apache.kafka.clients.consumer.*;
import java.time.Duration;
import java.util.*;

public class OrderConsumer {
    
    public static void main(String[] args) {
        // è¨­å®š Consumer åƒæ•¸
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, 
                  "kafka-1:9092,kafka-2:9092,kafka-3:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "order-processing-group");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, 
                  "org.apache.kafka.common.serialization.StringDeserializer");
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, 
                  "org.apache.kafka.common.serialization.StringDeserializer");
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);
        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 100);
        
        // å»ºç«‹ Consumer
        try (KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props)) {
            
            // è¨‚é–± Topic
            consumer.subscribe(Collections.singletonList("order-events"));
            
            // æŒçºŒæ¶ˆè²»
            while (true) {
                ConsumerRecords<String, String> records = 
                    consumer.poll(Duration.ofMillis(1000));
                
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("Partition: %d, Offset: %d, Key: %s, Value: %s%n",
                                      record.partition(), 
                                      record.offset(), 
                                      record.key(), 
                                      record.value());
                    
                    // è™•ç†æ¥­å‹™é‚è¼¯
                    processOrder(record.value());
                }
                
                // æ‰‹å‹•æäº¤ Offset
                consumer.commitSync();
            }
        }
    }
    
    private static void processOrder(String orderJson) {
        // æ¥­å‹™è™•ç†é‚è¼¯
        System.out.println("Processing: " + orderJson);
    }
}
```

### 5.4 Offset ç®¡ç†

#### 5.4.1 æŸ¥çœ‹ Consumer Group Offset

```bash
# åˆ—å‡ºæ‰€æœ‰ Consumer Group
kafka-consumer-groups.sh --list --bootstrap-server localhost:9092

# æŸ¥çœ‹ç‰¹å®š Group çš„ Offset ç‹€æ…‹
kafka-consumer-groups.sh --describe \
  --group order-processing-group \
  --bootstrap-server localhost:9092

# è¼¸å‡ºç¯„ä¾‹ï¼š
# GROUP                  TOPIC          PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG
# order-processing-group order-events   0          1000            1050            50
# order-processing-group order-events   1          980             980             0
# order-processing-group order-events   2          1100            1200            100
```

#### 5.4.2 é‡ç½® Offset

```bash
# é‡ç½®åˆ°æœ€æ—©ï¼ˆéœ€å…ˆåœæ­¢ Consumerï¼‰
kafka-consumer-groups.sh --reset-offsets \
  --group order-processing-group \
  --topic order-events \
  --to-earliest \
  --execute \
  --bootstrap-server localhost:9092

# é‡ç½®åˆ°æœ€æ–°
kafka-consumer-groups.sh --reset-offsets \
  --group order-processing-group \
  --topic order-events \
  --to-latest \
  --execute \
  --bootstrap-server localhost:9092

# é‡ç½®åˆ°ç‰¹å®š Offset
kafka-consumer-groups.sh --reset-offsets \
  --group order-processing-group \
  --topic order-events:0 \
  --to-offset 500 \
  --execute \
  --bootstrap-server localhost:9092

# é‡ç½®åˆ°ç‰¹å®šæ™‚é–“
kafka-consumer-groups.sh --reset-offsets \
  --group order-processing-group \
  --topic order-events \
  --to-datetime "2024-01-01T00:00:00.000" \
  --execute \
  --bootstrap-server localhost:9092
```

### 5.5 è¨Šæ¯é †åºæ€§èˆ‡é‡è¤‡æ¶ˆè²»

#### 5.5.1 è¨Šæ¯é †åºä¿è­‰

```mermaid
flowchart TB
    subgraph é †åºä¿è­‰ç¯„åœ
        P[Producer] -->|Key: order-001| Part0[Partition 0]
        P -->|Key: order-002| Part1[Partition 1]
        P -->|Key: order-001| Part0
        
        Part0 -->|é †åºæ¶ˆè²»| C1[Consumer 1]
        Part1 -->|é †åºæ¶ˆè²»| C2[Consumer 2]
    end
```

**é †åºä¿è­‰åŸå‰‡**ï¼š
1. **åŒä¸€ Partition å…§**ï¼šè¨Šæ¯é †åºä¿è­‰
2. **è·¨ Partition**ï¼šç„¡é †åºä¿è­‰
3. **ä½¿ç”¨ç›¸åŒ Key**ï¼šæœƒè¢«åˆ†é…åˆ°åŒä¸€ Partition

```java
// ç¢ºä¿ç›¸åŒè¨‚å–®çš„è¨Šæ¯é †åº
String orderId = "order-001";
ProducerRecord<String, String> record = new ProducerRecord<>(
    "order-events",
    orderId,  // ä½¿ç”¨ orderId ä½œç‚º Key
    "{\"status\":\"created\"}"
);
```

#### 5.5.2 é¿å…é‡è¤‡æ¶ˆè²»

**é‡è¤‡æ¶ˆè²»çš„åŸå› **ï¼š
1. Consumer è™•ç†å®Œæˆå¾Œã€æäº¤ Offset å‰ç•¶æ©Ÿ
2. Rebalance å°è‡´é‡æ–°æ¶ˆè²»
3. æ‰‹å‹•é‡ç½® Offset

**è§£æ±ºæ–¹æ¡ˆ**ï¼š

```java
// æ–¹æ¡ˆ 1ï¼šå†ªç­‰è™•ç†ï¼ˆæ¨è–¦ï¼‰
public void processOrder(ConsumerRecord<String, String> record) {
    String messageId = record.topic() + "-" + record.partition() + "-" + record.offset();
    
    // æª¢æŸ¥æ˜¯å¦å·²è™•ç†é
    if (processedMessages.contains(messageId)) {
        return; // è·³éå·²è™•ç†çš„è¨Šæ¯
    }
    
    // è™•ç†æ¥­å‹™é‚è¼¯
    doBusinessLogic(record.value());
    
    // è¨˜éŒ„å·²è™•ç†
    processedMessages.add(messageId);
}

// æ–¹æ¡ˆ 2ï¼šä½¿ç”¨æ¥­å‹™ Key å»é‡
public void processOrderWithDedup(String orderId, String orderData) {
    // ä½¿ç”¨ Redis æˆ–è³‡æ–™åº«æª¢æŸ¥
    if (redis.setnx("processed:" + orderId, "1") == 0) {
        return; // å·²è™•ç†é
    }
    redis.expire("processed:" + orderId, 86400); // è¨­å®šéæœŸæ™‚é–“
    
    doBusinessLogic(orderData);
}
```

### ğŸ’¡ å¯¦å‹™å»ºè­°

> **æ¶ˆè²»ç«¯æœ€ä½³å¯¦å‹™**ï¼š
> 1. ä½¿ç”¨æ‰‹å‹•æäº¤ Offsetï¼Œåœ¨æ¥­å‹™è™•ç†æˆåŠŸå¾Œå†æäº¤
> 2. å¯¦ä½œå†ªç­‰è™•ç†é‚è¼¯ï¼Œå®¹è¨±é‡è¤‡æ¶ˆè²»
> 3. ç›£æ§ Consumer Lagï¼Œé¿å…æ¶ˆè²»éæ…¢
> 4. åˆç†è¨­å®š `max.poll.records`ï¼Œé¿å…å–®æ¬¡è™•ç†éå¤š

---

## 6. Kafka èˆ‡æ‡‰ç”¨ç³»çµ±ä¸²æ¥æ–¹å¼

### 6.1 èˆ‡ Spring Boot æ•´åˆ

#### 6.1.1 Maven ä¾è³´

```xml
<dependencies>
    <!-- Spring Kafka -->
    <dependency>
        <groupId>org.springframework.kafka</groupId>
        <artifactId>spring-kafka</artifactId>
        <version>3.1.0</version>
    </dependency>
    
    <!-- JSON åºåˆ—åŒ– -->
    <dependency>
        <groupId>com.fasterxml.jackson.core</groupId>
        <artifactId>jackson-databind</artifactId>
    </dependency>
</dependencies>
```

#### 6.1.2 è¨­å®šæª”ï¼ˆapplication.ymlï¼‰

```yaml
spring:
  kafka:
    bootstrap-servers: kafka-1:9092,kafka-2:9092,kafka-3:9092
    
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
      acks: all
      retries: 3
      properties:
        enable.idempotence: true
        max.in.flight.requests.per.connection: 5
    
    consumer:
      group-id: order-service
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      auto-offset-reset: earliest
      enable-auto-commit: false
      properties:
        spring.json.trusted.packages: "com.example.model"
    
    listener:
      ack-mode: manual
      concurrency: 3
```

#### 6.1.3 Producer å¯¦ä½œ

```java
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.support.SendResult;
import org.springframework.stereotype.Service;

@Service
public class OrderEventProducer {
    
    private final KafkaTemplate<String, OrderEvent> kafkaTemplate;
    
    public OrderEventProducer(KafkaTemplate<String, OrderEvent> kafkaTemplate) {
        this.kafkaTemplate = kafkaTemplate;
    }
    
    // åŒæ­¥ç™¼é€
    public void sendSync(String topic, String key, OrderEvent event) {
        try {
            SendResult<String, OrderEvent> result = 
                kafkaTemplate.send(topic, key, event).get();
            
            log.info("Sent message to partition {} with offset {}",
                     result.getRecordMetadata().partition(),
                     result.getRecordMetadata().offset());
        } catch (Exception e) {
            log.error("Failed to send message", e);
            throw new RuntimeException(e);
        }
    }
    
    // éåŒæ­¥ç™¼é€ï¼ˆæ¨è–¦ï¼‰
    public void sendAsync(String topic, String key, OrderEvent event) {
        kafkaTemplate.send(topic, key, event)
            .whenComplete((result, ex) -> {
                if (ex != null) {
                    log.error("Failed to send message", ex);
                } else {
                    log.info("Sent message to partition {} with offset {}",
                             result.getRecordMetadata().partition(),
                             result.getRecordMetadata().offset());
                }
            });
    }
}

// è¨Šæ¯æ¨¡å‹
@Data
public class OrderEvent {
    private String orderId;
    private String eventType;
    private BigDecimal amount;
    private LocalDateTime timestamp;
}
```

#### 6.1.4 Consumer å¯¦ä½œ

```java
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.support.Acknowledgment;
import org.springframework.stereotype.Service;

@Service
public class OrderEventConsumer {
    
    @KafkaListener(
        topics = "order-events",
        groupId = "order-service",
        containerFactory = "kafkaListenerContainerFactory"
    )
    public void handleOrderEvent(
            @Payload OrderEvent event,
            @Header(KafkaHeaders.RECEIVED_PARTITION) int partition,
            @Header(KafkaHeaders.OFFSET) long offset,
            Acknowledgment ack) {
        
        log.info("Received event: {} from partition {} offset {}",
                 event, partition, offset);
        
        try {
            // è™•ç†æ¥­å‹™é‚è¼¯
            processOrderEvent(event);
            
            // æ‰‹å‹•ç¢ºèª
            ack.acknowledge();
        } catch (Exception e) {
            log.error("Failed to process event", e);
            // ä¸ç¢ºèªï¼Œè¨Šæ¯å°‡è¢«é‡æ–°æ¶ˆè²»
            throw e;
        }
    }
    
    // æ‰¹æ¬¡æ¶ˆè²»
    @KafkaListener(
        topics = "order-events",
        groupId = "order-batch-service",
        batch = "true"
    )
    public void handleBatchEvents(
            List<OrderEvent> events,
            Acknowledgment ack) {
        
        log.info("Received {} events", events.size());
        
        for (OrderEvent event : events) {
            processOrderEvent(event);
        }
        
        ack.acknowledge();
    }
    
    private void processOrderEvent(OrderEvent event) {
        // æ¥­å‹™è™•ç†
    }
}
```

#### 6.1.5 Kafka è¨­å®šé¡åˆ¥

```java
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.core.*;
import org.springframework.kafka.listener.ContainerProperties.AckMode;

@Configuration
public class KafkaConfig {
    
    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, OrderEvent> 
            kafkaListenerContainerFactory(
                ConsumerFactory<String, OrderEvent> consumerFactory) {
        
        ConcurrentKafkaListenerContainerFactory<String, OrderEvent> factory =
            new ConcurrentKafkaListenerContainerFactory<>();
        
        factory.setConsumerFactory(consumerFactory);
        factory.setConcurrency(3); // ä¸¦è¡Œæ¶ˆè²»æ•¸
        factory.getContainerProperties().setAckMode(AckMode.MANUAL);
        
        // éŒ¯èª¤è™•ç†
        factory.setCommonErrorHandler(new DefaultErrorHandler(
            new FixedBackOff(1000L, 3) // é‡è©¦ 3 æ¬¡ï¼Œé–“éš” 1 ç§’
        ));
        
        return factory;
    }
}
```

### 6.2 ç³»çµ±è§£è€¦æ¶æ§‹è¨­è¨ˆ

```mermaid
flowchart TB
    subgraph å‰ç«¯ç³»çµ±
        WEB[Web Application]
        APP[Mobile App]
    end
    
    subgraph API Gateway
        GW[Gateway]
    end
    
    subgraph æ ¸å¿ƒæœå‹™
        ORDER[Order Service]
        USER[User Service]
        INVENTORY[Inventory Service]
        PAYMENT[Payment Service]
    end
    
    subgraph Kafka Topics
        T1[order-events]
        T2[payment-events]
        T3[inventory-events]
    end
    
    subgraph ä¸‹æ¸¸æœå‹™
        NOTIFY[Notification Service]
        ANALYTICS[Analytics Service]
        AUDIT[Audit Service]
    end
    
    WEB --> GW
    APP --> GW
    GW --> ORDER
    GW --> USER
    
    ORDER -->|ç™¼å¸ƒ| T1
    PAYMENT -->|ç™¼å¸ƒ| T2
    INVENTORY -->|ç™¼å¸ƒ| T3
    
    T1 --> NOTIFY
    T1 --> ANALYTICS
    T1 --> AUDIT
    T2 --> NOTIFY
    T2 --> ANALYTICS
```

### 6.3 åŒæ­¥ç³»çµ± vs äº‹ä»¶é©…å‹•æ¶æ§‹

| ç‰¹æ€§ | åŒæ­¥ç³»çµ± | äº‹ä»¶é©…å‹•æ¶æ§‹ |
|------|----------|--------------|
| **è€¦åˆåº¦** | é«˜ï¼ˆç›´æ¥å‘¼å«ï¼‰ | ä½ï¼ˆé€éäº‹ä»¶ï¼‰ |
| **å›æ‡‰æ™‚é–“** | å³æ™‚ | éå³æ™‚ |
| **å¯é æ€§** | ç›¸ä¾æœå‹™æ•…éšœå½±éŸ¿å¤§ | éš”é›¢æ€§å¥½ |
| **æ“´å±•æ€§** | å—é™ | æ˜“æ–¼æ“´å±• |
| **è¿½è¹¤** | è¼ƒå®¹æ˜“ | éœ€é¡å¤–æ©Ÿåˆ¶ |
| **ä¸€è‡´æ€§** | å¼·ä¸€è‡´æ€§ | æœ€çµ‚ä¸€è‡´æ€§ |

### 6.4 å¸¸è¦‹æ•´åˆæ¶æ§‹æ¨¡å¼

#### 6.4.1 Event Sourcing

```mermaid
flowchart LR
    CMD[Command] --> AGG[Aggregate]
    AGG -->|ç”¢ç”Ÿ| EVT[Event]
    EVT --> ES[Event Store<br/>Kafka]
    ES --> PROJ[Projection]
    PROJ --> READ[Read Model]
```

#### 6.4.2 CDCï¼ˆChange Data Captureï¼‰

```mermaid
flowchart LR
    DB[(Database)] -->|Debezium| KAFKA[Kafka]
    KAFKA --> ES[Elasticsearch]
    KAFKA --> DW[Data Warehouse]
    KAFKA --> CACHE[Redis Cache]
```

#### 6.4.3 Saga æ¨¡å¼

```mermaid
sequenceDiagram
    participant OrderService
    participant Kafka
    participant InventoryService
    participant PaymentService
    
    OrderService->>Kafka: OrderCreated
    Kafka->>InventoryService: Reserve Inventory
    InventoryService->>Kafka: InventoryReserved
    Kafka->>PaymentService: Process Payment
    PaymentService->>Kafka: PaymentCompleted
    Kafka->>OrderService: Complete Order
```

### ğŸ’¡ å¯¦å‹™å»ºè­°

> **æ•´åˆæœ€ä½³å¯¦å‹™**ï¼š
> 1. å®šç¾©æ¸…æ¥šçš„ Event Schemaï¼Œä½¿ç”¨ Avro æˆ– Protobuf ç®¡ç†
> 2. å¯¦ä½œ Dead Letter Queueï¼ˆDLQï¼‰è™•ç†ç„¡æ³•æ¶ˆè²»çš„è¨Šæ¯
> 3. è€ƒæ…®ä½¿ç”¨ Kafka Streams æˆ– ksqlDB é€²è¡Œä¸²æµè™•ç†
> 4. ç›£æ§ Consumer Lag ä¸¦è¨­å®šå‘Šè­¦

---

## 7. Kafka ç³»çµ±ç¶­é‹èˆ‡ç›£æ§

### 7.1 å¸¸è¦‹ç›£æ§æŒ‡æ¨™

#### 7.1.1 Broker å±¤ç´šæŒ‡æ¨™

| æŒ‡æ¨™ | èªªæ˜ | å‘Šè­¦é–¾å€¼å»ºè­° |
|------|------|-------------|
| **UnderReplicatedPartitions** | å‰¯æœ¬æœªåŒæ­¥çš„ Partition æ•¸ | > 0 è­¦å‘Š |
| **OfflinePartitionsCount** | é›¢ç·š Partition æ•¸ | > 0 ç·Šæ€¥ |
| **ActiveControllerCount** | æ´»å‹• Controller æ•¸ | â‰  1 ç·Šæ€¥ |
| **RequestsPerSec** | æ¯ç§’è«‹æ±‚æ•¸ | è¦–å®¹é‡è€Œå®š |
| **BytesInPerSec** | æ¯ç§’å¯«å…¥ä½å…ƒçµ„æ•¸ | è¦–å®¹é‡è€Œå®š |
| **BytesOutPerSec** | æ¯ç§’è®€å–ä½å…ƒçµ„æ•¸ | è¦–å®¹é‡è€Œå®š |

#### 7.1.2 Topic/Partition å±¤ç´šæŒ‡æ¨™

| æŒ‡æ¨™ | èªªæ˜ | å‘Šè­¦é–¾å€¼å»ºè­° |
|------|------|-------------|
| **MessagesInPerSec** | æ¯ç§’è¨Šæ¯æ•¸ | è¦–æ¥­å‹™é‡è€Œå®š |
| **LogEndOffset** | Log æœ€æ–° Offset | ç›£æ§æˆé•·è¶¨å‹¢ |
| **LogSize** | åˆ†å€å¤§å° | è¦–ç£ç¢Ÿç©ºé–“è€Œå®š |

#### 7.1.3 Consumer å±¤ç´šæŒ‡æ¨™

| æŒ‡æ¨™ | èªªæ˜ | å‘Šè­¦é–¾å€¼å»ºè­° |
|------|------|-------------|
| **Consumer Lag** | æ¶ˆè²»å»¶é²ï¼ˆæœªæ¶ˆè²»è¨Šæ¯æ•¸ï¼‰ | > 10000 è­¦å‘Š |
| **Records Consumed Rate** | æ¶ˆè²»é€Ÿç‡ | ç›£æ§ä¸‹é™è¶¨å‹¢ |
| **Commit Latency** | Offset æäº¤å»¶é² | > 100ms è­¦å‘Š |

### 7.2 Consumer Lag ç›£æ§èˆ‡è™•ç†

#### 7.2.1 æŸ¥çœ‹ Consumer Lag

```bash
# ä½¿ç”¨ kafka-consumer-groups.sh
kafka-consumer-groups.sh --describe \
  --group order-processing-group \
  --bootstrap-server localhost:9092

# è¼¸å‡ºç¯„ä¾‹ï¼š
# GROUP                  TOPIC          PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG
# order-processing-group order-events   0          1000            1500            500
# order-processing-group order-events   1          800             800             0
# order-processing-group order-events   2          1200            2000            800
```

#### 7.2.2 Lag éé«˜çš„åŸå› èˆ‡è§£æ±ºæ–¹æ¡ˆ

```mermaid
flowchart TB
    LAG[Consumer Lag éé«˜]
    
    LAG --> C1[Consumer è™•ç†å¤ªæ…¢]
    LAG --> C2[Consumer æ•¸é‡ä¸è¶³]
    LAG --> C3[Partition ä¸å¹³è¡¡]
    LAG --> C4[ç¶²è·¯å•é¡Œ]
    
    C1 --> S1[å„ªåŒ–è™•ç†é‚è¼¯<br/>ä½¿ç”¨æ‰¹æ¬¡è™•ç†]
    C2 --> S2[å¢åŠ  Consumer<br/>æœ€å¤š = Partition æ•¸]
    C3 --> S3[é‡æ–°åˆ†é… Partition]
    C4 --> S4[æª¢æŸ¥ç¶²è·¯é€£ç·š<br/>å¢åŠ  fetch è¨­å®š]
```

### 7.3 ç³»çµ±ç›£æ§è¨­å®š

#### 7.3.1 JMX ç›£æ§è¨­å®š

```bash
# å•Ÿå‹• Kafka æ™‚å•Ÿç”¨ JMX
export KAFKA_JMX_OPTS="-Dcom.sun.management.jmxremote \
  -Dcom.sun.management.jmxremote.port=9999 \
  -Dcom.sun.management.jmxremote.authenticate=false \
  -Dcom.sun.management.jmxremote.ssl=false"

kafka-server-start.sh config/server.properties
```

#### 7.3.2 Prometheus æ•´åˆ

ä½¿ç”¨ JMX Exporter å°‡ Kafka æŒ‡æ¨™åŒ¯å‡ºè‡³ Prometheusï¼š

```yaml
# jmx_exporter_config.yml
lowercaseOutputName: true
lowercaseOutputLabelNames: true

rules:
  - pattern: kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value
    name: kafka_server_$1_$2
    type: GAUGE
    labels:
      clientId: "$3"
      topic: "$4"
      partition: "$5"
  
  - pattern: kafka.server<type=(.+), name=(.+)><>Value
    name: kafka_server_$1_$2
    type: GAUGE
    
  - pattern: kafka.controller<type=(.+), name=(.+)><>Value
    name: kafka_controller_$1_$2
    type: GAUGE
```

#### 7.3.3 Grafana Dashboard

å»ºè­°ç›£æ§é¢æ¿åŒ…å«ï¼š
1. Broker ç¸½è¦½ï¼ˆCPUã€Memoryã€Diskã€Networkï¼‰
2. Topic æŒ‡æ¨™ï¼ˆè¨Šæ¯é€Ÿç‡ã€å¤§å°ï¼‰
3. Consumer Lag ç›£æ§
4. Producer æŒ‡æ¨™ï¼ˆæˆåŠŸç‡ã€å»¶é²ï¼‰

### 7.4 å¸¸è¦‹ç‡Ÿé‹å•é¡Œèˆ‡æ’æŸ¥

#### 7.4.1 å•é¡Œæ’æŸ¥æµç¨‹

```mermaid
flowchart TB
    START[ç™¼ç¾å•é¡Œ] --> CHECK1{Broker ç‹€æ…‹}
    CHECK1 -->|ç•°å¸¸| FIX1[æª¢æŸ¥æ—¥èªŒ<br/>é‡å•Ÿ Broker]
    CHECK1 -->|æ­£å¸¸| CHECK2{Controller ç‹€æ…‹}
    
    CHECK2 -->|ç•°å¸¸| FIX2[æª¢æŸ¥ ZK/KRaft<br/>Controller é¸èˆ‰]
    CHECK2 -->|æ­£å¸¸| CHECK3{Partition ç‹€æ…‹}
    
    CHECK3 -->|Under Replicated| FIX3[æª¢æŸ¥å‰¯æœ¬åŒæ­¥<br/>ç¶²è·¯å•é¡Œ]
    CHECK3 -->|æ­£å¸¸| CHECK4{Consumer ç‹€æ…‹}
    
    CHECK4 -->|Lag é«˜| FIX4[æ“´å±• Consumer<br/>å„ªåŒ–è™•ç†é‚è¼¯]
    CHECK4 -->|æ­£å¸¸| CHECK5[æª¢æŸ¥ Client ç«¯]
```

#### 7.4.2 å¸¸è¦‹å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ

| å•é¡Œ | å¯èƒ½åŸå›  | è§£æ±ºæ–¹æ¡ˆ |
|------|----------|----------|
| **Broker ç„¡æ³•å•Ÿå‹•** | é…ç½®éŒ¯èª¤ã€Port è¢«ä½”ç”¨ | æª¢æŸ¥æ—¥èªŒã€ç¢ºèªé…ç½® |
| **UnderReplicatedPartitions** | Follower åŒæ­¥æ…¢ã€ç¶²è·¯å•é¡Œ | æª¢æŸ¥ç¶²è·¯ã€èª¿æ•´è¤‡å¯«è¨­å®š |
| **Producer è¶…æ™‚** | Broker è² è¼‰é«˜ã€acks è¨­å®š | æ“´å®¹ã€èª¿æ•´è¨­å®š |
| **Consumer Lag æŒçºŒå¢åŠ ** | è™•ç†æ…¢ã€Consumer æ•¸ä¸è¶³ | å„ªåŒ–è™•ç†ã€å¢åŠ  Consumer |
| **ç£ç¢Ÿç©ºé–“ä¸è¶³** | ä¿ç•™ç­–ç•¥ä¸ç•¶ã€è³‡æ–™é‡å¤§ | èª¿æ•´ retentionã€æ“´å……ç£ç¢Ÿ |
| **è¨˜æ†¶é«”ä¸è¶³ï¼ˆOOMï¼‰** | JVM è¨­å®šä¸ç•¶ | èª¿æ•´ Heap å¤§å° |

#### 7.4.3 æ—¥èªŒæª¢æŸ¥

```bash
# Broker æ—¥èªŒä½ç½®
/opt/kafka/logs/server.log
/opt/kafka/logs/controller.log

# å¸¸è¦‹éŒ¯èª¤é—œéµå­—
grep -E "ERROR|WARN|Exception" /opt/kafka/logs/server.log

# æª¢æŸ¥ Controller æ—¥èªŒ
grep "controller" /opt/kafka/logs/controller.log
```

### ğŸ’¡ å¯¦å‹™å»ºè­°

> **ç¶­é‹æœ€ä½³å¯¦å‹™**ï¼š
> 1. è¨­å®šå®Œæ•´çš„ç›£æ§å‘Šè­¦ï¼Œç‰¹åˆ¥æ˜¯ Under Replicated Partitions
> 2. å®šæœŸæª¢æŸ¥ç£ç¢Ÿä½¿ç”¨ç‡ï¼Œè¨­å®šè‡ªå‹•æ¸…ç†
> 3. å»ºç«‹æ¨™æº–çš„æ•…éšœæ’é™¤æµç¨‹æ–‡ä»¶
> 4. å®šæœŸé€²è¡Œæ•…éšœæ¼”ç·´ï¼ˆChaos Engineeringï¼‰

---

## 8. Kafka ç³»çµ±å‡ç´šèˆ‡ç‰ˆæœ¬æ§ç®¡

### 8.1 å‡ç´šç­–ç•¥ï¼ˆRolling Upgradeï¼‰

```mermaid
flowchart LR
    subgraph å‡ç´šæµç¨‹
        B1[Broker 1<br/>èˆŠç‰ˆæœ¬] -->|1. åœæ©Ÿ| B1U[Broker 1<br/>å‡ç´šä¸­]
        B1U -->|2. å•Ÿå‹•| B1N[Broker 1<br/>æ–°ç‰ˆæœ¬]
        
        B2[Broker 2<br/>èˆŠç‰ˆæœ¬] -->|3. åœæ©Ÿ| B2U[Broker 2<br/>å‡ç´šä¸­]
        B2U -->|4. å•Ÿå‹•| B2N[Broker 2<br/>æ–°ç‰ˆæœ¬]
        
        B3[Broker 3<br/>èˆŠç‰ˆæœ¬] -->|5. åœæ©Ÿ| B3U[Broker 3<br/>å‡ç´šä¸­]
        B3U -->|6. å•Ÿå‹•| B3N[Broker 3<br/>æ–°ç‰ˆæœ¬]
    end
```

#### å‡ç´šæ­¥é©Ÿ

```bash
# 1. æª¢æŸ¥å¢é›†ç‹€æ…‹
kafka-metadata.sh --snapshot /var/kafka-logs/__cluster_metadata-0/*.log \
  --command "describe"

# 2. è¨­å®š inter.broker.protocol.versionï¼ˆå‡ç´šå‰ï¼‰
# ä¿æŒèˆ‡èˆŠç‰ˆæœ¬ç›¸å®¹
inter.broker.protocol.version=3.5

# 3. é€ä¸€åœæ­¢ Broker
kafka-server-stop.sh

# 4. å‚™ä»½é…ç½®æª”
cp /opt/kafka/config/kraft/server.properties \
   /opt/kafka/config/kraft/server.properties.bak

# 5. æ›´æ–° Kafka
cd /opt
wget https://downloads.apache.org/kafka/3.7.0/kafka_2.13-3.7.0.tgz
tar -xzf kafka_2.13-3.7.0.tgz
rm kafka
ln -s kafka_2.13-3.7.0 kafka

# 6. æ¢å¾©é…ç½®æª”
cp /opt/kafka/config/kraft/server.properties.bak \
   /opt/kafka/config/kraft/server.properties

# 7. å•Ÿå‹• Broker
kafka-server-start.sh -daemon /opt/kafka/config/kraft/server.properties

# 8. ç¢ºèª Broker åŠ å…¥å¢é›†
kafka-metadata.sh --snapshot /var/kafka-logs/__cluster_metadata-0/*.log \
  --command "describe"

# 9. æ‰€æœ‰ Broker å‡ç´šå®Œæˆå¾Œï¼Œæ›´æ–° protocol version
# ç·¨è¼¯ server.propertiesï¼Œç§»é™¤æˆ–æ›´æ–° inter.broker.protocol.version

# 10. å†æ¬¡é€ä¸€é‡å•Ÿ Broker
```

### 8.2 å‡ç´šå‰æª¢æŸ¥æ¸…å–®

| é …ç›® | æª¢æŸ¥å…§å®¹ | ç¢ºèª |
|------|----------|------|
| **ç‰ˆæœ¬ç›¸å®¹æ€§** | ç¢ºèªæ–°ç‰ˆæœ¬èˆ‡ç¾æœ‰ Client ç›¸å®¹ | â˜ |
| **ç¡¬é«”è³‡æº** | ç¢ºèªç£ç¢Ÿç©ºé–“ã€è¨˜æ†¶é«”å……è¶³ | â˜ |
| **å‚™ä»½** | å‚™ä»½é…ç½®æª”èˆ‡é‡è¦è³‡æ–™ | â˜ |
| **å¢é›†ç‹€æ…‹** | ç¢ºèªç„¡ Under Replicated Partitions | â˜ |
| **Client ç‰ˆæœ¬** | ç¢ºèª Producer/Consumer ç‰ˆæœ¬ç›¸å®¹ | â˜ |
| **ç›£æ§** | ç¢ºèªç›£æ§ç³»çµ±æ­£å¸¸é‹ä½œ | â˜ |
| **å›æ»¾è¨ˆç•«** | æº–å‚™å›æ»¾æ­¥é©Ÿèˆ‡èˆŠç‰ˆæœ¬å®‰è£åŒ… | â˜ |
| **ç¶­è­·é€šçŸ¥** | é€šçŸ¥ç›¸é—œåœ˜éšŠå‡ç´šæ™‚é–“ | â˜ |

### 8.3 å‡ç´šé¢¨éšªèˆ‡å›å¾©æ©Ÿåˆ¶

#### 8.3.1 å¸¸è¦‹é¢¨éšª

| é¢¨éšª | å½±éŸ¿ | é é˜²æªæ–½ |
|------|------|----------|
| **ç‰ˆæœ¬ä¸ç›¸å®¹** | Client ç„¡æ³•é€£ç·š | å…ˆåœ¨æ¸¬è©¦ç’°å¢ƒé©—è­‰ |
| **é…ç½®éºå¤±** | Broker ç„¡æ³•å•Ÿå‹• | å‚™ä»½é…ç½®æª” |
| **è³‡æ–™æå£** | è¨Šæ¯éºå¤± | ç¢ºä¿ ISR æ­£å¸¸å†å‡ç´š |
| **æ•ˆèƒ½ä¸‹é™** | ç³»çµ±è®Šæ…¢ | ç›£æ§å‡ç´šå¾Œæ•ˆèƒ½ |

#### 8.3.2 å›æ»¾æ­¥é©Ÿ

```bash
# 1. åœæ­¢ Broker
kafka-server-stop.sh

# 2. æ¢å¾©èˆŠç‰ˆæœ¬
rm /opt/kafka
ln -s kafka_2.13-3.5.0 /opt/kafka

# 3. æ¢å¾©é…ç½®æª”
cp /opt/kafka/config/kraft/server.properties.bak \
   /opt/kafka/config/kraft/server.properties

# 4. å•Ÿå‹• Broker
kafka-server-start.sh -daemon /opt/kafka/config/kraft/server.properties

# 5. é©—è­‰æœå‹™æ­£å¸¸
kafka-topics.sh --list --bootstrap-server localhost:9092
```

### 8.4 Client ç›¸å®¹æ€§

#### ç‰ˆæœ¬ç›¸å®¹çŸ©é™£

| Client ç‰ˆæœ¬ | Broker 3.5 | Broker 3.6 | Broker 3.7 |
|-------------|------------|------------|------------|
| Client 3.3 | âœ… | âœ… | âœ… |
| Client 3.4 | âœ… | âœ… | âœ… |
| Client 3.5 | âœ… | âœ… | âœ… |
| Client 3.6 | âš ï¸ | âœ… | âœ… |
| Client 3.7 | âš ï¸ | âš ï¸ | âœ… |

> **èªªæ˜**ï¼šâš ï¸ è¡¨ç¤ºéƒ¨åˆ†æ–°åŠŸèƒ½å¯èƒ½ç„¡æ³•ä½¿ç”¨ï¼Œä½†åŸºæœ¬åŠŸèƒ½æ­£å¸¸

### ğŸ’¡ å¯¦å‹™å»ºè­°

> **å‡ç´šæœ€ä½³å¯¦å‹™**ï¼š
> 1. å…ˆåœ¨æ¸¬è©¦ç’°å¢ƒå®Œæ•´æ¸¬è©¦å‡ç´šæµç¨‹
> 2. é¸æ“‡æ¥­å‹™ä½å³°æœŸé€²è¡Œå‡ç´š
> 3. ä¿æŒèˆŠç‰ˆæœ¬å®‰è£åŒ…ï¼Œä»¥ä¾¿å¿«é€Ÿå›æ»¾
> 4. å‡ç´šå¾Œè§€å¯Ÿè‡³å°‘ 24 å°æ™‚å†é€²è¡Œä¸‹ä¸€æ­¥

---

## 9. å®‰å…¨æ€§èˆ‡æ¬Šé™æ§ç®¡

### 9.1 SSL/TLS åŠ å¯†

#### 9.1.1 å»ºç«‹ SSL æ†‘è­‰

```bash
# 1. å»ºç«‹ CA
openssl req -new -x509 -keyout ca-key -out ca-cert -days 365 \
  -subj "/CN=Kafka-CA"

# 2. å»ºç«‹ Broker Keystore
keytool -genkey -keystore kafka.server.keystore.jks -validity 365 \
  -storepass changeit -keypass changeit \
  -dname "CN=kafka-broker-1" -storetype pkcs12

# 3. å»ºç«‹æ†‘è­‰ç°½ç½²è«‹æ±‚ï¼ˆCSRï¼‰
keytool -keystore kafka.server.keystore.jks -certreq \
  -file kafka-broker-1.csr -storepass changeit

# 4. CA ç°½ç½²æ†‘è­‰
openssl x509 -req -CA ca-cert -CAkey ca-key -in kafka-broker-1.csr \
  -out kafka-broker-1-signed.crt -days 365 -CAcreateserial

# 5. åŒ¯å…¥ CA æ†‘è­‰
keytool -keystore kafka.server.keystore.jks -import -alias CARoot \
  -file ca-cert -storepass changeit -noprompt

# 6. åŒ¯å…¥å·²ç°½ç½²çš„æ†‘è­‰
keytool -keystore kafka.server.keystore.jks -import -alias kafka-broker-1 \
  -file kafka-broker-1-signed.crt -storepass changeit -noprompt

# 7. å»ºç«‹ Truststore
keytool -keystore kafka.server.truststore.jks -import -alias CARoot \
  -file ca-cert -storepass changeit -noprompt
```

#### 9.1.2 Broker SSL è¨­å®š

```properties
# server.properties
listeners=SSL://0.0.0.0:9093
advertised.listeners=SSL://kafka-broker-1:9093

ssl.keystore.location=/opt/kafka/ssl/kafka.server.keystore.jks
ssl.keystore.password=changeit
ssl.key.password=changeit
ssl.truststore.location=/opt/kafka/ssl/kafka.server.truststore.jks
ssl.truststore.password=changeit

ssl.client.auth=required
ssl.enabled.protocols=TLSv1.2,TLSv1.3
```

### 9.2 SASL èªè­‰

#### 9.2.1 SASL/PLAIN è¨­å®š

```properties
# server.properties
listeners=SASL_SSL://0.0.0.0:9094
advertised.listeners=SASL_SSL://kafka-broker-1:9094

sasl.enabled.mechanisms=PLAIN
sasl.mechanism.inter.broker.protocol=PLAIN

# JAAS è¨­å®š
listener.name.sasl_ssl.plain.sasl.jaas.config=\
  org.apache.kafka.common.security.plain.PlainLoginModule required \
  username="admin" \
  password="admin-secret" \
  user_admin="admin-secret" \
  user_producer="producer-secret" \
  user_consumer="consumer-secret";
```

#### 9.2.2 Client SASL è¨­å®š

```properties
# client.properties
security.protocol=SASL_SSL
sasl.mechanism=PLAIN
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \
  username="producer" \
  password="producer-secret";

ssl.truststore.location=/opt/kafka/ssl/kafka.client.truststore.jks
ssl.truststore.password=changeit
```

### 9.3 ACL æ¬Šé™æ§ç®¡

#### 9.3.1 è¨­å®š ACL

```bash
# å…è¨± producer ç”¨æˆ¶å¯«å…¥ order-events topic
kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 \
  --add --allow-principal User:producer \
  --operation Write --topic order-events

# å…è¨± consumer ç”¨æˆ¶å¾ order-events topic è®€å–
kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 \
  --add --allow-principal User:consumer \
  --operation Read --topic order-events \
  --group order-processing-group

# æŸ¥çœ‹ ACL
kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181 \
  --list --topic order-events
```

#### 9.3.2 å¸¸è¦‹ ACL æ¬Šé™

| æ“ä½œ | èªªæ˜ | é©ç”¨è³‡æº |
|------|------|----------|
| **Read** | è®€å–è¨Šæ¯ | Topic, Group |
| **Write** | å¯«å…¥è¨Šæ¯ | Topic |
| **Create** | å»ºç«‹è³‡æº | Topic, Cluster |
| **Delete** | åˆªé™¤è³‡æº | Topic |
| **Alter** | ä¿®æ”¹é…ç½® | Topic, Cluster |
| **Describe** | æŸ¥çœ‹è³‡è¨Š | Topic, Group, Cluster |

### 9.4 ä¼æ¥­å®‰å…¨è¨­è¨ˆå»ºè­°

```mermaid
flowchart TB
    subgraph å®‰å…¨æ¶æ§‹
        EXT[å¤–éƒ¨ç¶²è·¯] -->|é˜²ç«ç‰†| LB[Load Balancer]
        LB -->|SSL/TLS| KAFKA[Kafka Cluster]
        KAFKA -->|SASL| INT[å…§éƒ¨æœå‹™]
    end
    
    subgraph æ¬Šé™ç®¡ç†
        ADMIN[Admin] -->|Full Access| ACL[ACL]
        PROD[Producer] -->|Write Only| ACL
        CONS[Consumer] -->|Read Only| ACL
    end
```

#### å®‰å…¨å»ºè­°æ¸…å–®

| é …ç›® | å»ºè­° |
|------|------|
| **ç¶²è·¯éš”é›¢** | Kafka æ”¾åœ¨å…§ç¶²ï¼Œé€é Gateway å°å¤– |
| **åŠ å¯†å‚³è¼¸** | ç”Ÿç”¢ç’°å¢ƒå¿…é ˆä½¿ç”¨ SSL/TLS |
| **èº«ä»½èªè­‰** | ä½¿ç”¨ SASL é€²è¡Œèº«ä»½é©—è­‰ |
| **æ¬Šé™æ§ç®¡** | ä½¿ç”¨ ACL å¯¦æ–½æœ€å°æ¬Šé™åŸå‰‡ |
| **ç¨½æ ¸æ—¥èªŒ** | å•Ÿç”¨ authorizer æ—¥èªŒè¨˜éŒ„å­˜å– |
| **æ†‘è­‰ç®¡ç†** | å®šæœŸæ›´æ›æ†‘è­‰èˆ‡å¯†ç¢¼ |

### ğŸ’¡ å¯¦å‹™å»ºè­°

> **å®‰å…¨æœ€ä½³å¯¦å‹™**ï¼š
> 1. ç”Ÿç”¢ç’°å¢ƒå‹™å¿…å•Ÿç”¨ SSL åŠ å¯†
> 2. å¯¦æ–½æœ€å°æ¬Šé™åŸå‰‡ï¼Œæ¯å€‹æ‡‰ç”¨ä½¿ç”¨ç¨ç«‹å¸³è™Ÿ
> 3. å®šæœŸå¯©æ ¸ ACL è¨­å®š
> 4. ç›£æ§ç•°å¸¸å­˜å–è¡Œç‚º

---

## 10. æœ€ä½³å¯¦å‹™èˆ‡å¸¸è¦‹åœ°é›·

### 10.1 Topic å‘½åå»ºè­°

#### å‘½åè¦ç¯„

```
æ ¼å¼ï¼š<domain>.<entity>.<event-type>
ç¯„ä¾‹ï¼š
- order.payment.completed
- user.profile.updated
- inventory.stock.depleted
```

| è¦å‰‡ | èªªæ˜ | ç¯„ä¾‹ |
|------|------|------|
| **ä½¿ç”¨å°å¯«** | é¿å…å¤§å°å¯«æ··æ·† | âœ… `order-events` âŒ `Order-Events` |
| **ä½¿ç”¨åˆ†éš”ç¬¦** | ä½¿ç”¨ `.` æˆ– `-` | âœ… `order.events` âœ… `order-events` |
| **é¿å…ç‰¹æ®Šå­—å…ƒ** | åªç”¨å­—æ¯ã€æ•¸å­—ã€`.`ã€`-`ã€`_` | âŒ `order@events` |
| **æœ‰æ„ç¾©çš„å‘½å** | æ¸…æ¥šè¡¨é”ç”¨é€” | âœ… `order-created` âŒ `topic1` |
| **åŒ…å«ç‰ˆæœ¬** | éœ€è¦ç‰ˆæœ¬æ§ç®¡æ™‚ | `order.events.v2` |

### 10.2 Partition è¨­è¨ˆåœ°é›·

#### âŒ å¸¸è¦‹éŒ¯èª¤

| éŒ¯èª¤ | å•é¡Œ | æ­£ç¢ºåšæ³• |
|------|------|----------|
| **Partition å¤ªå°‘** | Consumer ç„¡æ³•ä¸¦è¡Œæ“´å±• | é ä¼°æœªä¾†éœ€æ±‚ï¼Œé ç•™è¶³å¤ æ•¸é‡ |
| **Partition å¤ªå¤š** | å¢åŠ  Leader é¸èˆ‰ã€è¨˜æ†¶é«”é–‹éŠ· | æ§åˆ¶åœ¨åˆç†ç¯„åœï¼ˆæ•¸ç™¾è‡³æ•¸åƒï¼‰ |
| **å¾ŒæœŸå¢åŠ  Partition** | Key åˆ†é…æœƒæ”¹è®Šï¼Œå½±éŸ¿é †åº | ä¸€é–‹å§‹å°±è¦åŠƒå¥½ |
| **å–®ä¸€ Partition** | ç„¡æ³•æ°´å¹³æ“´å±• | é™¤ééœ€è¦å…¨å±€é †åº |

#### Partition æ•¸é‡è¨ˆç®—

```
å»ºè­°å…¬å¼ï¼š
Partition æ•¸é‡ = max(T/P, C)

T = é æœŸååé‡ï¼ˆMB/sï¼‰
P = å–®ä¸€ Partition ååé‡ï¼ˆç´„ 10 MB/sï¼‰
C = Consumer æ•¸é‡
```

### 10.3 Consumer Group éŒ¯èª¤æ¡ˆä¾‹

#### æ¡ˆä¾‹ 1ï¼šConsumer æ•¸é‡è¶…é Partition

```mermaid
flowchart TB
    subgraph Topic
        P0[Partition 0]
        P1[Partition 1]
    end
    
    subgraph Consumer Group
        C1[Consumer 1]
        C2[Consumer 2]
        C3[Consumer 3 é–’ç½®]
    end
    
    P0 --> C1
    P1 --> C2
    C3 -.->|ç„¡æ³•åˆ†é…| Topic
```

**å•é¡Œ**ï¼šConsumer 3 é–’ç½®ï¼Œæµªè²»è³‡æº

**è§£æ±º**ï¼šConsumer æ•¸é‡ â‰¤ Partition æ•¸é‡

#### æ¡ˆä¾‹ 2ï¼šèª¤ç”¨ Consumer Group

```java
// âŒ éŒ¯èª¤ï¼šæ¯å€‹å¯¦ä¾‹ä½¿ç”¨ä¸åŒçš„ Group ID
String groupId = "consumer-" + UUID.randomUUID();

// âœ… æ­£ç¢ºï¼šåŒä¸€æœå‹™çš„å¯¦ä¾‹ä½¿ç”¨ç›¸åŒçš„ Group ID
String groupId = "order-processing-service";
```

#### æ¡ˆä¾‹ 3ï¼šRebalance é¢¨æš´

```java
// âŒ éŒ¯èª¤ï¼šè™•ç†æ™‚é–“éé•·å°è‡´é »ç¹ Rebalance
while (true) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
    for (ConsumerRecord<String, String> record : records) {
        slowProcess(record); // è™•ç†æ™‚é–“ > max.poll.interval.ms
    }
}

// âœ… æ­£ç¢ºï¼šæ§åˆ¶å–®æ¬¡è™•ç†æ™‚é–“
props.put("max.poll.records", 100);  // æ¸›å°‘å–®æ¬¡æ‹‰å–æ•¸é‡
props.put("max.poll.interval.ms", 300000);  // é©ç•¶å¢åŠ é–“éš”
```

### 10.4 çœŸå¯¦å°ˆæ¡ˆå¸¸è¦‹èª¤ç”¨æƒ…å¢ƒ

#### 10.4.1 å°‡ Kafka ç•¶ä½œè³‡æ–™åº«ä½¿ç”¨

```
âŒ éŒ¯èª¤ï¼š
- é »ç¹æŸ¥è©¢ç‰¹å®šè¨Šæ¯
- ä¾è³´ Kafka åšéš¨æ©Ÿè®€å–
- å°‡ Kafka ä½œç‚ºå”¯ä¸€çš„è³‡æ–™å„²å­˜

âœ… æ­£ç¢ºï¼š
- Kafka é©åˆé †åºè®€å–èˆ‡å¯«å…¥
- éœ€è¦æŸ¥è©¢æ™‚ï¼Œå°‡è³‡æ–™åŒæ­¥è‡³è³‡æ–™åº«
- Kafka ä½œç‚ºäº‹ä»¶å‚³è¼¸ç®¡é“ï¼Œä¸æ˜¯è³‡æ–™åº«
```

#### 10.4.2 å¿½ç•¥å†ªç­‰æ€§è¨­è¨ˆ

```java
// âŒ éŒ¯èª¤ï¼šæœªè™•ç†é‡è¤‡æ¶ˆè²»
public void processOrder(OrderEvent event) {
    orderRepository.save(event.getOrder()); // é‡è¤‡æ¶ˆè²»æœƒç”¢ç”Ÿé‡è¤‡è³‡æ–™
}

// âœ… æ­£ç¢ºï¼šå¯¦ä½œå†ªç­‰è™•ç†
public void processOrder(OrderEvent event) {
    if (orderRepository.existsById(event.getOrderId())) {
        return; // å·²è™•ç†é
    }
    orderRepository.save(event.getOrder());
}
```

#### 10.4.3 åŒæ­¥å‘¼å«èˆ‡éåŒæ­¥æ··ç”¨

```java
// âŒ éŒ¯èª¤ï¼šåœ¨åŒæ­¥ API ä¸­ç­‰å¾… Kafka å›æ‡‰
@PostMapping("/orders")
public ResponseEntity<Order> createOrder(@RequestBody Order order) {
    producer.send(record).get(); // é˜»å¡ç­‰å¾…ï¼Œå»¶é²é«˜
    return ResponseEntity.ok(order);
}

// âœ… æ­£ç¢ºï¼šåˆ†é›¢åŒæ­¥èˆ‡éåŒæ­¥é‚è¼¯
@PostMapping("/orders")
public ResponseEntity<Order> createOrder(@RequestBody Order order) {
    Order savedOrder = orderRepository.save(order); // åŒæ­¥å„²å­˜
    producer.send(record); // éåŒæ­¥ç™¼é€äº‹ä»¶
    return ResponseEntity.ok(savedOrder);
}
```

#### 10.4.4 å¤§è¨Šæ¯å‚³è¼¸

```
âŒ éŒ¯èª¤ï¼š
- å‚³è¼¸ > 1MB çš„è¨Šæ¯ï¼ˆå¦‚æª”æ¡ˆã€åœ–ç‰‡ï¼‰
- æœªè¨­å®š max.request.size

âœ… æ­£ç¢ºï¼š
- å¤§å‹è³‡æ–™å­˜æ”¾åœ¨ç‰©ä»¶å„²å­˜ï¼ˆS3ã€Azure Blobï¼‰
- Kafka åªå‚³è¼¸è³‡æ–™çš„åƒè€ƒï¼ˆURLã€IDï¼‰
- å¿…è¦æ™‚èª¿æ•´ max.message.bytesã€max.request.size
```

### 10.5 æœ€ä½³å¯¦å‹™ç¸½çµ

| é¡åˆ¥ | å»ºè­° |
|------|------|
| **Topic è¨­è¨ˆ** | ä¾æ¥­å‹™é ˜åŸŸå‘½åã€é ä¼° Partition æ•¸é‡ |
| **Producer** | å•Ÿç”¨å†ªç­‰æ€§ã€é©ç•¶çš„ acks è¨­å®š |
| **Consumer** | æ‰‹å‹•æäº¤ Offsetã€å¯¦ä½œå†ªç­‰è™•ç† |
| **ç›£æ§** | ç›£æ§ Lagã€è¨­å®šå‘Šè­¦ã€å®šæœŸæª¢æŸ¥ |
| **å®‰å…¨** | å•Ÿç”¨ SSLã€ACL æ¬Šé™æ§ç®¡ |
| **ç¶­é‹** | å®šæœŸå‚™ä»½ã€åˆ¶å®šå‡ç´šè¨ˆç•« |

---

## 11. æª¢æŸ¥æ¸…å–®ï¼ˆChecklistï¼‰

### 11.1 æ–°å°ˆæ¡ˆå°å…¥ Checklist

#### è¦åŠƒéšæ®µ

- [ ] ç¢ºèªä½¿ç”¨å ´æ™¯é©åˆ Kafka
- [ ] è©•ä¼°é æœŸååé‡èˆ‡å»¶é²éœ€æ±‚
- [ ] è¨­è¨ˆ Topic çµæ§‹èˆ‡å‘½åè¦ç¯„
- [ ] æ±ºå®š Partition æ•¸é‡èˆ‡ Replication Factor
- [ ] è¦åŠƒ Consumer Group æ¶æ§‹
- [ ] è©•ä¼°å®‰å…¨æ€§éœ€æ±‚ï¼ˆSSLã€ACLï¼‰

#### éƒ¨ç½²éšæ®µ

- [ ] æº–å‚™ç¡¬é«”è³‡æºï¼ˆCPUã€Memoryã€Diskã€Networkï¼‰
- [ ] å®‰è£ JDKï¼ˆå»ºè­° Java 17ï¼‰
- [ ] å®‰è£ä¸¦è¨­å®š Kafkaï¼ˆKRaft æ¨¡å¼ï¼‰
- [ ] è¨­å®š JVM åƒæ•¸ï¼ˆHeap å¤§å°ï¼‰
- [ ] è¨­å®šé˜²ç«ç‰†è¦å‰‡
- [ ] å»ºç«‹ Systemd Service
- [ ] é©—è­‰å¢é›†ç‹€æ…‹

#### é–‹ç™¼éšæ®µ

- [ ] å¼•å…¥ Kafka Client ä¾è³´
- [ ] å¯¦ä½œ Producer èˆ‡ Consumer
- [ ] å¯¦ä½œéŒ¯èª¤è™•ç†èˆ‡é‡è©¦æ©Ÿåˆ¶
- [ ] å¯¦ä½œå†ªç­‰è™•ç†é‚è¼¯
- [ ] æ’°å¯«æ•´åˆæ¸¬è©¦
- [ ] é€²è¡Œæ•ˆèƒ½æ¸¬è©¦

#### ä¸Šç·šå‰

- [ ] è¨­å®šç›£æ§èˆ‡å‘Šè­¦
- [ ] å»ºç«‹ Runbookï¼ˆæ“ä½œæ‰‹å†Šï¼‰
- [ ] æº–å‚™å›æ»¾è¨ˆç•«
- [ ] é€²è¡Œå£“åŠ›æ¸¬è©¦
- [ ] å®‰å…¨æ€§æª¢æŸ¥

### 11.2 æ—¥å¸¸ç¶­é‹ Checklist

#### æ¯æ—¥æª¢æŸ¥

- [ ] ç¢ºèªæ‰€æœ‰ Broker æ­£å¸¸é‹ä½œ
- [ ] æª¢æŸ¥ Under Replicated Partitions
- [ ] æª¢æŸ¥ Consumer Lag
- [ ] æª¢æŸ¥ç£ç¢Ÿä½¿ç”¨ç‡
- [ ] æª¢æŸ¥éŒ¯èª¤æ—¥èªŒ

#### æ¯é€±æª¢æŸ¥

- [ ] æª¢æŸ¥ Topic å¢é•·è¶¨å‹¢
- [ ] æª¢æŸ¥æ•ˆèƒ½æŒ‡æ¨™
- [ ] å¯©æ ¸ ACL è¨­å®š
- [ ] ç¢ºèªå‚™ä»½æ­£å¸¸

#### æ¯æœˆæª¢æŸ¥

- [ ] æª¢æŸ¥ Kafka ç‰ˆæœ¬ï¼Œè©•ä¼°å‡ç´šéœ€æ±‚
- [ ] æª¢æŸ¥æ†‘è­‰éæœŸæ™‚é–“
- [ ] å¯©æ ¸å®¹é‡è¦åŠƒ
- [ ] æ›´æ–°æ–‡ä»¶èˆ‡ Runbook

### 11.3 æ•…éšœæ’é™¤ Checklist

#### Broker å•é¡Œ

- [ ] æª¢æŸ¥ Broker æ˜¯å¦å•Ÿå‹•ï¼ˆjpsï¼‰
- [ ] æª¢æŸ¥æ—¥èªŒæª”æ¡ˆ
- [ ] æª¢æŸ¥ JVM è¨˜æ†¶é«”
- [ ] æª¢æŸ¥ç£ç¢Ÿç©ºé–“
- [ ] æª¢æŸ¥ç¶²è·¯é€£ç·š

#### Producer å•é¡Œ

- [ ] ç¢ºèª bootstrap.servers è¨­å®š
- [ ] æª¢æŸ¥ acks è¨­å®š
- [ ] æª¢æŸ¥ç¶²è·¯é€£ç·š
- [ ] æª¢æŸ¥è¨Šæ¯å¤§å°
- [ ] æª¢æŸ¥åºåˆ—åŒ–è¨­å®š

#### Consumer å•é¡Œ

- [ ] ç¢ºèª Consumer Group ID
- [ ] æª¢æŸ¥ Offset ç‹€æ…‹
- [ ] æª¢æŸ¥ Rebalance é »ç‡
- [ ] æª¢æŸ¥è™•ç†æ™‚é–“
- [ ] æª¢æŸ¥ååºåˆ—åŒ–è¨­å®š

### 11.4 å‡ç´š Checklist

#### å‡ç´šå‰

- [ ] é–±è®€ Release Notes
- [ ] ç¢ºèªç‰ˆæœ¬ç›¸å®¹æ€§
- [ ] å‚™ä»½é…ç½®æª”
- [ ] åœ¨æ¸¬è©¦ç’°å¢ƒé©—è­‰
- [ ] é€šçŸ¥ç›¸é—œåœ˜éšŠ
- [ ] æº–å‚™å›æ»¾è¨ˆç•«

#### å‡ç´šä¸­

- [ ] ç¢ºèªå¢é›†ç‹€æ…‹æ­£å¸¸
- [ ] é€ä¸€åœæ­¢ã€å‡ç´šã€å•Ÿå‹• Broker
- [ ] æ¯æ¬¡å‡ç´šå¾Œé©—è­‰
- [ ] ç›£æ§ç•°å¸¸æŒ‡æ¨™

#### å‡ç´šå¾Œ

- [ ] é©—è­‰æ‰€æœ‰åŠŸèƒ½æ­£å¸¸
- [ ] æ›´æ–° inter.broker.protocol.version
- [ ] ç›£æ§è‡³å°‘ 24 å°æ™‚
- [ ] æ›´æ–°æ–‡ä»¶

---

## é™„éŒ„

### é™„éŒ„ Aï¼šå¸¸ç”¨æŒ‡ä»¤é€ŸæŸ¥

### Topic ç®¡ç†

```bash
# åˆ—å‡º Topic
kafka-topics.sh --list --bootstrap-server localhost:9092

# å»ºç«‹ Topic
kafka-topics.sh --create --topic <name> --partitions <n> \
  --replication-factor <r> --bootstrap-server localhost:9092

# æŸ¥çœ‹ Topic
kafka-topics.sh --describe --topic <name> --bootstrap-server localhost:9092

# åˆªé™¤ Topic
kafka-topics.sh --delete --topic <name> --bootstrap-server localhost:9092
```

### Consumer Group ç®¡ç†

```bash
# åˆ—å‡º Consumer Group
kafka-consumer-groups.sh --list --bootstrap-server localhost:9092

# æŸ¥çœ‹ Offset ç‹€æ…‹
kafka-consumer-groups.sh --describe --group <group> \
  --bootstrap-server localhost:9092

# é‡ç½® Offset
kafka-consumer-groups.sh --reset-offsets --group <group> \
  --topic <topic> --to-earliest --execute --bootstrap-server localhost:9092
```

### Console Producer/Consumer

```bash
# ç™¼é€è¨Šæ¯
kafka-console-producer.sh --topic <topic> --bootstrap-server localhost:9092

# æ¶ˆè²»è¨Šæ¯
kafka-console-consumer.sh --topic <topic> --from-beginning \
  --bootstrap-server localhost:9092
```

---

### é™„éŒ„ Bï¼šè¨­å®šåƒæ•¸é€ŸæŸ¥

### Broker é‡è¦åƒæ•¸

| åƒæ•¸ | é è¨­å€¼ | èªªæ˜ |
|------|--------|------|
| `broker.id` | - | Broker å”¯ä¸€è­˜åˆ¥ç¢¼ |
| `log.dirs` | /tmp/kafka-logs | æ—¥èªŒå„²å­˜ç›®éŒ„ |
| `num.partitions` | 1 | é è¨­ Partition æ•¸ |
| `default.replication.factor` | 1 | é è¨­å‰¯æœ¬å› å­ |
| `min.insync.replicas` | 1 | æœ€å°åŒæ­¥å‰¯æœ¬æ•¸ |
| `log.retention.hours` | 168 | æ—¥èªŒä¿ç•™æ™‚é–“ |

### Producer é‡è¦åƒæ•¸

| åƒæ•¸ | é è¨­å€¼ | èªªæ˜ |
|------|--------|------|
| `acks` | all | ç¢ºèªæ©Ÿåˆ¶ |
| `retries` | 2147483647 | é‡è©¦æ¬¡æ•¸ |
| `batch.size` | 16384 | æ‰¹æ¬¡å¤§å° |
| `linger.ms` | 0 | æ‰¹æ¬¡ç­‰å¾…æ™‚é–“ |
| `enable.idempotence` | true | å•Ÿç”¨å†ªç­‰æ€§ |

### Consumer é‡è¦åƒæ•¸

| åƒæ•¸ | é è¨­å€¼ | èªªæ˜ |
|------|--------|------|
| `group.id` | - | Consumer Group ID |
| `auto.offset.reset` | latest | Offset é‡ç½®ç­–ç•¥ |
| `enable.auto.commit` | true | è‡ªå‹•æäº¤ Offset |
| `max.poll.records` | 500 | å–®æ¬¡æ‹‰å–æœ€å¤§ç­†æ•¸ |
| `session.timeout.ms` | 45000 | Session è¶…æ™‚ |

---

### é™„éŒ„ Cï¼šåƒè€ƒè³‡æº

- [Apache Kafka å®˜æ–¹æ–‡ä»¶](https://kafka.apache.org/documentation/)
- [Confluent Kafka æ–‡ä»¶](https://docs.confluent.io/)
- [Spring Kafka æ–‡ä»¶](https://spring.io/projects/spring-kafka)
- [Kafka: The Definitive Guideï¼ˆæ›¸ç±ï¼‰](https://www.confluent.io/resources/kafka-the-definitive-guide/)

---

> **æ–‡ä»¶ç¶­è­·**ï¼šæœ¬æ‰‹å†Šæ‡‰å®šæœŸæ›´æ–°ï¼Œåæ˜ æœ€æ–°ç‰ˆæœ¬èˆ‡åœ˜éšŠå¯¦å‹™ç¶“é©—ã€‚  
> **å›é¥‹å»ºè­°**ï¼šå¦‚æœ‰å•é¡Œæˆ–å»ºè­°ï¼Œè«‹è¯ç¹«å¹³å°åœ˜éšŠã€‚

